[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "How To Prove It with Lean",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\mathbin{∆}}\n$$"
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "How To Prove It with Lean",
    "section": "About This Book",
    "text": "About This Book\nThis book is intended to accompany my book How To Prove It: A Structured Approach, 3rd edition (henceforth called HTPI), which is published by Cambridge University Press. Although this book is self-contained, we will sometimes have occasion to refer to passages in HTPI, so this book will be easiest to understand if you have a copy of HTPI available to you.\nHTPI explains a systematic approach to constructing mathematical proofs. The purpose of this book is to show you how to use a computer software package called Lean to help you master the techniques presented in HTPI. Lean is free software that is available for Windows, MacOS, and Unix computers. It is also possible to run Lean in a web browser using GitHub Codespaces. We will explain below how to set up Lean on your computer; in a later chapter we’ll explain how to get started using Lean.\nThe chapters and sections of this book are numbered to match the sections of HTPI to which they correspond. The first two chapters of HTPI cover preliminary topics in elementary logic and set theory that are needed to understand the proof techniques presented in later chapters. We assume that you are already familiar with that material (if not, go read those chapters in HTPI!), so Chapters 1 and 2 of this book will just briefly summarize the most important points. Those chapters are followed by an introduction to Lean that explains the basics of using Lean to write proofs. The presentation of proof techniques in HTPI begins in earnest in Chapter 3, so that is where we will begin to discuss how Lean can be used to master those techniques.\nIf you are reading this book online, then at the end of the title in the left margin you will find an icon that is a link to a pdf version of the book. Below that is a search box, which you can use to search for any word or phrase anywhere in the book. Below the search box is a list of the chapters of the book. Click on any chapter to go to that chapter. Within each chapter, a table of contents in the right margin lists the sections in that chapter. Again, you can go to any section by clicking on it. At the end of each chapter there are links to take you to the next or previous chapter."
  },
  {
    "objectID": "index.html#about-lean",
    "href": "index.html#about-lean",
    "title": "How To Prove It with Lean",
    "section": "About Lean",
    "text": "About Lean\nLean is a kind of software package called a proof assistant. What that means is that Lean can help you to write proofs. As we will see over the course of this book, there are several ways in which Lean can be helpful. First of all, if you type a proof into Lean, then Lean can check the correctness of the proof and point out errors. As you are typing a proof into Lean, it will keep track of what has been accomplished so far in the proof and what remains to be done to finish the proof, and it will display that information for you. That can keep you moving in the right direction as you are figuring out a proof. And sometimes Lean can fill in small details of the proof for you.\nOf course, to make this possible, you must type your proof in a format that Lean understands. Much of this book will be taken up with explaining how to write a proof so that Lean will understand it.\nNote that this book uses a customized version of Lean. The customization is designed to make Lean proofs more readable and to bring Lean into closer agreement with HTPI. The appendix of this book includes advice about transitioning from the Lean in this book to standard Lean."
  },
  {
    "objectID": "index.html#setting-up-the-htpi-lean-package",
    "href": "index.html#setting-up-the-htpi-lean-package",
    "title": "How To Prove It with Lean",
    "section": "Setting Up The HTPI Lean Package",
    "text": "Setting Up The HTPI Lean Package\nThis book comes with a Lean “package” (or “project”), which is a folder containing a collection of files to be used with Lean. We will be using a software package called Visual Studio Code (VS Code) to access the files in the HTPI Lean Package. There are two ways to do this: by using GitHub Codespaces, or by installing Lean on your computer.\n\nUsing GitHub Codespaces\nThe easiest way to get started with Lean is to use GitHub Codespaces in your web browser. To open the HTPI Lean Package in Codespaces, click here.\nYou will be prompted to create a GitHub account if you don’t already have one; a free account gives you 120 hours of use per month. Then you will be given various options for the creation of your codespace; you can use the default options. Click on the green “Create codespace” button. It will take several minutes to create your codespace, but this only needs to be done once; be patient. When that process is complete, you should see a window that looks something like this:\n\nThis image shows a version of VS Code, running in your codespace. You can access the menus for VS Code by clicking on the “\\(\\equiv\\)” symbol at the top of the left sidebar. To the right of this sidebar, under the heading “Explorer,” you will see a list of the files and folders in the HTPI Lean Package. You can now click on the “X” on the right side of the lower part of the window (circled in red above) to close the terminal pane, since we won’t be using that pane.\n\n\nInstalling Lean\nIf you prefer to run Lean on your computer, you can find instructions for installing Lean here. These instructions will lead you through installing VS Code, installing the Lean 4 extension in VS Code, and then opening the Lean 4 setup guide. The setup guide will then tell you how to install Lean dependencies and the Lean version manager. There is no need to consult the recommended books and documentation at this time—this book will tell you everything you need to know to use Lean with HTPI. (But later, if you want to learn more about Lean, you may find those resources useful.) And there is no need to follow the instructions for setting up a Lean project, because you will be using the HTPI Lean package.\nYou can download the HTPI Lean Package from https://github.com/djvelleman/HTPILeanPackage. After following the link, click on the green “Code” button and, in the pop-up menu, select “Download ZIP”. Once the zip file has been downloaded, extract the contents of the file into a folder. You can put this folder wherever you want on your computer.\nNext, in VS Code, select “Open Folder …” from the File menu and open the folder containing the HTPI Lean Package that you downloaded. (If VS Code asks if you want to open the package in a “container”, it is probably best to say no.) Under the heading “Explorer” on the left side of the window, you should see a list of the files in the package. (If you don’t see the list, try clicking on the Explorer icon at the top of the left sidebar; it looks like this: .) Your screen should look similar to the image above in the section about using GitHub Codespaces. Click on the file “Blank.lean” in the file list. Then click on the “\\(\\forall\\)” icon, and in the pop-up menu select “Project Actions … > Project: Build Project”. Lean should “build” the HTPI Lean Package. You’ll see a number of messages, including some warnings that you can ignore. Eventually you should see the message “Project built successfully.” Your installation is now complete."
  },
  {
    "objectID": "index.html#vs-code-and-the-htpi-lean-package",
    "href": "index.html#vs-code-and-the-htpi-lean-package",
    "title": "How To Prove It with Lean",
    "section": "VS Code and The HTPI Lean Package",
    "text": "VS Code and The HTPI Lean Package\nWhen you view the HTPI Lean Package in VS Code, you will see a list of the files and folders in the package under the “Explorer” heading. The folders are listed first, with a “\\(>\\)” symbol in front of each; click on a folder to see the files inside it.\nYou won’t need to use most of the files in the package. In the chapter “Introduction to Lean,” you will learn how to edit the file “Blank.lean” to write your first proofs. The files “Chap3Ex.lean” through “Chap8Ex.lean” contain all of the exercises; you will enter your solutions to the exercises in those files. In the exercise file for a chapter, all Lean definitions and theorems from that chapter and all earlier chapters are available for use in solving the exercises.\nThe only other files that you may need to look at are in the HTPILib folder. That folder includes files containing all Lean definitions and theorems in all the chapters, starting with “Introduction to Lean.” There is also a file in that folder that defines the customization mentioned earlier. All of these files are needed to make the package work. Do not edit the files in the HTPILib folder.\nSince you won’t be using the HTPI Lean Package right away, you can close it for now. If you opened it in GitHub Codespaces, then you can close the browser tab or window containing your codespace. GitHub will have given your codespace a whimsical name and saved it for you. When you are ready to start using Lean, you can return to your codespace from the GitHub Codespaces page at https://github.com/codespaces. If you installed VS Code and Lean on your computer, then you can quit VS Code and open it again when you are ready to start using Lean."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "How To Prove It with Lean",
    "section": "License",
    "text": "License\nThis book is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This license allows you to share or adapt the book. In any adaptation, you must identify Daniel J. Velleman as the author, and you must also acknowledge that excerpts from How To Prove It, 3rd edition, copyright Daniel J. Velleman 2019, published by Cambridge University Press, are reprinted with the permission of Cambridge University Press. Each such excerpt is identified in this book with a parenthetical note “(HTPI p. …)” specifying the page of How To Prove It, 3rd edition from which the excerpt is taken. For further details, see the text of the license."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "How To Prove It with Lean",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nA number of people have provided advice, encouragement, and feedback about this project. In particular, I would like to thank Jeremy Avigad, Clayton Cafiero, Nathan Carter, François Dorais, Charles Hoskinson, Heather Macbeth, Pietro Monticone, and Ketil Wright."
  },
  {
    "objectID": "Chap1.html",
    "href": "Chap1.html",
    "title": "1  Sentential Logic",
    "section": "",
    "text": "Chapter 1 of How To Prove It introduces the following symbols of logic:\n\n\n\n\nSymbol\nMeaning\n\n\n\n\n\\(\\neg\\)\nnot\n\n\n\\(\\wedge\\)\nand\n\n\n\\(\\vee\\)\nor\n\n\n\\(\\to\\)\nif … then\n\n\n\\(\\leftrightarrow\\)\niff (that is, if and only if)\n\n\n\n\nAs we will see, Lean uses the same symbols, with the same meanings. A statement of the form \\(P \\wedge Q\\) is called a conjunction, a statement of the form \\(P \\vee Q\\) is called a disjunction, a statement of the form \\(P \\to Q\\) is an implication or a conditional statement (with antecedent \\(P\\) and consequent \\(Q\\)), and a statement of the form \\(P \\leftrightarrow Q\\) is a biconditional statement. The statement \\(\\neg P\\) is the negation of \\(P\\).\nThis chapter also establishes a number of logical equivalences that will be useful to us later:\n\n\n\n\n\n\n\n\n\n\nName\n\nEquivalence\n\n\n\n\n\nDe Morgan’s Laws\n\\(\\neg (P \\wedge Q)\\)\nis equivalent to\n\\(\\neg P \\vee \\neg Q\\)\n\n\n\n\\(\\neg (P \\vee Q)\\)\nis equivalent to\n\\(\\neg P \\wedge \\neg Q\\)\n\n\nDouble Negation Law\n\\(\\neg\\neg P\\)\nis equivalent to\n\\(P\\)\n\n\nConditional Laws\n\\(P \\to Q\\)\nis equivalent to\n\\(\\neg P \\vee Q\\)\n\n\n\n\\(P \\to Q\\)\nis equivalent to\n\\(\\neg(P \\wedge \\neg Q)\\)\n\n\nContrapositive Law\n\\(P \\to Q\\)\nis equivalent to\n\\(\\neg Q \\to \\neg P\\)\n\n\n\n\nFinally, Chapter 1 of HTPI introduces some concepts from set theory. A set is a collection of objects; the objects in the collection are called elements of the set. The notation \\(x \\in A\\) means that \\(x\\) is an element of \\(A\\). Two sets \\(A\\) and \\(B\\) are equal if they have exactly the same elements. We say that \\(A\\) is a subset of \\(B\\), denoted \\(A \\subseteq B\\), if every element of \\(A\\) is an element of \\(B\\). If \\(P(x)\\) is a statement about \\(x\\), then \\(\\{x \\mid P(x)\\}\\) denotes the set whose elements are the objects \\(x\\) for which \\(P(x)\\) is true. And we have the following operations on sets:\n\n\\(A \\cap B = \\{x \\mid x \\in A \\wedge x \\in B\\} = {}\\) the intersection of \\(A\\) and \\(B\\),\n\\(A \\cup B = \\{x \\mid x \\in A \\vee x \\in B\\} = {}\\) the union of \\(A\\) and \\(B\\),\n\\(A \\setmin B = \\{x \\mid x \\in A \\wedge x \\notin B\\} = {}\\) the difference of \\(A\\) and \\(B\\),\n\\(A \\symmdiff B = (A \\setmin B) \\cup (B \\setmin A) = {}\\) the symmetric difference of \\(A\\) and \\(B\\)."
  },
  {
    "objectID": "Chap2.html",
    "href": "Chap2.html",
    "title": "2  Quantificational Logic",
    "section": "",
    "text": "Chapter 2 of How To Prove It introduces two more symbols of logic, the quantifiers \\(\\forall\\) and \\(\\exists\\). If \\(P(x)\\) is a statement about an object \\(x\\), then\n\n\\(\\forall x\\,P(x)\\) means “for all \\(x\\), \\(P(x)\\),”\n\nand\n\n\\(\\exists x\\,P(x)\\) means “there exists some \\(x\\) such that \\(P(x)\\).”\n\nLean also uses these symbols, although we will see that quantified statements are written slightly differently in Lean from the way they are written in HTPI. In the statement \\(P(x)\\), the variable \\(x\\) is called a free variable. But in \\(\\forall x\\,P(x)\\) or \\(\\exists x\\,P(x)\\), it is a bound variable; we say that the quantifiers \\(\\forall\\) and \\(\\exists\\) bind the variable.\nOnce again, there are logical equivalences involving these symbols that will be useful to us later:\n\n\n\n\n\nQuantifier Negation Laws\n\n\n\n\n\n\\(\\neg \\exists x\\,P(x)\\)\nis equivalent to\n\\(\\forall x\\,\\neg P(x)\\)\n\n\n\\(\\neg \\forall x\\,P(x)\\)\nis equivalent to\n\\(\\exists x\\,\\neg P(x)\\)\n\n\n\n\nChapter 2 of HTPI also introduces some more advanced set theory operations. For any set \\(A\\),\n\n\\(\\mathscr{P}(A) = \\{X \\mid X \\subseteq A\\} = {}\\) the power set of \\(A\\).\n\nAlso, if \\(\\mathcal{F}\\) is a family of sets—that is, a set whose elements are sets—then\n\n\\(\\bigcap \\mathcal{F} = \\{x \\mid \\forall A(A \\in \\mathcal{F} \\to x \\in A)\\} = {}\\) the intersection of the family \\(\\mathcal{F}\\),\n\\(\\bigcup \\mathcal{F} = \\{x \\mid \\exists A(A \\in \\mathcal{F} \\wedge x \\in A)\\} = {}\\) the union of the family \\(\\mathcal{F}\\).\n\nFinally, Chapter 2 introduces the notation \\(\\exists ! x\\,P(x)\\) to mean “there is exactly one \\(x\\) such that \\(P(x).\\)” This can be thought of as an abbreviation for \\(\\exists x(P(x) \\wedge \\neg\\exists y(P(y) \\wedge y \\ne x))\\). By the quantifier negation, De Morgan, and conditional laws, this is equivalent to \\(\\exists x(P(x) \\wedge \\forall y(P(y) \\to y = x))\\)."
  },
  {
    "objectID": "IntroLean.html",
    "href": "IntroLean.html",
    "title": "Introduction to Lean",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\mathbin{∆}}\n$$\nIf you are reading this book in conjunction with How To Prove It, you should complete Section 3.2 of HTPI before reading this chapter. Once you have reached that point in HTPI, you are ready to start learning about Lean. In this chapter we’ll explain the basics of writing proofs in Lean and getting feedback from Lean."
  },
  {
    "objectID": "IntroLean.html#a-first-example",
    "href": "IntroLean.html#a-first-example",
    "title": "Introduction to Lean",
    "section": "A First Example",
    "text": "A First Example\nWe’ll start with Example 3.2.4 in How To Prove It. Here is how the theorem and proof in that example appear in HTPI (HTPI p. 110; consult HTPI if you want to see how this proof was constructed):\n\nSuppose \\(P \\to (Q \\to R)\\). Then \\(\\neg R \\to (P \\to \\neg Q)\\).\n\n\nProof. Suppose \\(\\neg R\\). Suppose \\(P\\). Since \\(P\\) and \\(P \\to (Q \\to R)\\), it follows that \\(Q \\to R\\). But then, since \\(\\neg R\\), we can conclude \\(\\neg Q\\). Thus, \\(P \\to \\neg Q\\). Therefore \\(\\neg R \\to (P \\to \\neg Q)\\).  □\n\nAnd here is how we would write the proof in Lean.1\ntheorem Example_3_2_4\n    (P Q R : Prop) (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  have h4 : Q → R := h h3\n  contrapos at h4            --Now h4 : ¬R → ¬Q\n  show ¬Q from h4 h2\n  done\nLet’s go through this Lean proof line-by-line and see what it means. The first line tells Lean that we are going to prove a theorem, and it gives the theorem a name, Example_3_2_4. The next line states the theorem. In the theorem as stated in HTPI, the letters \\(P\\), \\(Q\\), and \\(R\\) are used to stand for statements that are either true or false. In logic, such statements are often called propositions. The expression (P Q R : Prop) on the second line tells Lean that P, Q, and R will be used in this theorem to stand for propositions. The next parenthetical expression, (h : P → (Q → R)), states the hypothesis of the theorem and gives it the name h; the technical term that Lean uses is that h is an identifier for the hypothesis. Assigning an identifier to the hypothesis gives us a way to refer to it when it is used later in the proof. Almost any string of characters that doesn’t begin with a digit can be used as an identifier, but it is traditional to use identifiers beginning with the letter h for statements. After the statement of the hypothesis there is a colon followed by the conclusion of the theorem, ¬R → (P → ¬Q). Finally, at the end of the second line, the expression := by signals the beginning of the proof.\nEach of the remaining lines is a step in the proof. The first line of the proof introduces the assumption ¬R and gives it the identifier h2. Of course, this corresponds precisely to the first sentence of the proof in HTPI. Similarly, the second line, corresponding to the second sentence of the HTPI proof, assigns the identifier h3 to the assumption P. The next line makes the inference Q → R, giving it the identifier h4. The inference is justified by combining statements h and h3—that is, the statements P → (Q → R) and P—exactly as in the third sentence of the proof in HTPI.\nThe next step of the proof in HTPI combines the statements \\(Q \\to R\\) and \\(\\neg R\\) to draw the inference \\(\\neg Q\\). This reasoning is justified by the contrapositive law, which says that \\(Q \\to R\\) is equivalent to its contrapositive, \\(\\neg R \\to \\neg Q\\). In the Lean proof, this inference is broken up into two steps. In the fourth line of the proof, we ask Lean to rewrite statement h4—that is, Q → R—using the contrapositive law. Two hyphens in a row tell Lean that the rest of the line is a comment. Lean ignores comments and displays them in green. The comment on line four serves as a reminder that h4 now stands for the statement ¬R → ¬Q. Finally, in the last step of the proof, we combine the new h4 with h2 to infer ¬Q. There is no need to give this statement an identifier, because it completes the proof. In the proof in HTPI, there are a couple of final sentences explaining why this completes the proof, but Lean doesn’t require this explanation."
  },
  {
    "objectID": "IntroLean.html#term-mode",
    "href": "IntroLean.html#term-mode",
    "title": "Introduction to Lean",
    "section": "Term Mode",
    "text": "Term Mode\nNow that you have seen an example of a proof in Lean, it is time for you to write your first proof. Lean has two modes for writing proofs, called term mode and tactic mode. The example above was written in tactic mode, and that is the mode we will use for most proofs in this book. But before we study the construction of proofs in tactic mode, it will be helpful to learn a bit about term mode. Term mode is best for simple proofs, so we begin with a few very short proofs.\nIf you have not yet opened the HTPI Lean package in VS Code, either in GitHub Codespaces or on your computer, then go back and follow the instructions in the preface. Once you have opened the package, click on the file Blank.lean in the list of files. The file starts with the line import HTPILib.HTPIDefs. Click on the blank line at the end of the file; this is where you will be typing your first proofs.\nNow type in the following theorem and proof. (If you are reading this book online, then Lean examples like the one below will appear in gray boxes. You can copy the example to your clipboard by clicking in the upper-right corner of the box, and then you can paste it into a file in VS Code to try it out.)\ntheorem extremely_easy (P : Prop) (h : P) : P := h\nIf you have typed this correctly, Lean will put a check mark in the margin to the left of the theorem, indicating that the proof is correct. This theorem and proof are so short that we have put everything on one line. In this theorem, the letter P is used to stand for a proposition. The theorem has one hypothesis, P, which has been given the identifier h, and the conclusion of the theorem is also P. The notation := indicates that what follows will be a proof in term mode.\nOf course, the proof of the theorem is extremely easy: to prove P, we just have to point out that it is given as the hypothesis h. And so the proof in Lean consists of just one letter: h.\nEven though this example is a triviality, there are some things to be learned from it. First of all, although we have been describing the letter h as an identifier for the hypothesis P, this example illustrates that Lean also considers h to be a proof of P. In general, when we see h : P in a Lean proof, where P is a proposition, we can think of it as meaning, not just that h is an identifier for the statement P, but also that h is a proof of P.\nWe can learn something else from this example by changing it slightly. If you change the final h to a different letter—say, f—you will see that Lean puts a red squiggly line under the f, like this:\ntheorem extremely_easy (P : Prop) (h : P) : P := **f::\nThis indicates that Lean has detected an error in the proof. Lean always indicates errors by putting a red squiggle under the offending text. Lean also puts a red X with a circle around it in the left margin next to the line with the error, and it puts a message in the Lean Infoview pane explaining what the error is. (If you don’t see the Infoview pane on the right side of the window, click on the “\\(\\forall\\)” icon near the top of the window and select “Infoview: Toggle Infoview” from the popup menu to make the Infoview pane appear.) In this case, the message is unknown identifier 'f'. The message is introduced by a heading, in red, that identifies the file, the line number, and the character position on that line where the error appears. If you change f back to h, the red X with the circle around it, red squiggle, and error message go away, and Lean returns the check mark to the left margin next to the theorem.\nLet’s try a slightly less trivial example. You can type the next theorem below the previous one, leaving a blank line between them to keep them visually separate. To type the → symbol in the next example, type \\to and then hit either the space bar or the tab key; when you type either space or tab, the \\to will change to →. Alternatively, you can type \\r (short for “right arrow”) or \\imp (short for “implies”), again followed by either space or tab. Or, you can type ->, and Lean will interpret it as →. (There is a list in the appendix showing how to type all of the symbols used in this book.)\ntheorem very_easy\n    (P Q : Prop) (h1 : P → Q) (h2 : P) : Q := h1 h2\nIndenting the second line is not necessary, but it is traditional. When stating a theorem, we will generally indent all lines after the first with two tabs in VS Code. Once you indent a line, VS Code will maintain that same indenting in subsequent lines until you delete tabs at the beginning of a line to reduce or eliminate indenting.\nThis time there are two hypotheses, h1 : P → Q and h2 : P. As explained in Section 3.2 of HTPI, the conclusion Q follows from these hypotheses by the logical rule modus ponens. To use modus ponens to complete this proof in term mode, we simply write the identifiers of the two hypotheses—which, as we have just seen, can also be thought of as proofs of the two hypotheses—one after the other, with a space between them. It is important to write the proof of the conditional hypothesis first, so the proof is written h1 h2; if you try writing this proof as h2 h1, you will get a red squiggle. In general, if a is a proof of any conditional statement X → Y, and b is a proof of the antecedent X, then a b is a proof of the consequent Y. The proofs a and b need not be simply identifiers; any proofs of a conditional statement and its antecedent can be combined in this way.\nWe’ll try one more proof in term mode:\ntheorem easy (P Q R : Prop) (h1 : P → Q)\n    (h2 : Q → R) (h3 : P) : R :=\nNote that in the statement of the theorem, you can break the lines however you please; this time we have put the declaration of P, Q, and R and the first hypothesis on the first line and the other two hypotheses on the second line. How can we prove the conclusion R? Well, we have h2 : Q → R, so if we could prove Q then we could use modus ponens to reach the desired conclusion. In other words, h2 _ will be a proof of R, if we can fill in the blank with a proof of Q. Can we prove Q? Yes, Q follows from P → Q and P by modus ponens, so h1 h3 is a proof of Q. Filling in the blank, we conclude that h2 (h1 h3) is a proof of R. Type it in, and you’ll see that Lean will accept it. Note that the parentheses are important; if you write h2 h1 h3 then Lean will interpret it as (h2 h1) h3, which doesn’t make sense, and you’ll get an error."
  },
  {
    "objectID": "IntroLean.html#tactic-mode",
    "href": "IntroLean.html#tactic-mode",
    "title": "Introduction to Lean",
    "section": "Tactic Mode",
    "text": "Tactic Mode\nFor more complicated proofs, it is easier to use tactic mode. Type the following theorem into Lean; to type the symbol ¬, type \\not, followed again by either space or tab. Alternatively, if you type Not P, Lean will interpret it as meaning ¬P.\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P :=\nLean is now waiting for you to type a proof in term mode. To switch to tactic mode, type by after :=. We find it helpful to set off a tactic proof from the surrounding text by indenting it with one tab, and also by marking where the proof ends. To do this, leave a blank line after the statement of the theorem, adjust the indenting to one tab, and type done. You will type your proof between the statement of the theorem and the line containing done, so click on the blank line between them to position the cursor there. Lean can be fussy about indenting; it will be important to indent all steps of the proof by the same amount.\nOne of the advantages of tactic mode is that Lean displays, in the Lean Infoview pane, information about the status of the proof as your write it. As soon as you position your cursor on the blank line, Lean displays what it calls the “tactic state” in the Infoview pane. Your screen should look like this. (If you are reading this book online, then examples like the one below may not display well if your window is too narrow; try adjusting the width of the window.)\n\n\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P := by\n\n  **done::\n\n\nP Q R : Prop\nh1 : P → Q\nh2 : Q → ¬R\n⊢ R → ¬P\n\n\nThe red squiggle under done indicates that Lean knows that the proof isn’t done. The tactic state in the Infoview pane is very similar to the lists of givens and goals that are used in HTPI. The hypotheses h1 : P → Q and h2 : Q → ¬R are examples of what are called givens in HTPI. The tactic state above says that P, Q, and R stand for propositions, and then it lists the two givens h1 and h2. The symbol ⊢ in the last line labels the goal, R → ¬P. The tactic state is a valuable tool for guiding you as you are figuring out a proof; whenever you are trying to decide on the next step of a proof, you should look at the tactic state to see what givens you have to work with and what goal you need to prove.\nFrom the givens h1 and h2 it shouldn’t be hard to prove P → ¬R, but the goal is R → ¬P. This suggests that we should prove the contrapositive of the goal. Type contrapos (indented by one tab, to match the indenting of done) to tell Lean that you want to replace the goal with its contrapositive. As soon as you type contrapos, Lean will update the tactic state to reflect the change in the goal. You should now see this:\n\n\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P := by\n  contrapos\n  **done::\n\n\nP Q R : Prop\nh1 : P → Q\nh2 : Q → ¬R\n⊢ P → ¬R\n\n\nIf you want to make your proof a little more readable, you could add a comment saying that the goal has been changed to P → ¬R. To prove the new goal, we will assume P and prove ¬R. So type assume h3 : P on a new line (after contrapos, but before done). Once again, the tactic state is immediately updated. Lean adds h3 : P as a new given, and it knows, without having to be told, that the goal should now be ¬R:\n\n\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P := by\n  contrapos           --Goal is now P → ¬R\n  assume h3 : P\n  **done::\n\n\nP Q R : Prop\nh1 : P → Q\nh2 : Q → ¬R\nh3 : P\n⊢ ¬R\n\n\nWe can now use modus ponens to infer Q from h1 : P → Q and h3 : P. As we saw earlier, this means that h1 h3 is a term-mode proof of Q. So on the next line, type have h4 : Q := h1 h3. To make an inference, you need to provide a justification, so := here is followed by the term-mode proof of Q. Usually we will use have to make easy inferences for which we can give simple term-mode proofs. (We’ll see later that it is also possible to use have to make an inference justified by a tactic-mode proof.) Of course, Lean updates the tactic state by adding the new given h4 : Q:\n\n\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P := by\n  contrapos           --Goal is now P → ¬R\n  assume h3 : P\n  have h4 : Q := h1 h3\n  **done::\n\n\nP Q R : Prop\nh1 : P → Q\nh2 : Q → ¬R\nh3 : P\nh4 : Q\n⊢ ¬R\n\n\nFinally, to complete the proof, we can infer the goal ¬R from h2 : Q → ¬R and h4 : Q, using the term-mode proof h2 h4. Type show ¬R from h2 h4 to complete the proof. You’ll notice several changes in the display: the error will disappear from the word done, a check mark will appear in the left margin next to the theorem, and the tactic state will say “No goals” to indicate that there is nothing left to prove:\n\n\ntheorem two_imp (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → ¬R) : R → ¬P := by\n  contrapos           --Goal is now P → ¬R\n  assume h3 : P\n  have h4 : Q := h1 h3\n  show ¬R from h2 h4\n  done\n\n\nNo goals\n\n\nCongratulations! You’ve written your first proof in tactic mode. If you move your cursor around in the proof, you will see that Lean always displays in the Infoview the tactic state at the point in the proof where the cursor is located. Try clicking on different lines of the proof to see how the tactic state changes over the course of the proof. If you want to try another example, you could try typing in the first example in this chapter. You will learn the most from this book if you continue to type the examples into Lean and see for yourself how the tactic state gets updated as the proof is written.\nIn each step of a tactic-mode proof, we invoke a tactic. In the proofs above, we have used four tactics: contrapos, assume, have, and show. If the goal is a conditional statement, the contrapos tactic replaces it with its contrapositive. If h is a given that is a conditional statement, then contrapos at h will replace h with its contrapositive. If the goal is a conditional statement P → Q, you can use the assume tactic to assume the antecedent P, and Lean will set the goal to be the consequent Q. You can use the have tactic to make an inference from your givens, as long as you can justify the inference with a proof. The show tactic is similar, but it is used to infer the goal, thus completing the proof. And we have learned how to use one rule of inference in term mode: modus ponens. In the rest of this book we will learn about other tactics and other term-mode rules.\nBefore continuing, it might be useful to summarize how you type statements into Lean. We have already told you how to type the symbols → and ¬, but you will want to know how to type all of the logical connectives. In each case, the command to produce the symbol must be followed by space or tab, but there is also a plain text alternative:\n\n\n\n\nSymbol\nHow To Type It\nPlain Text Alternative\n\n\n\n\n¬\n\\not or \\n\nNot\n\n\n∧\n\\and\n/\\\n\n\n∨\n\\or or \\v\n\\/\n\n\n→\n\\to or \\r or \\imp\n->\n\n\n↔︎\n\\iff or \\lr\n<->\n\n\n\n\nLean has conventions that it follows to interpret a logical statement when there are not enough parentheses to indicate how terms are grouped in the statement. For our purposes, the most important of these conventions is that P → Q → R is interpreted as P → (Q → R), not (P → Q) → R. The reason for this is simply that statements of the form P → (Q → R) come up much more often in proofs than statements of the form (P → Q) → R. (Lean also follows this “grouping-to-the-right” convention for ∧ and ∨, although this makes less of a difference, since these connectives are associative.) Of course, when in doubt about how to type a statement, you can always put in extra parentheses to avoid confusion.\nWe will be using tactics to apply several logical equivalences. Here are tactics corresponding to all of the logical laws listed in Chapter 1, as well as one additional law:\n\n\n\n\n\n\n\n\n\n\n\nLogical Law\nTactic\n\nTransformation\n\n\n\n\n\nContrapositive Law\ncontrapos\nP → Q\nis changed to\n¬Q → ¬P\n\n\nDe Morgan’s Laws\ndemorgan\n¬(P ∧ Q)\nis changed to\n¬P ∨ ¬Q\n\n\n\n\n¬(P ∨ Q)\nis changed to\n¬P ∧ ¬Q\n\n\n\n\nP ∧ Q\nis changed to\n¬(¬P ∨ ¬Q)\n\n\n\n\nP ∨ Q\nis changed to\n¬(¬P ∧ ¬Q)\n\n\nConditional Laws\nconditional\nP → Q\nis changed to\n¬P ∨ Q\n\n\n\n\n¬(P → Q)\nis changed to\nP ∧ ¬Q\n\n\n\n\nP ∨ Q\nis changed to\n¬P → Q\n\n\n\n\nP ∧ Q\nis changed to\n¬(P → ¬Q)\n\n\nDouble Negation Law\ndouble_neg\n¬¬P\nis changed to\nP\n\n\nBiconditional Negation Law\nbicond_neg\n¬(P ↔︎ Q)\nis changed to\n¬P ↔︎ Q\n\n\n\n\nP ↔︎ Q\nis changed to\n¬(¬P ↔︎ Q)\n\n\n\n\nAll of these tactics work the same way as the contrapos tactic: by default, the transformation is applied to the goal; to apply it to a given h, add at h after the tactic name."
  },
  {
    "objectID": "IntroLean.html#types",
    "href": "IntroLean.html#types",
    "title": "Introduction to Lean",
    "section": "Types",
    "text": "Types\nAll of our examples so far have just used letters to stand for propositions. To prove theorems with mathematical content, we will need to introduce one more idea.\nThe underlying theory on which Lean is based is called type theory. We won’t go very deeply into type theory, but we will need to make use of the central idea of the theory: every variable in Lean must have a type. What this means is that, when you introduce a variable to stand for a mathematical object in a theorem or proof, you must specify what type of object the variable stands for. We have already seen this idea in action: in our first example, the expression (P Q R : Prop) told Lean that the variables P, Q, and R have type Prop, which means they stand for propositions. There are types for many kinds of mathematical objects. For example, Nat is the type of natural numbers, and Real is the type of real numbers. So if you want to state a theorem about real numbers x and y, the statement of your theorem might start with (x y : Real). You must include such a type declaration before you can use the variables x and y as free variables in the hypotheses or conclusion of your theorem.\nWhat about sets? If you want to prove a theorem about a set A, can you say that A has type Set? No, Lean is fussier than that. Lean wants to know, not only that A is a set, but also what the type of the elements of A is. So you can say that A has type Set Nat if A is a set whose elements are natural numbers, or Set Real if it is a set of real numbers, or even Set (Set Nat) if it is a set whose elements are sets of natural numbers. Here is an example of a simple theorem about sets; it is a simplified version of Example 3.2.5 in HTPI. To type the symbols ∈, ∉, and \\ in this theorem, type \\in, \\notin, and \\\\, respectively.\n\n\ntheorem Example_3_2_5_simple\n    (B C : Set Nat) (a : Nat)\n    (h1 : a ∈ B) (h2 : a ∉ B \\ C) : a ∈ C := by\n\n  **done::\n\n\nB C : Set ℕ\na : ℕ\nh1 : a ∈ B\nh2 : a ∉ B \\ C\n⊢ a ∈ C\n\n\nThe second line of this theorem statement declares that the variables B and C stand for sets of natural numbers, and a stands for a natural number. The third line states the two hypotheses of the theorem, a ∈ B and a ∉ B \\ C, and the conclusion, a ∈ C. (Note that Lean occasionally writes things slightly differently in the tactic state. In this case, Lean has written ℕ instead of Nat.)\nTo figure out this proof, we’ll imitate the reasoning in Example 3.2.5 in HTPI. We begin by writing out the meaning of the given h2. Fortunately, we have a tactic for that. The tactic define writes out the definition of the goal, and as usual we can add at to apply the tactic to a given rather than the goal. Here’s the situation after using the tactic define at h2:\n\n\ntheorem Example_3_2_5_simple\n    (B C : Set Nat) (a : Nat)\n    (h1 : a ∈ B) (h2 : a ∉ B \\ C) : a ∈ C := by\n  define at h2       --Now h2 : ¬(a ∈ B ∧ a ∉ C)\n  **done::\n\n\nB C : Set ℕ\na : ℕ\nh1 : a ∈ B\nh2 : ¬(a ∈ B ∧ a ∉ C)\n⊢ a ∈ C\n\n\nLooking at the tactic state, we see that Lean has written out the meaning of set difference in h2. And now we can see that, as in Example 3.2.5 in HTPI, we can put h2 into a more useful form by applying first one of De Morgan’s laws to rewrite it as a ∉ B ∨ a ∈ C and then a conditional law to change it to a ∈ B → a ∈ C:\n\n\ntheorem Example_3_2_5_simple\n    (B C : Set Nat) (a : Nat)\n    (h1 : a ∈ B) (h2 : a ∉ B \\ C) : a ∈ C := by\n  define at h2       --Now h2 : ¬(a ∈ B ∧ a ∉ C)\n  demorgan at h2     --Now h2 : a ∉ B ∨ a ∈ C\n  conditional at h2  --Now h2 : a ∈ B → a ∈ C\n  **done::\n\n\nB C : Set ℕ\na : ℕ\nh1 : a ∈ B\nh2 : a ∈ B → a ∈ C\n⊢ a ∈ C\n\n\nOccasionally, you may feel that the application of two tactics one after the other should be thought of as a single step. To allow for this, Lean lets you put two tactics on the same line, separated by a semicolon. For example, in this proof you could write the use of De Morgan’s law and the conditional law as a single step by writing demorgan at h2; conditional at h2. Now the rest is easy: we can apply modus ponens to reach the goal:\n\n\ntheorem Example_3_2_5_simple\n    (B C : Set Nat) (a : Nat)\n    (h1 : a ∈ B) (h2 : a ∉ B \\ C) : a ∈ C := by\n  define at h2       --Now h2 : ¬(a ∈ B ∧ a ∉ C)\n  demorgan at h2; conditional at h2\n                     --Now h2 : a ∈ B → a ∈ C\n  show a ∈ C from h2 h1\n  done\n\n\nNo goals\n\n\nThere is one unfortunate feature of this theorem: We have stated it as a theorem about sets of natural numbers, but the proof has nothing to do with natural numbers. Exactly the same reasoning would prove a similar theorem about sets of real numbers, or sets of objects of any other type. Do we need to write a different theorem for each of these cases? No, fortunately there is a way to write one theorem that covers all the cases:\ntheorem Example_3_2_5_simple_general\n    (U : Type) (B C : Set U) (a : U)\n    (h1 : a ∈ B) (h2 : a ∉ B \\ C) : a ∈ C := by\nIn this version of the theorem, we have introduced a new variable U, whose type is … Type! So U can stand for any type. You can think of the variable U as playing the role of the universe of discourse, an idea that was introduced in Section 1.3 of HTPI. The sets B and C contain elements from that universe of discourse, and a belongs to the universe. You can prove the new version of the theorem by using exactly the same sequence of tactics as before."
  },
  {
    "objectID": "Chap3.html",
    "href": "Chap3.html",
    "title": "3  Proofs",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\mathbin{∆}}\n$$"
  },
  {
    "objectID": "Chap3.html#proofs-involving-negations-and-conditionals",
    "href": "Chap3.html#proofs-involving-negations-and-conditionals",
    "title": "3  Proofs",
    "section": "3.1 & 3.2. Proofs Involving Negations and Conditionals",
    "text": "3.1 & 3.2. Proofs Involving Negations and Conditionals\nSections 3.1 and 3.2 of How To Prove It present strategies for dealing with givens and goals involving negations and conditionals. We restate those strategies here, and explain how to use them with Lean.\nSection 3.1 gives two strategies for proving a goal of the form P → Q (HTPI pp. 95, 96):\n\nTo prove a goal of the form P → Q:\n\nAssume P is true and prove Q.\nAssume Q is false and prove that P is false.\n\nWe’ve already seen how to carry out both of these strategies in Lean. For the first strategy, use the assume tactic to introduce the assumption P and assign an identifier to it; Lean will automatically set Q as the goal. We can summarize the effect of using this strategy by showing how the tactic state changes if you use the tactic assume h : P:\n\n\n>> ⋮\n⊢ P → Q\n\n\n>> ⋮\nh : P\n⊢ Q\n\n\nThe second strategy is justified by the contrapositive law. In Lean, you can use the contrapos tactic to rewrite the goal as ¬Q → ¬P and then use the tactic assume h : ¬Q. The net effect of these two tactics is:\n\n\n>> ⋮\n⊢ P → Q\n\n\n>> ⋮\nh : ¬Q\n⊢ ¬P\n\n\nSection 3.2 gives two strategies for using givens of the form P → Q, with the second once again being a variation on the first based on the contrapositive law (HTPI p. 108):\n\n\nTo use a given of the form P → Q:\n\nIf you are also given P, or you can prove that P is true, then you can use this given to conclude that Q is true.\nIf you are also given ¬Q, or you can prove that Q is false, then you can use this given to conclude that P is false.\n\nThe first strategy is the modus ponens rule of inference, and we saw in the last chapter that if you have h1 : P → Q and h2 : P, then h1 h2 is a (term-mode) proof of Q; often we use this rule with the have or show tactic. For the second strategy, if you have h1 : P → Q and h2 : ¬Q, then the contrapos at h1 tactic will change h1 to h1 : ¬Q → ¬P, and then h1 h2 will be a proof of ¬P.\nAll of the strategies listed above for working with conditional statements as givens or goals were illustrated in examples in the last chapter.\nSection 3.2 of HTPI offers two strategies for proving negative goals (HTPI pp. 101, 102):\n\n\nTo prove a goal of the form ¬P:\n\nReexpress the goal in some other form.\nUse proof by contradiction: assume P is true and try to deduce a contradiction.\n\nFor the first strategy, the tactics demorgan, conditional, double_neg, and bicond_neg may be useful, and we saw how those tactics work in the last chapter. But how do you write a proof by contradiction in Lean? The answer is to use a tactic called by_contra. If the goal is ¬P, then the tactic by_contra h will introduce the assumption h : P and set the goal to be False, like this:\n\n\n>> ⋮\n⊢ ¬P\n\n\n>> ⋮\nh : P\n⊢ False\n\n\nIn Lean, False represents a statement that is always false—that is, a contradiction, as that term is defined in Section 1.2 of HTPI. The by_contra tactic can actually be used even if the goal is not a negative statement. If the goal is a statement P that is not a negative statement, then by_contra h will initiate a proof by contradiction by introducing the assumption h : ¬P and setting the goal to be False.\nYou will usually complete a proof by contradiction by deducing two contradictory statements—say, h1 : Q and h2 : ¬Q. But how do you convince Lean that the proof is over? You must be able to prove the goal False from the two givens h1 and h2. There are two ways to do this. The first is based on the fact that Lean treats a statement of the form ¬Q as meaning the same thing as Q → False. This makes sense, because these statements are logically equivalent, as shown by the following truth table:\n\n\n\n\nQ\n¬Q\n(Q\n→\nFalse)\n\n\n\n\nF\nT\nF\nT\n    F\n\n\nT\nF\nT\nF\n    F\n\n\n\n\nThinking of h2 : ¬Q as meaning h2 : Q → False, we can combine it with h1 : Q using modus ponens to deduce False. In other words, h2 h1 is a proof of False.\nBut there is a second way of completing the proof that it is worthwhile to know about. From contradictory statements h1 : Q and h2 : ¬Q you can validly deduce any statement. This follows from the definition of a valid argument in Section 1.1 of HTPI. According to that definition, you can validly infer a conclusion R from premises h1 : Q and h2 : ¬Q if the premises cannot both be true without the conclusion also being true. In this case, that standard is met, for the simple reason that the premises cannot both be true! (This gives part of the answer to exercise 18 in Section 1.2 of HTPI.) Thus, Lean has a rule that allows you to prove any statement from contradictory premises. If you have h1 : Q and h2 : ¬Q, then Lean will recognize absurd h1 h2 as a (term-mode) proof of any statement.\nTo summarize, if you have h1 : Q and h2 : ¬Q, then there are two ways to prove False. Lean will recognize h2 h1 as a proof of False, and it will recognize absurd h1 h2 as a proof of any statement, including False. Notice the difference in the order in which h1 and h2 are listed in these two proofs: In the first one, the negative statement h2 must come first, just as the conditional statement must come first in an application of modus ponens. But in a proof using absurd, the negative statement must come second.\nTo illustrate proof by contradiction in Lean, let’s redo our first example from the last Chapter in a different way. That example was based on Example 3.2.4 in HTPI. We’ll begin with the same first two steps, introducing two assumptions.\n\n\ntheorem Example_3_2_4_v2 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  **done::\n\n\nP Q R : Prop\nh : P → Q → R\nh2 : ¬R\nh3 : P\n⊢ ¬Q\n\n\nNow the goal is a negative statement, so we use the tactic by_contra h4 to introduce the assumption h4 : Q and set the goal to be False:\n\n\ntheorem Example_3_2_4_v2 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n  **done::\n\n\nP Q R : Prop\nh : P → Q → R\nh2 : ¬R\nh3 : P\nh4 : Q\n⊢ False\n\n\nUsing the givens h, h3, and h4 we can deduce first Q → R and then R by two applications of modus ponens:\n\n\ntheorem Example_3_2_4_v2 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n  have h5 : Q → R := h h3\n  have h6 : R := h5 h4\n  **done::\n\n\nP Q R : Prop\nh : P → Q → R\nh2 : ¬R\nh3 : P\nh4 : Q\nh5 : Q → R\nh6 : R\n⊢ False\n\n\nNow we have a contradiction: h2 : ¬R and h6 : R. To complete the proof, we deduce False from these two givens. Either h2 h6 or absurd h6 h2 would be accepted by Lean as a proof of False:\n\n\ntheorem Example_3_2_4_v2 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n  have h5 : Q → R := h h3\n  have h6 : R := h5 h4\n  show False from h2 h6\n  done\n\n\nNo goals\n\n\nFinally, we have two strategies for using a given that is a negative statement (HTPI pp. 105, 108):\n\n\nTo use a given of the form ¬P:\n\nReexpress the given in some other form.\nIf you are doing a proof by contradiction, you can achieve a contradiction by proving P, since that would contradict the given ¬P.\n\nOf course, strategy 1 suggests the use of the demorgan, conditional, double_neg, and bicond_neg tactics, if they apply. For strategy 2, if you are doing a proof by contradiction and you have a given h : ¬P, then the tactic contradict h will set the goal to be P, which will complete the proof by contradicting h. In fact, this tactic can be used with any given; if you have a given h : P, where P is not a negative statement, then contradict h will set the goal to be ¬P. You can also follow the word contradict with a proof that is more complicated than a single identifier. For example, if you have givens h1 : P → ¬Q and h2 : P, then h1 h2 is a proof of ¬Q, so the tactic contradict h1 h2 will set the goal to be Q.\nIf you’re not doing a proof by contradiction, then the tactic contradict h with h' will first initiate a proof by contradiction by assuming the negation of the goal, giving that assumption the identifier h', and then it will set the goal to be the negation of the statement proven by h. In other words, contradict h with h' is shorthand for by_contra h'; contradict h.\nWe can illustrate this with yet another way to write the proof from Example 3.2.4. Our first three steps will be the same as last time:\n\n\ntheorem Example_3_2_4_v3 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n  **done::\n\n\nP Q R : Prop\nh : P → Q → R\nh2 : ¬R\nh3 : P\nh4 : Q\n⊢ False\n\n\nSince we are now doing a proof by contradiction and the given h2 : ¬R is a negative statement, a likely way to proceed is to try to prove R, which would contradict h2. So we use the tactic contradict h2:\n\n\ntheorem Example_3_2_4_v3 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n  contradict h2\n  **done::\n\n\nP Q R : Prop\nh : P → Q → R\nh2 : ¬R\nh3 : P\nh4 : Q\n⊢ R\n\n\nAs before, we can now prove R by combining h, h3, and h4. In fact, we could do it in one step: by modus ponens, h h3 is a proof of Q → R, and therefore, by another application of modus ponens, (h h3) h4 is a proof of R. The parentheses here are not necessary; Lean will interpret h h3 h4 as (h h3) h4, so we can complete the proof like this:\n\n\ntheorem Example_3_2_4_v3 (P Q R : Prop)\n    (h : P → (Q → R)) : ¬R → (P → ¬Q) := by\n  assume h2 : ¬R\n  assume h3 : P\n  by_contra h4\n  contradict h2\n  show R from h h3 h4\n  done\n\n\nNo goals\n\n\nYou could shorten this proof slightly by replacing the lines by_contra h4 and contradict h2 with the single line contradict h2 with h4.\nThere is one more idea that is introduced in Section 3.2 of HTPI. The last example in that section illustrates how you can sometimes use rules of inference to work backwards. Here’s a similar example in Lean:\n\n\ntheorem Like_Example_3_2_5\n    (U : Type) (A B C : Set U) (a : U)\n    (h1 : a ∈ A) (h2 : a ∉ A \\ B)\n    (h3 : a ∈ B → a ∈ C) : a ∈ C := by\n\n  **done::\n\n\nU : Type\nA B C : Set U\na : U\nh1 : a ∈ A\nh2 : a ∉ A \\ B\nh3 : a ∈ B → a ∈ C\n⊢ a ∈ C\n\n\nThe goal is a ∈ C, and the only given that even mentions C is h3 : a ∈ B → a ∈ C. If only we could prove a ∈ B, then we could apply h3, using modus ponens, to reach our goal. So it would make sense to work toward the goal of proving a ∈ B.\nTo get Lean to use this proof strategy, we use the tactic apply h3 _. The underscore here represents a blank to be filled in by Lean. You might think of this tactic as asking Lean the question: If we want h3 _ to be a proof of the goal a ∈ C, what do we have to put in the blank? Lean is able to figure out that the answer is: a proof of a ∈ B. So it sets the goal to be a ∈ B, since a proof of that goal, when inserted into the blank in h3 _, would prove the original goal a ∈ C:\n\n\ntheorem Like_Example_3_2_5\n    (U : Type) (A B C : Set U) (a : U)\n    (h1 : a ∈ A) (h2 : a ∉ A \\ B)\n    (h3 : a ∈ B → a ∈ C) : a ∈ C := by\n  apply h3 _\n  **done::\n\n\nU : Type\nA B C: Set U\na : U\nh1 : a ∈ A\nh2 : a ∉ A \\ B\nh3 : a ∈ B → a ∈ C\n⊢ a ∈ B\n\n\nOur situation now is very much like the one in the theorem Example_3_2_5_Simple in the previous chapter, and the rest of our proof will be similar to the proof there. The given h2 is a negative statement (a ∉ A \\ B is shorthand for ¬a ∈ A \\ B), so, as suggested by our first strategy for using negative givens, we reexpress it as an equivalent positive statement. Writing out the definition of set difference, h2 means ¬(a ∈ A ∧ a ∉ B), and then one of De Morgan’s laws and a conditional law allow us to rewrite it first as a ∉ A ∨ a ∈ B and then as a ∈ A → a ∈ B. Of course, we have tactics to accomplish all of these reexpressions:\n\n\ntheorem Like_Example_3_2_5\n    (U : Type) (A B C : Set U) (a : U)\n    (h1 : a ∈ A) (h2 : a ∉ A \\ B)\n    (h3 : a ∈ B → a ∈ C) : a ∈ C := by\n  apply h3 _\n  define at h2\n  demorgan at h2; conditional at h2\n  **done::\n\n\nU : Type\nA B C : Set U\na : U\nh1 : a ∈ A\nh2 : a ∈ A → a ∈ B\nh3 : a ∈ B → a ∈ C\n⊢ a ∈ B\n\n\nAnd now it is easy to complete the proof by applying modus ponens, using h2 and h1:\n\n\ntheorem Like_Example_3_2_5\n    (U : Type) (A B C : Set U) (a : U)\n    (h1 : a ∈ A) (h2 : a ∉ A \\ B)\n    (h3 : a ∈ B → a ∈ C) : a ∈ C := by\n  apply h3 _\n  define at h2\n  demorgan at h2; conditional at h2\n  show a ∈ B from h2 h1\n  done\n\n\nNo goals\n\n\nWe will see many more uses of the apply tactic later in this book.\nSections 3.1 and 3.2 of HTPI contain several proofs that involve algebraic reasoning. Although one can do such proofs in Lean, it requires ideas that we are not ready to introduce yet. So for the moment we will stick to proofs involving only logic and set theory.\n\n\nExercises\nFill in proofs of the following theorems. All of them are based on exercises in HTPI. You can find these exercises in the file Chap3Ex.lean in the HTPI Lean package, and you can enter your solutions in that file.\n\ntheorem Exercise_3_2_1a (P Q R : Prop)\n    (h1 : P → Q) (h2 : Q → R) : P → R := by\n  \n  **done::\n\n\ntheorem Exercise_3_2_1b (P Q R : Prop)\n    (h1 : ¬R → (P → ¬Q)) : P → (Q → R) := by\n  \n  **done::\n\n\ntheorem Exercise_3_2_2a (P Q R : Prop)\n    (h1 : P → Q) (h2 : R → ¬Q) : P → ¬R := by\n  \n  **done::\n\n\ntheorem Exercise_3_2_2b (P Q : Prop)\n    (h1 : P) : Q → ¬(Q → ¬P) := by\n  \n  **done::"
  },
  {
    "objectID": "Chap3.html#proofs-involving-quantifiers",
    "href": "Chap3.html#proofs-involving-quantifiers",
    "title": "3  Proofs",
    "section": "3.3. Proofs Involving Quantifiers",
    "text": "3.3. Proofs Involving Quantifiers\nIn the notation used in HTPI, if \\(P(x)\\) is a statement about \\(x\\), then \\(\\forall x\\, P(x)\\) means “for all \\(x\\), \\(P(x)\\),” and \\(\\exists x\\, P(x)\\) means “there exists at least one \\(x\\) such that \\(P(x)\\).” The letter \\(P\\) here does not stand for a proposition; it is only when it is applied to some object \\(x\\) that we get a proposition. We will say that \\(P\\) is a predicate, and when we apply \\(P\\) to an object \\(x\\) we get the proposition \\(P(x)\\). You might want to think of the predicate \\(P\\) as representing some property that an object might have, and the proposition \\(P(x)\\) asserts that \\(x\\) has that property.\nTo use a predicate in Lean, you must tell Lean the type of objects to which it applies. If U is a type, then Pred U is the type of predicates that apply to objects of type U. If P has type Pred U (that is, P is a predicate applying to objects of type U) and x has type U, then to apply P to x we just write P x (with a space but no parentheses). Thus, if we have P : Pred U and x : U, then P x is an expression of type Prop. That is, P x is a proposition, and its meaning is that x has the property represented by the predicate P.\nThere are a few differences between the way quantified statements are written in HTPI and the way they are written in Lean. First of all, when we apply a quantifier to a variable in Lean we will specify the type of the variable explicitly. Also, Lean requires that after specifying the variable and its type, you must put a comma before the proposition to which the quantifier is applied. Thus, if P has type Pred U, then to say that P holds for all objects of type U we would write ∀ (x : U), P x. Similarly, ∃ (x : U), P x is the proposition asserting that there exists at least one x of type U such that P x.\nAnd there is one more important difference between the way quantified statements are written in HTPI and Lean. In HTPI, a quantifier is interpreted as applying to as little as possible. Thus, \\(\\forall x\\, P(x) \\wedge Q(x)\\) is interpreted as \\((\\forall x\\, P(x)) \\wedge Q(x)\\); if you want the quantifier \\(\\forall x\\) to apply to the entire statement \\(P(x) \\wedge Q(x)\\) you must use parentheses and write \\(\\forall x(P(x) \\wedge Q(x))\\). The convention in Lean is exactly the opposite: a quantifier applies to as much as possible. Thus, Lean will interpret ∀ (x : U), P x ∧ Q x as meaning ∀ (x : U), (P x ∧ Q x). If you want the quantifier to apply to only P x, then you must use parentheses and write (∀ (x : U), P x) ∧ Q x.\nWith this preparation, we are ready to consider how to write proofs involving quantifiers in Lean. The most common way to prove a goal of the form ∀ (x : U), P x is to use the following strategy (HTPI p. 114):\n\nTo prove a goal of the form ∀ (x : U), P x:\n\nLet x stand for an arbitrary object of type U and prove P x. If the letter x is already being used in the proof to stand for something, then you must choose an unused variable, say y, to stand for the arbitrary object, and prove P y.\n\nTo do this in Lean, you should use the tactic fix x : U, which tells Lean to treat x as standing for some fixed but arbitrary object of type U. This has the following effect on the tactic state:\n\n\n>> ⋮\n⊢ ∀ (x : U), P x\n\n\n>> ⋮\nx : U\n⊢ P x\n\n\nTo use a given of the form ∀ (x : U), P x, we usually apply a rule of inference called universal instantiation, which is described by the following proof strategy (HTPI p. 121):\n\n\nTo use a given of the form ∀ (x : U), P x:\n\nYou may plug in any value of type U, say a, for x and use this given to conclude that P a is true.\n\nThis strategy says that if you have h : ∀ (x : U), P x and a : U, then you can infer P a. Indeed, in this situation Lean will recognize h a as a proof of P a. For example, you can write have h' : P a := h a in a Lean tactic-mode proof, and Lean will add h' : P a to the tactic state. Note that a here need not be simply a variable; it can be any expression denoting an object of type U.\nLet’s try these strategies out in a Lean proof. In Lean, if you don’t want to give a theorem a name, you can simply call it an example rather than a theorem, and then there is no need to give it a name. In the following example, you can enter the symbol ∀ by typing \\forall or \\all, and you can enter ∃ by typing \\exists or \\ex.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x → ¬Q x)\n    (h2 : ∀ (x : U), Q x) :\n    ¬∃ (x : U), P x := by\n  \n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x → ¬Q x\nh2 : ∀ (x : U), Q x\n⊢ ¬∃ (x : U), P x\n\n\nTo use the givens h1 and h2, we will probably want to use universal instantiation. But to do that we would need an object of type U to plug in for x in h1 and h2, and there is no object of type U in the tactic state. So at this point, we can’t apply universal instantiation to h1 and h2. We should watch for an object of type U to come up in the course of the proof, and consider applying universal instantiation if one does. Until then, we turn our attention to the goal.\nThe goal is a negative statement, so we begin by reexpressing it as an equivalent positive statement, using a quantifier negation law. The tactic quant_neg applies a quantifier negation law to rewrite the goal. As with the other tactics for applying logical equivalences, you can write quant_neg at h if you want to apply a quantifier negation law to a given h. The effect of the tactic can be summarized as follows:\n\n\n\n\n\nquant_neg Tactic\n\n\n\n\n\n¬∀ (x : U), P x\nis changed to\n∃ (x : U), ¬P x\n\n\n¬∃ (x : U), P x\nis changed to\n∀ (x : U), ¬P x\n\n\n∀ (x : U), P x\nis changed to\n¬∃ (x : U), ¬P x\n\n\n∃ (x : U), P x\nis changed to\n¬∀ (x : U), ¬P x\n\n\n\n\nUsing the quant_neg tactic leads to the following result.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x → ¬Q x)\n    (h2 : ∀ (x : U), Q x) :\n    ¬∃ (x : U), P x := by\n  quant_neg     --Goal is now ∀ (x : U), ¬P x\n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x → ¬Q x\nh2 : ∀ (x : U), Q x\n⊢ ∀ (x : U), ¬P x\n\n\nNow the goal starts with ∀, so we use the strategy above and introduce an arbitrary object of type U. Since the variable x occurs as a bound variable in several statements in this theorem, it might be best to use a different letter for the arbitrary object; this isn’t absolutely necessary, but it may help to avoid confusion. So our next tactic is fix y : U.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x → ¬Q x)\n    (h2 : ∀ (x : U), Q x) :\n    ¬∃ (x : U), P x := by\n  quant_neg     --Goal is now ∀ (x : U), ¬P x\n  fix y : U\n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x → ¬Q x\nh2 : ∀ (x : U), Q x\ny : U\n⊢ ¬P y\n\n\nNow we have an object of type U in the tactic state, namely, y. So let’s try applying universal instantiation to h1 and h2 and see if it helps.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x → ¬Q x)\n    (h2 : ∀ (x : U), Q x) :\n    ¬∃ (x : U), P x := by\n  quant_neg     --Goal is now ∀ (x : U), ¬P x\n  fix y : U\n  have h3 : P y → ¬Q y := h1 y\n  have h4 : Q y := h2 y\n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x → ¬Q x\nh2 : ∀ (x : U), Q x\ny : U\nh3 : P y → ¬Q y\nh4 : Q y\n⊢ ¬P y\n\n\nWe’re almost done, because the goal now follows easily from h3 and h4. If we use the contrapositive law to rewrite h3 as Q y → ¬P y, then we can apply modus ponens to the rewritten h3 and h4 to reach the goal:\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x → ¬Q x)\n    (h2 : ∀ (x : U), Q x) :\n    ¬∃ (x : U), P x := by\n  quant_neg     --Goal is now ∀ (x : U), ¬P x\n  fix y : U\n  have h3 : P y → ¬Q y := h1 y\n  have h4 : Q y := h2 y\n  contrapos at h3  --Now h3 : Q y → ¬P y\n  show ¬P y from h3 h4\n  done\n\n\nNo goals\n\n\nOur next example is a theorem of set theory. You already know how to type a few set theory symbols in Lean, but you’ll need a few more for our next example. Here’s a summary of the most important set theory symbols and how to type them in Lean.\n\n\n\n\nSymbol\nHow To Type It\n\n\n\n\n∈\n\\in\n\n\n∉\n\\notin or \\inn\n\n\n⊆\n\\sub\n\n\n⊈\n\\subn\n\n\n=\n=\n\n\n≠\n\\ne\n\n\n∪\n\\union or \\cup\n\n\n∩\n\\inter or \\cap\n\n\n\\\n\\\\\n\n\n∆\n\\symmdiff\n\n\n∅\n\\emptyset\n\n\n𝒫\n\\powerset\n\n\n\n\nWith this preparation, we can turn to our next example.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  \n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ x ∈ A, x ∉ B\n⊢ A ⊆ C\n\n\nNotice that in the Infoview, Lean has written h2 as ∀ x ∈ A, x ∉ B, using a bounded quantifier. As explained in Section 2.2 of HTPI (see p. 72), this is a shorter way of writing the statement ∀ (x : U), x ∈ A → x ∉ B. We begin by using the define tactic to write out the definition of the goal.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal : ∀ ⦃a : U⦄, a ∈ A → a ∈ C\n  **done::\n\n\nU : Type\nA B C: Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ x ∈ A, x ∉ B\n⊢ ∀ ⦃a : U⦄,\n>>  a ∈ A → a ∈ C\n\n\nNotice that Lean’s definition of the goal starts with ∀ ⦃a : U⦄, not ∀ (a : U). Why did Lean use those funny double braces rather than parentheses? We’ll return to that question shortly. The difference doesn’t affect our next steps, which are to introduce an arbitrary object y of type U and assume y ∈ A.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal : ∀ ⦃a : U⦄, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ x ∈ A, x ∉ B\ny : U\nh3 : y ∈ A\n⊢ y ∈ C\n\n\nNow we can combine h2 and h3 to conclude that y ∉ B. Since we have y : U, by universal instantiation, h2 y is a proof of y ∈ A → y ∉ B, and therefore by modus ponens, h2 y h3 is a proof of y ∉ B.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal : ∀ ⦃a : U⦄, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n  have h4 : y ∉ B := h2 y h3\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ x ∈ A, x ∉ B\ny : U\nh3 : y ∈ A\nh4 : y ∉ B\n⊢ y ∈ C\n\n\nWe should be able to use similar reasoning to combine h1 and h3, if we first write out the definition of h1.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal : ∀ ⦃a : U⦄, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n  have h4 : y ∉ B := h2 y h3\n  define at h1  --h1 : ∀ ⦃a : U⦄, a ∈ A → a ∈ B ∪ C\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : ∀ ⦃a : U⦄,\n>>  a ∈ A → a ∈ B ∪ C\nh2 : ∀ x ∈ A, x ∉ B\ny : U\nh3 : y ∈ A\nh4 : y ∉ B\n⊢ y ∈ C\n\n\nOnce again, Lean has used double braces to define h1, and now we are ready to explain what they mean. If the definition had been h1 : ∀ (a : U), a ∈ A → a ∈ B ∪ C, then exactly as in the previous step, h1 y h3 would be a proof of y ∈ B ∪ C. The use of double braces in the definition h1 : ∀ ⦃a : U⦄, a ∈ A → a ∈ B ∪ C means that you don’t need to tell Lean that y is being plugged in for a in the universal instantiation step; Lean will figure that out on its own. Thus, you can just write h1 h3 as a proof of y ∈ B ∪ C. Indeed, if you write h1 y h3 then you will get an error message, because Lean expects not to be told what to plug in for a. You might think of the definition of h1 as meaning h1 : _ ∈ A → _ ∈ B ∪ C, where the blanks can be filled in with anything of type U (with the same thing being put in both blanks). When you ask Lean to apply modus ponens by combining this statement with h3 : y ∈ A, Lean figures out that in order for modus ponens to apply, the blanks must be filled in with y.\nIn this situation, the a in h1 is called an implicit argument. What this means is that, when h1 is applied to make an inference in a proof, the value to be assigned to a is not specified explicitly; rather, the value is inferred by Lean. We will see many more examples of implicit arguments later in this book. In fact, there are two slightly different kinds of implicit arguments in Lean. One kind is indicated using the double braces ⦃ ⦄ used in this example, and the other is indicated using curly braces, { }. The difference between these two kinds of implicit arguments won’t be important in this book; all that will matter to us is that if you see either ∀ ⦃a : U⦄ or ∀ {a : U} rather than ∀ (a : U), then you must remember that a is an implicit argument.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal : ∀ ⦃a : U⦄, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n  have h4 : y ∉ B := h2 y h3\n  define at h1  --h1 : ∀ ⦃a : U⦄, a ∈ A → a ∈ B ∪ C\n  have h5 : y ∈ B ∪ C := h1 h3\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : ∀ ⦃a : U⦄,\n>>  a ∈ A → a ∈ B ∪ C\nh2 : ∀ x ∈ A, x ∉ B\ny : U\nh3 : y ∈ A\nh4 : y ∉ B\nh5 : y ∈ B ∪ C\n⊢ y ∈ C\n\n\nIf Lean was able to figure out that y should be plugged in for a in h1 in this step, couldn’t it have figured out that y should be plugged in for x in h2 in the previous have step? The answer is yes. Of course, in h2, x was not an implicit argument, so Lean wouldn’t automatically figure out what to plug in for x. But we could have asked it to figure it out by writing the proof in the previous step as h2 _ h3 rather than h2 y h3. In a term-mode proof, an underscore represents a blank to be filled in by Lean. Try changing the earlier step of the proof to have h4 : y ∉ B := h2 _ h3 and you will see that Lean will accept it. Of course, in this case this doesn’t save us any typing, but in some situations it is useful to let Lean figure out some part of a proof.\nLean’s ability to fill in blanks in term-mode proofs is limited. For example, if you try changing the previous step to have h4 : y ∉ B := h2 y _, you’ll get a red squiggle under the blank, and the error message in the Infoview pane will say don't know how to synthesize placeholder. In other words, Lean was unable to figure out how to fill in the blank in this case. In future proofs you might try replacing some expressions with blanks to get a feel for what Lean can and cannot figure out for itself.\nContinuing with the proof, we see that we’re almost done, because we can combine h4 and h5 to reach our goal. To see how, we first write out the definition of h5.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal : ∀ ⦃a : U⦄, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n  have h4 : y ∉ B := h2 y h3\n  define at h1  --h1 : ∀ ⦃a : U⦄, a ∈ A → a ∈ B ∪ C\n  have h5 : y ∈ B ∪ C := h1 h3\n  define at h5  --h5 : y ∈ B ∨ y ∈ C\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : ∀ ⦃a : U⦄,\n>>  a ∈ A → a ∈ B ∪ C\nh2 : ∀ x ∈ A, x ∉ B\ny : U\nh3 : y ∈ A\nh4 : y ∉ B\nh5 : y ∈ B ∨ y ∈ C\n⊢ y ∈ C\n\n\nA conditional law will convert h5 to y ∉ B → y ∈ C, and then modus ponens with h4 will complete the proof.\n\n\nexample (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ∀ (x : U), x ∈ A → x ∉ B) : A ⊆ C := by\n  define  --Goal : ∀ ⦃a : U⦄, a ∈ A → a ∈ C\n  fix y : U\n  assume h3 : y ∈ A\n  have h4 : y ∉ B := h2 y h3\n  define at h1  --h1 : ∀ ⦃a : U⦄, a ∈ A → a ∈ B ∪ C\n  have h5 : y ∈ B ∪ C := h1 h3\n  define at h5  --h5 : y ∈ B ∨ y ∈ C\n  conditional at h5  --h5 : y ∉ B → y ∈ C\n  show y ∈ C from h5 h4\n  done\n\n\nNo goals\n\n\nNext we turn to strategies for working with existential quantifiers (HTPI p. 118).\n\n\nTo prove a goal of the form ∃ (x : U), P x:\n\nFind a value of x, say a, for which you think P a is true, and prove P a.\n\nThis strategy is based on the fact that if you have a : U and h : P a, then you can infer ∃ (x : U), P x. Indeed, in this situation the expression Exists.intro a h is a Lean term-mode proof of ∃ (x : U), P x. The name Exists.intro indicates that this is a rule for introducing an existential quantifier.\nNote that, as with the universal instantiation rule, a here can be any expression denoting an object of type U; it need not be simply a variable. For example, if A and B have type Set U, F has type Set (Set U), and you have a given h : A ∪ B ∈ F, then Exists.intro (A ∪ B) h is a proof of ∃ (x : Set U), x ∈ F.\nAs suggested by the strategy above, we will often want to use the Exists.intro rule in situations in which our goal is ∃ (x : U), P x and we have an object a of type U that we think makes P a true, but we don’t yet have a proof of P a. In that situation we can use the tactic apply Exists.intro a _. Recall that the apply tactic asks Lean to figure out what to put in the blank to turn Exists.intro a _ into a proof of the goal. Lean will figure out that what needs to go in the blank is a proof of P a, so it sets P a to be the goal. In other words, the tactic apply Exists.intro a _ has the following effect on the tactic state:\n\n\n>> ⋮\na : U\n⊢ ∃ (x : U), P x\n\n\n>> ⋮\na : U\n⊢ P a\n\n\nOur strategy for using an existential given is a rule that is called existential instantiation in HTPI (HTPI p. 120):\n\n\nTo use a given of the form ∃ (x : U), P x:\n\nIntroduce a new variable, say u, into the proof to stand for an object of type U for which P u is true.\n\nSuppose that, in a Lean proof, you have h : ∃ (x : U), P x. To apply the existential instantiation rule, you would use the tactic obtain (u : U) (h' : P u) from h. This tactic introduces into the tactic state both a new variable u of type U and also the identifier h' for the new given P u. Note that h can be any proof of a statement of the form ∃ (x : U), P x; it need not be just a single identifier.\nOften, if your goal is an existential statement ∃ (x : U), P x, you won’t be able to use the strategy above for existential goals right away, because you won’t know what object a to use in the tactic apply Exists.intro a _. You may have to wait until a likely candidate for a pops up in the course of the proof. On the other hand, it is usually best to use the obtain tactic right away if you have an existential given. This is illustrated in our next example.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  \n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ (y : U),\n>>  P x → ¬Q y\nh2 : ∃ (x : U), ∀ (y : U),\n>>  P x → Q y\n⊢ ∃ (x : U), ¬P x\n\n\nThe goal is the existential statement ∃ (x : U), ¬P x, and our strategy for existential goals says that we should try to find an object a of type U that we think would make the statement ¬P a true. But we don’t have any objects of type U in the tactic state, so it looks like we can’t use that strategy yet. Similarly, we can’t use the given h1 yet, since we have nothing to plug in for x in h1. However, h2 is an existential given, and we can use it right away.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ (y : U),\n>>  P x → ¬Q y\nh2 : ∃ (x : U), ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\n⊢ ∃ (x : U), ¬P x\n\n\nNow that we have a : U, we can apply universal instantiation to h1, plugging in a for x.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ (y : U),\n>>  P x → ¬Q y\nh2 : ∃ (x : U), ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\nh4 : ∃ (y : U), P a → ¬Q y\n⊢ ∃ (x : U), ¬P x\n\n\nBy the way, this is another case in which Lean could have figured out a part of the proof on its own. Try changing h1 a in the last step to h1 _, and you’ll see that Lean will be able to figure out how to fill in the blank.\nOur new given h4 is another existential statement, so again we use it right away to introduce another object of type U. Since this object might not be the same as a, we must give it a different name. (Indeed, if you try to use the name a again, Lean will give you an error message.)\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n  obtain (b : U) (h5 : P a → ¬ Q b) from h4\n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ (y : U),\n>>  P x → ¬Q y\nh2 : ∃ (x : U), ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\nh4 : ∃ (y : U), P a → ¬Q y\nb : U\nh5 : P a → ¬Q b\n⊢ ∃ (x : U), ¬P x\n\n\nWe have not yet used h3. We could plug in either a or b for y in h3, but a little thought should show you that plugging in b is more useful.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n  obtain (b : U) (h5 : P a → ¬ Q b) from h4\n  have h6 : P a → Q b := h3 b\n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ (y : U),\n>>  P x → ¬Q y\nh2 : ∃ (x : U), ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\nh4 : ∃ (y : U), P a → ¬Q y\nb : U\nh5 : P a → ¬Q b\nh6 : P a → Q b\n⊢ ∃ (x : U), ¬P x\n\n\nNow look at h5 and h6. They show that P a leads to contradictory conclusions, ¬Q b and Q b. This means that P a must be false. We finally know what value of x to use to prove the goal.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n  obtain (b : U) (h5 : P a → ¬ Q b) from h4\n  have h6 : P a → Q b := h3 b\n  apply Exists.intro a _\n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ (y : U),\n>>  P x → ¬Q y\nh2 : ∃ (x : U), ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\nh4 : ∃ (y : U), P a → ¬Q y\nb : U\nh5 : P a → ¬Q b\nh6 : P a → Q b\n⊢ ¬P a\n\n\nSince the goal is now a negative statement that cannot be reexpressed as a positive statement, we use proof by contradiction.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n  obtain (b : U) (h5 : P a → ¬ Q b) from h4\n  have h6 : P a → Q b := h3 b\n  apply Exists.intro a _\n  by_contra h7\n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), ∃ (y : U),\n>>  P x → ¬Q y\nh2 : ∃ (x : U), ∀ (y : U),\n>>  P x → Q y\na : U\nh3 : ∀ (y : U), P a → Q y\nh4 : ∃ (y : U), P a → ¬Q y\nb : U\nh5 : P a → ¬Q b\nh6 : P a → Q b\nh7 : P a\n⊢ False\n\n\nNow h5 h7 is a proof of ¬Q b and h6 h7 is a proof of Q b, so h5 h7 (h6 h7) is a proof of False.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), ∃ (y : U), P x → ¬ Q y)\n    (h2 : ∃ (x : U), ∀ (y : U), P x → Q y) :\n    ∃ (x : U), ¬P x := by\n  obtain (a : U)\n    (h3 : ∀ (y : U), P a → Q y) from h2\n  have h4 : ∃ (y : U), P a → ¬ Q y := h1 a\n  obtain (b : U) (h5 : P a → ¬ Q b) from h4\n  have h6 : P a → Q b := h3 b\n  apply Exists.intro a _\n  by_contra h7\n  show False from h5 h7 (h6 h7)\n  done\n\n\nNo goals\n\n\nWe conclude this section with the theorem from Example 3.3.5 in HTPI. That theorem concerns a union of a family of sets. In HTPI, such a union is written using a large union symbol, \\(\\bigcup\\). Lean uses the symbol ⋃₀, which is entered by typing \\U0 (that is, backslash–capital U–zero). For an intersection of a family of sets, Lean uses ⋂₀, typed as \\I0.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀ F ⊆ B → F ⊆ 𝒫 B := by\n  \n  **done::\n\n\nU : Type\nB : Set U\nF : Set (Set U)\n⊢ ⋃₀ F ⊆ B → F ⊆ 𝒫 B\n\n\nNote that F has type Set (Set U), which means that it is a set whose elements are sets of objects of type U. Since the goal is a conditional statement, we assume the antecedent and set the consequent as our goal. We’ll also write out the definition of the new goal.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀ F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ⋃₀ F ⊆ B\n  define\n  **done::\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ⋃₀ F ⊆ B\n⊢ ∀ ⦃a : Set U⦄,\n>>  a ∈ F → a ∈ 𝒫 B\n\n\nBased on the form of the goal, we introduce an arbitrary object x of type Set U and assume x ∈ F. The new goal will be x ∈ 𝒫 B. The define tactic works out that this means x ⊆ B, which can be further expanded to ∀ ⦃a : U⦄, a ∈ x → a ∈ B.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀ F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ⋃₀ F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  **done::\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ⋃₀ F ⊆ B\nx : Set U\nh2 : x ∈ F\n⊢ ∀ ⦃a : U⦄,\n>>  a ∈ x → a ∈ B\n\n\nOnce again the form of the goal dictates our next steps: introduce an arbitrary y of type U and assume y ∈ x.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀ F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ⋃₀ F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n  **done::\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ⋃₀ F ⊆ B\nx : Set U\nh2 : x ∈ F\ny : U\nh3 : y ∈ x\n⊢ y ∈ B\n\n\nThe goal can be analyzed no further, so we turn to the givens. We haven’t used h1 yet. To see how to use it, we write out its definition.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀ F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ⋃₀ F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n  define at h1\n  **done::\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ∀ ⦃a : U⦄,\n>>  a ∈ ⋃₀ F → a ∈ B\nx : Set U\nh2 : x ∈ F\ny : U\nh3 : y ∈ x\n⊢ y ∈ B\n\n\nNow we see that we can try to use h1 to reach our goal. Indeed, h1 _ would be a proof of the goal if we could fill in the blank with a proof of y ∈ ∪₀F. So we use the apply h1 _ tactic.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀ F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ⋃₀ F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n  define at h1\n  apply h1 _\n  **done::\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ∀ ⦃a : U⦄,\n>>  a ∈ ⋃₀ F → a ∈ B\nx : Set U\nh2 : x ∈ F\ny : U\nh3 : y ∈ x\n⊢ y ∈ ⋃₀ F\n\n\nOnce again we have a goal that can be analyzed by using the define tactic.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀ F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ⋃₀ F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n  define at h1\n  apply h1 _\n  define\n  **done::\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ∀ ⦃a : U⦄,\n>>  a ∈ ⋃₀ F → a ∈ B\nx : Set U\nh2 : x ∈ F\ny : U\nh3 : y ∈ x\n⊢ ∃ t ∈ F, y ∈ t\n\n\nOur goal now is ∃ (t : Set U), t ∈ F ∧ y ∈ t, although once again Lean has used a bounded quantifier to write this in a shorter form. So we look for a value of t that will make the statement t ∈ F ∧ y ∈ t true. The givens h2 and h3 tell us that x is such a value, so as described earlier our next tactic should be apply Exists.intro x _.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀ F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ⋃₀ F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n  define at h1\n  apply h1 _\n  define\n  apply Exists.intro x _\n  **done::\n\n\nU : Type\nB : Set U\nF : Set (Set U)\nh1 : ∀ ⦃a : U⦄,\n>>  a ∈ ⋃₀ F → a ∈ B\nx : Set U\nh2 : x ∈ F\ny : U\nh3 : y ∈ x\n⊢ x ∈ F ∧ y ∈ x\n\n\nClearly the goal now follows from h2 and h3, but how do we write the proof in Lean? Since we need to introduce the “and” symbol ∧, you shouldn’t be surprised to learn that the rule we need is called And.intro. Proof strategies for statements involving “and” will be the subject of the next section.\n\n\ntheorem Example_3_3_5 (U : Type) (B : Set U)\n    (F : Set (Set U)) : ⋃₀ F ⊆ B → F ⊆ 𝒫 B := by\n  assume h1 : ⋃₀ F ⊆ B\n  define\n  fix x : Set U\n  assume h2 : x ∈ F\n  define\n  fix y : U\n  assume h3 : y ∈ x\n  define at h1\n  apply h1 _\n  define\n  apply Exists.intro x _\n  show x ∈ F ∧ y ∈ x from And.intro h2 h3\n  done\n\n\nNo goals\n\n\nYou might want to compare the Lean proof above to the way the proof was written in HTPI. Here are the theorem and proof from HTPI (HTPI p. 125):\n\nSuppose \\(B\\) is a set and \\(\\mathcal{F}\\) is a family of sets. If \\(\\bigcup\\mathcal{F} \\subseteq B\\) then \\(\\mathcal{F} \\subseteq \\mathscr{P}(B)\\).\n\n\nProof. Suppose \\(\\bigcup \\mathcal{F} \\subseteq B\\). Let \\(x\\) be an arbitrary element of \\(\\mathcal{F}\\). Let \\(y\\) be an arbitrary element of \\(x\\). Since \\(y \\in x\\) and \\(x \\in \\mathcal{F}\\), by the definition of \\(\\bigcup \\mathcal{F}\\), \\(y \\in \\bigcup \\mathcal{F}\\). But then since \\(\\bigcup \\mathcal{F} \\subseteq B\\), \\(y \\in B\\). Since \\(y\\) was an arbitrary element of \\(x\\), we can conclude that \\(x \\subseteq B\\), so \\(x \\in \\mathscr{P}(B)\\). But \\(x\\) was an arbitrary element of \\(\\mathcal{F}\\), so this shows that \\(\\mathcal{F} \\subseteq \\mathscr{P}(B)\\), as required.  □\n\n\n\nExercises\n\ntheorem Exercise_3_3_1\n    (U : Type) (P Q : Pred U) (h1 : ∃ (x : U), P x → Q x) :\n    (∀ (x : U), P x) → ∃ (x : U), Q x := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_8 (U : Type) (F : Set (Set U)) (A : Set U)\n    (h1 : A ∈ F) : A ⊆ ⋃₀ F := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_9 (U : Type) (F : Set (Set U)) (A : Set U)\n    (h1 : A ∈ F) : ⋂₀ F ⊆ A := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_10 (U : Type) (B : Set U) (F : Set (Set U))\n    (h1 : ∀ (A : Set U), A ∈ F → B ⊆ A) : B ⊆ ⋂₀ F := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_13 (U : Type)\n    (F G : Set (Set U)) : F ⊆ G → ⋂₀ G ⊆ ⋂₀ F := by\n  \n  **done::"
  },
  {
    "objectID": "Chap3.html#proofs-involving-conjunctions-and-biconditionals",
    "href": "Chap3.html#proofs-involving-conjunctions-and-biconditionals",
    "title": "3  Proofs",
    "section": "3.4. Proofs Involving Conjunctions and Biconditionals",
    "text": "3.4. Proofs Involving Conjunctions and Biconditionals\nThe strategies in HTPI for working with conjunctions are very simple (HTPI p. 130).\n\nTo prove a goal of the form P ∧ Q:\n\nProve P and Q separately.\n\nWe already saw an example, at the end of the last section, of the use of the rule And.intro to prove a conjunction. In general, if you have h1 : P and h2 : Q, then And.intro h1 h2 is a proof of P ∧ Q. It follows that if your goal is P ∧ Q but you don’t yet have proofs of P and Q, then you can use the tactic apply And.intro _ _. Lean will figure out that the blanks need to be filled in with proofs of P and Q, so it will ask you to prove P and Q separately, as suggested by the strategy above.\nIf you already have a proof of either P or Q, then you can fill in one of the blanks in the apply tactic. For example, if you have h : P, then you can write apply And.intro h _, and Lean will tell you that you just have to prove Q to complete the proof. Similarly, if you have h : Q, then apply And.intro _ h will lead to just the single goal P. There is also a shortcut you can use with the apply tactic: any blanks that come at the end of the tactic can be left out. So instead of apply And.intro _ _, you can just write apply And.intro, and instead of apply And.intro h _, you can write apply And.intro h. On the other hand, apply And.intro _ h can’t be shortened; it is only blanks at the end that can be left out.\nThe strategy for a given that is a conjunction is similar (HTPI p. 131).\n\n\nTo use a given of the form P ∧ Q:\n\nTreat this as two separate givens: P, and Q.\n\nIf you have a given h : P ∧ Q, then Lean will recognize h.left as a proof of P, and h.right as a proof of Q.\nHere’s an example that illustrates these strategies. It is similar to Example 3.4.1 in HTPI\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  \n  **done::\n\n\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ (c : U), c ∈ C ∩ D\n⊢ A ∩ C ⊆ B \\ D\n\n\nThe define tactic will rewrite the goal as ∀ ⦃a : U⦄, a ∈ A ∩ C → a ∈ B \\ D, and then we can introduce an arbitrary x : U and assume x ∈ A ∩ C.\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  **done::\n\n\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ (c : U), c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∩ C\n⊢ x ∈ B \\ D\n\n\nNow let’s take a look at the definitions of h3 and the goal:\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  **done::\n\n\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ (c : U), c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\n⊢ x ∈ B ∧ x ∉ D\n\n\nSince the goal is now a conjunction, we apply the strategy above by using the tactic apply And.intro.\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  apply And.intro\n  **done::\n\n\ncase left\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ (c : U), c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\n⊢ x ∈ B\ncase right\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ (c : U), c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\n⊢ x ∉ D\n\n\nLook carefully at the tactic state. Lean has listed two goals, one after the other, and it has helpfully labeled them “case left” and “case right,” indicating that the first goal is to prove the left side of the conjunction and the second is to prove the right. The given information in both cases is the same, but in the first case the goal is x ∈ B, and in the second it is x ∉ D. At this point, if we simply continue with the proof, Lean will interpret our tactics as applying to the first goal, until we achieve that goal. Once we achieve it, Lean will move on to the second goal.\nHowever, we can make our proof more readable by separating and labeling the proofs of the two goals. To do this, we type a bullet (which looks like this: ·) and then a comment describing the first goal. (To type a bullet, type \\.—that is, backslash–period.) The proof of the first goal will appear below this line, indented further and ending with done. To prepare for this, we leave a blank line, type tab to increase the indenting, and then type done. Then we do the same for the second goal: on the next line, we return to the previous level of indenting and type a bullet and a comment describing the second goal. We follow this with a blank line and then an indented done to indicate the end of the proof of the second goal. We’re going to work on the first goal first, so we click on the first blank line to position the cursor there. The screen now looks like this:\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  apply And.intro\n  · -- Proof that x ∈ B\n\n    **done::\n  · -- Proof that x ∉ D\n\n    **done::\n  done\n\n\ncase left\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ (c : U), c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\n⊢ x ∈ B\n\n\nOf course, there are red squiggles under both new occurrences of done, since neither goal has yet been achieved. We can work on the goals in either order by positioning the cursor on either blank line, and the Infoview pane will show the tactic state for the goal at the position of the cursor. In the display above, we have positioned the cursor on the first blank line, so the Infoview shows the tactic state for the first goal.\nThis first goal is easy: We have h1 : A ⊆ B and, as explained above, h3.left : x ∈ A. As we have seen in several previous examples, the tactic define at h1 will rewrite h1 as ∀ ⦃a : U⦄, a ∈ A → a ∈ B, and then h1 h3.left will be a proof of x ∈ B. And now we’ll let you in on a little secret: usually the define tactic isn’t really necessary. You may find the define tactic to be useful in many situations, because it helps you see what a statement means. But Lean doesn’t need to be told to work out what the statement means; it will do that automatically. So we can skip the define tactic and just give h1 h3.left as a proof of x ∈ B. In general, if you have h1 : A ⊆ B and h2 : x ∈ A, then Lean will recognize h1 h2 as a proof of x ∈ B. Thus, the tactic show x ∈ B from h1 h3.left will complete the first goal. Once we type this (indented to the same position as the done for the first goal), the red squiggle disappears from the first done, and the tactic state shows the No goals message. If we then click on the blank line for the second goal, the Infoview pane shows the tactic state for that goal:\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  apply And.intro\n  · -- Proof that x ∈ B:\n    show x ∈ B from h1 h3.left\n    done\n  · -- Proof that x ∉ D\n\n    **done::\n  done\n\n\ncase right\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ (c : U), c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\n⊢ x ∉ D\n\n\nThe second goal is a negative statement, and the given h2 is also a negative statement. This suggests using proof by contradiction, and achieving the contradiction by contradicting h2.\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  apply And.intro\n  · -- Proof that x ∈ B.\n    show x ∈ B from h1 h3.left\n    done\n  · -- Proof that x ∉ D.\n    contradict h2 with h4\n    **done::\n  done\n\n\ncase right\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ (c : U), c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\nh4 : x ∈ D\n⊢ ∃ (c : U), c ∈ C ∩ D\n\n\nThe goal is now an existential statement, and looking at h3 and h4 it is clear that the right value to plug in for c in the goal is x. The tactic apply Exists.intro x will change the goal to x ∈ C ∩ D (we have again left off the unnecessary blank at the end of the apply tactic).\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  apply And.intro\n  · -- Proof that x ∈ B.\n    show x ∈ B from h1 h3.left\n    done\n  · -- Proof that x ∉ D.\n    contradict h2 with h4\n    apply Exists.intro x\n    **done::\n  done\n\n\ncase right\nU : Type\nA B C D : Set U\nh1 : A ⊆ B\nh2 : ¬∃ (c : U), c ∈ C ∩ D\nx : U\nh3 : x ∈ A ∧ x ∈ C\nh4 : x ∈ D\n⊢ x ∈ C ∩ D\n\n\nThe define tactic would now rewrite the goal as x ∈ C ∧ x ∈ D, and we could prove this goal by combining h3.right and h4, using the And.intro rule. But since we know what the result of the define tactic will be, there is really no need to use it. We can just use And.intro right away to complete the proof.\n\n\ntheorem Like_Example_3_4_1 (U : Type)\n    (A B C D : Set U) (h1 : A ⊆ B)\n    (h2 : ¬∃ (c : U), c ∈ C ∩ D) :\n    A ∩ C ⊆ B \\ D := by\n  define\n  fix x : U\n  assume h3 : x ∈ A ∩ C\n  define at h3; define\n  apply And.intro\n  · -- Proof that x ∈ B.\n    show x ∈ B from h1 h3.left\n    done\n  · -- Proof that x ∉ D.\n    contradict h2 with h4\n    apply Exists.intro x\n    show x ∈ C ∩ D from And.intro h3.right h4\n    done\n  done\n\n\nNo goals\n\n\nSince P ↔︎ Q is shorthand for (P → Q) ∧ (Q → P), the strategies given above for conjunctions lead immediately to the following strategies for biconditionals (HTPI p. 132):\n\n\nTo prove a goal of the form P ↔︎ Q:\n\nProve P → Q and Q → P separately.\n\n\n\nTo use a given of the form P ↔︎ Q:\n\nTreat this as two separate givens: P → Q, and Q → P.\n\nThe methods for using these strategies in Lean are similar to those we used above for conjunctions. If we have h1 : P → Q and h2 : Q → P, then Iff.intro h1 h2 is a proof of P ↔︎ Q. Thus, if the goal is P ↔︎ Q, then the tactic apply Iff.intro _ _ will convert this into two separate goals, P → Q and Q → P. Once again, you can fill in one of these blanks if you already have a proof of either P → Q or Q → P, and you can leave out any blanks at the end of the tactic. If you have a given h : P ↔︎ Q, then h.ltr is a proof of the left-to-right direction of the biconditional, P → Q, and h.rtl is a proof of the right-to-left direction, Q → P.\nLet’s try these strategies out in an example.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  \n  **done::\n\n\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\n⊢ (∃ (x : U), P x) ↔\n>>  ∃ (x : U), Q x\n\n\nThe goal is a biconditional statement, so we begin with the tactic apply Iff.intro.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  **done::\n\n\ncase mp\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\n⊢ (∃ (x : U), P x) →\n>>  ∃ (x : U), Q x\ncase mpr\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\n⊢ (∃ (x : U), Q x) →\n>>  ∃ (x : U), P x\n\n\nOnce again, we have two goals. (The case labels this time aren’t very intuitive; “mp” stands for “modus ponens” and “mpr” stands for “modus ponens reverse”.) Whenever we have multiple goals, we’ll use the bulleted-and-indented style introduced in the last example. As in HTPI, we’ll label the proofs of the two goals with (→) and (←), representing the two directions of the biconditional symbol ↔︎. (You can type ← in VS Code by typing \\l, short for “left”.) The first goal is a conditional statement, so we assume the antecedent.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  · -- (→)\n    assume h2 : ∃ (x : U), P x\n    **done::\n  · -- (←)\n\n    **done::\n  done\n\n\ncase mp\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\nh2 : ∃ (x : U), P x\n⊢ ∃ (x : U), Q x\n\n\nAs usual, when we have an existential given, we use it right away.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  · -- (→)\n    assume h2 : ∃ (x : U), P x\n    obtain (u : U) (h3 : P u) from h2\n    **done::\n  · -- (←)\n\n    **done::\n  done\n\n\ncase mp\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\nh2 : ∃ (x : U), P x\nu : U\nh3 : P u\n⊢ ∃ (x : U), Q x\n\n\nNow that we have an object of type U in the tactic state, we can use h1 by applying universal instantiation.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  · -- (→)\n    assume h2 : ∃ (x : U), P x\n    obtain (u : U) (h3 : P u) from h2\n    have h4 : P u ↔ Q u := h1 u\n    **done::\n  · -- (←)\n\n    **done::\n  done\n\n\ncase mp\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\nh2 : ∃ (x : U), P x\nu : U\nh3 : P u\nh4 : P u ↔ Q u\n⊢ ∃ (x : U), Q x\n\n\nLooking at h3 and h4, we can now see that we will be able to complete the proof if we assign the value u to x in the goal. So our next step is the tactic apply Exists.intro u.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  · -- (→)\n    assume h2 : ∃ (x : U), P x\n    obtain (u : U) (h3 : P u) from h2\n    have h4 : P u ↔ Q u := h1 u\n    apply Exists.intro u\n    **done::\n  · -- (←)\n\n    **done::\n  done\n\n\ncase mp\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\nh2 : ∃ (x : U), P x\nu : U\nh3 : P u\nh4 : P u ↔ Q u\n⊢ Q u\n\n\nTo complete the proof, we use the left-to-right direction of h4. We have h4.ltr : P u → Q u and h3 : P u, so by modus ponens, h4.ltr h3 proves the goal Q u. Once we enter this step, Lean indicates that the left-to-right proof is complete, and we can position the cursor below the right-to-left bullet to see the tactic state for the second half of the proof.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  · -- (→)\n    assume h2 : ∃ (x : U), P x\n    obtain (u : U) (h3 : P u) from h2\n    have h4 : P u ↔ Q u := h1 u\n    apply Exists.intro u\n    show Q u from h4.ltr h3\n    done\n  · -- (←)\n\n    **done::\n  done\n\n\ncase mpr\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\n⊢ (∃ (x : U), Q x) →\n>>  ∃ (x : U), P x\n\n\nThe second half of the proof is similar to the first. We begin by assuming h2 : ∃ (x : U), Q x, and then we use that assumption to obtain u : U and h3 : Q u.\n\n\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  · -- (→)\n    assume h2 : ∃ (x : U), P x\n    obtain (u : U) (h3 : P u) from h2\n    have h4 : P u ↔ Q u := h1 u\n    apply Exists.intro u\n    show Q u from h4.ltr h3\n    done\n  · -- (←)\n    assume h2 : ∃ (x : U), Q x\n    obtain (u : U) (h3 : Q u) from h2\n    **done::\n  done\n\n\ncase mpr\nU : Type\nP Q : Pred U\nh1 : ∀ (x : U), P x ↔ Q x\nh2 : ∃ (x : U), Q x\nu : U\nh3 : Q u\n⊢ ∃ (x : U), P x\n\n\nWe can actually shorten the proof by packing a lot into a single step. See if you can figure out the last line of the completed proof below; we’ll give an explanation after the proof.\nexample (U : Type) (P Q : Pred U)\n    (h1 : ∀ (x : U), P x ↔ Q x) :\n    (∃ (x : U), P x) ↔ ∃ (x : U), Q x := by\n  apply Iff.intro\n  · -- (→)\n    assume h2 : ∃ (x : U), P x\n    obtain (u : U) (h3 : P u) from h2\n    have h4 : P u ↔ Q u := h1 u\n    apply Exists.intro u\n    show Q u from h4.ltr h3\n    done\n  · -- (←)\n    assume h2 : ∃ (x : U), Q x\n    obtain (u : U) (h3 : Q u) from h2\n    show ∃ (x : U), P x from Exists.intro u ((h1 u).rtl h3)\n    done\n  done\nTo understand the last step, start with the fact that h1 u is a proof of P u ↔︎ Q u. Therefore (h1 u).rtl is a proof of Q u → P u, so by modus ponens, (h1 u).rtl h3 is a proof of P u. It follows that Exists.intro u ((h1 u).rtl h3) is a proof of ∃ (x : U), P x, which was the goal.\nThere is one more style of reasoning that is sometimes used in proofs of biconditional statements. It is illustrated in Example 3.4.5 of HTPI. Here is that theorem, as it is presented in HTPI (HTPI p. 137).\n\nSuppose \\(A\\), \\(B\\), and \\(C\\) are sets. Then \\(A \\cap (B \\setmin C) = (A \\cap B) \\setmin C\\).\n\n\nProof. Let \\(x\\) be arbitrary. Then \\[\\begin{align*}\nx \\in A \\cap (B \\setmin C) &\\text{ iff } x \\in A \\wedge x \\in B \\setmin C\\\\\n&\\text{ iff } x \\in A \\wedge x \\in B \\wedge x \\notin C\\\\\n&\\text{ iff } x \\in (A \\cap B) \\wedge x \\notin C\\\\\n&\\text{ iff } x \\in (A \\cap B) \\setmin C.\n\\end{align*}\\] Thus, \\(\\forall x(x \\in A \\cap (B \\setmin C) \\leftrightarrow x \\in (A \\cap B) \\setmin C)\\), so \\(A \\cap (B \\setmin C) = (A \\cap B) \\setmin C\\).  □\n\nThis proof is based on a fundamental principle of set theory that says that if two sets have exactly the same elements, then they are equal. This principle is called the axiom of extensionality, and it is what justifies the inference, in the last sentence, from \\(\\forall x(x \\in A \\cap (B \\setmin C) \\leftrightarrow x \\in (A \\cap B) \\setmin C)\\) to \\(A \\cap (B \\setmin C) = (A \\cap B) \\setmin C\\).\nThe heart of the proof is a string of equivalences that, taken together, establish the biconditional statement \\(x \\in A \\cap (B \\setmin C) \\leftrightarrow x \\in (A \\cap B) \\setmin C\\). One can also use this technique to prove a biconditional statement in Lean. This time we’ll simply present the complete proof first, and then explain it afterwards.\ntheorem Example_3_4_5 (U : Type)\n    (A B C : Set U) : A ∩ (B \\ C) = (A ∩ B) \\ C := by\n  apply Set.ext\n  fix x : U\n  show x ∈ A ∩ (B \\ C) ↔ x ∈ (A ∩ B) \\ C from\n    calc x ∈ A ∩ (B \\ C)\n      _ ↔ x ∈ A ∧ (x ∈ B ∧ x ∉ C) := Iff.refl _\n      _ ↔ (x ∈ A ∧ x ∈ B) ∧ x ∉ C := and_assoc.symm\n      _ ↔ x ∈ (A ∩ B) \\ C := Iff.refl _\n  done\nThe name of the axiom of extensionality in Lean is Set.ext, and it is applied in the first step of the Lean proof. As usual, the apply tactic works backwards from the goal. In other words, after the first line of the proof, the goal is ∀ (x : U), x ∈ A ∩ (B \\ C) ↔︎ x ∈ (A ∩ B) \\ C, because by Set.ext, the conclusion of the theorem would follow from this statement. The rest of the proof then proves this goal by introducing an arbitrary x of type U and then proving the biconditional by stringing together several equivalences, exactly as in the HTPI proof.\nThe proof of the biconditional is called a calculational proof, and it is introduced by the keyword calc. The calculational proof consists of a string of biconditional statements, each of which is provided with a proof. You can think of the underscore on the left side of each biconditional as standing for the right side of the previous biconditional (or, in the case of the first biconditional, the statement after calc).\nThe proofs of the individual biconditionals in the calculational proof require some explanation. Lean has a large library of theorems that it knows, and you can use those theorems in your proofs. In particular, Iff.refl and and_assoc are names of theorems in Lean’s library. You can find out what any theorem says by using the Lean command #check. (Commands that ask Lean for a response generally start with the character #.) If you type #check Iff.refl in a Lean file, you will see Lean’s response in the Infoview pane: Iff.refl (a : Prop) : a ↔︎ a. What this tells us is that Lean already knows the theorem\ntheorem Iff.refl (a : Prop) : a ↔ a\n(This theorem says that “iff” has a property called reflexivity; we’ll discuss reflexivity in Chapter 4.) When variables are declared in the statement of a theorem, it is understood that they can stand for anything of the appropriate type (see Section 3.1 of HTPI). Thus, the theorem Iff.refl can be thought of as establishing the truth of the statement ∀ (a : Prop), a ↔︎ a. In fact, you can get Lean to report the meaning of the theorem in this form with the command #check @Iff.refl. What this means is that, in any proof, Lean lets you treat Iff.refl as a proof of the statement ∀ (a : Prop), a ↔︎ a. Thus, by universal instantiation, for any proposition a, Lean will recognize Iff.refl a as a proof of a ↔︎ a. This is used to justify the first biconditional in the calculational proof.\nBut wait! The first biconditional in the calculational proof is x ∈ A ∩ (B \\ C) ↔︎ x ∈ A ∧ (x ∈ B ∧ x ∉ C), which does not have the form a ↔︎ a. How can it be justified by the theorem Iff.refl? Recall that Lean doesn’t need to be told to write out definitions of mathematical notation; it does that automatically. When the definitions of the set theory notation are written out, the first biconditional in the calculational proof becomes x ∈ A ∧ (x ∈ B ∧ x ∉ C) ↔︎ x ∈ A ∧ (x ∈ B ∧ x ∉ C), which does have the form a ↔︎ a, so it can be proven with the term-mode proof Iff.refl _. Note that we are using an underscore here to ask Lean to figure out what to plug in for a. This saves us the trouble of writing out the full term-mode proof, which would be Iff.refl (x ∈ A ∧ (x ∈ B ∧ x ∉ C)). The lesson of this example is that the theorem Iff.refl is more powerful than it looks. Not only can we use Iff.refl _ to prove statements of the form a ↔︎ a, we can also use it to prove statements of the form a ↔︎ a', if a and a' reduce to the same thing when definitions are filled in. We say in this case that a and a' are definitionally equal. This explains the third line of the calculational proof, which is also justified by the proof Iff.refl _.\nThe second line uses the theorem and_assoc. If you type #check and_assoc, you will get this response from Lean:\n\nand_assoc {a b c : Prop} : (a ∧ b) ∧ c ↔ a ∧ b ∧ c\n\nOnce again, it is understood that the variables a, b, and c can stand for any propositions, as you can see by giving the command #check @and_assoc. This generates the response\n\n@and_assoc : ∀ {a b c : Prop}, (a ∧ b) ∧ c ↔ a ∧ b ∧ c\n\nwhich is shorthand for\n\n@and_assoc : ∀ {a : Prop}, ∀ {b : Prop}, ∀ {c : Prop},\n              (a ∧ b) ∧ c ↔ a ∧ (b ∧ c)\n\nRecall that the curly braces indicate that a, b, and c are implicit arguments, and that Lean groups the logical connectives to the right, which means that it interprets a ∧ b ∧ c as a ∧ (b ∧ c). This is the associative law for “and” (see Section 1.2 of HTPI). Since a, b, and c are implicit, Lean will recognize and_assoc as a proof of any statement of the form (a ∧ b) ∧ c ↔︎ a ∧ (b ∧ c), where a, b, and c can be replaced with any propositions. Lean doesn’t need to be told what propositions are being used as a, b, and c; it will figure that out for itself. Unfortunately, the second biconditional in the calculational proof is x ∈ A ∧ (x ∈ B ∧ x ∉ C) ↔︎ (x ∈ A ∧ x ∈ B) ∧ x ∉ C, which has the form a ∧ (b ∧ c) ↔︎ (a ∧ b) ∧ c, not (a ∧ b) ∧ c ↔︎ a ∧ (b ∧ c). (Notice that the first of these biconditionals is the same as the second except that the left and right sides have been swapped.) To account for this discrepancy, we use the fact that if h is a proof of any biconditional P ↔︎ Q, then h.symm is a proof of Q ↔︎ P. Thus and_assoc.symm proves the second biconditional in the calculational proof. (By the way, the HTPI proof avoids any mention of the associativity of “and” by simply leaving out parentheses in the conjunction \\(x \\in A \\wedge x \\in B \\wedge x \\notin C\\). As explained in Section 1.2 of HTPI, this represents an implicit use of the associativity of “and.”)\nYou can get a better understanding of the first step of our last proof by typing #check @Set.ext. The result is\n\n@Set.ext : ∀ {α : Type u_1} {a b : Set α},\n            (∀ (x : α), x ∈ a ↔ x ∈ b) → a = b\n\nwhich is shorthand for\n\n@Set.ext : ∀ {α : Type u_1}, ∀ {a : Set α}, ∀ {b : Set α},\n            (∀ (x : α), x ∈ a ↔ x ∈ b) → a = b\n\nIgnoring the u_1, whose significance won’t be important to us, this means that Set.ext can be used to prove any statement of the form (∀ (x : α), x ∈ a ↔︎ x ∈ b) → a = b, where α can be replaced by any type and a and b can be replaced by any sets of objects of type α. Make sure you understand how this explains the effect of the tactic apply Set.ext in the first step of our last proof. Almost all of our proofs that two sets are equal will start with apply Set.ext.\nNotice that in Lean’s responses to both #check @and_assoc and #check @Set.ext, multiple universal quantifiers in a row were grouped together and written as a single universal quantifier followed by a list of variables (with types). Lean allows this notational shorthand for any sequence of consecutive quantifiers, as long as they are all of the same kind (all existential or all universal), and we will use this notation from now on.\n\n\nExercises\n\ntheorem Exercise_3_4_2 (U : Type) (A B C : Set U)\n    (h1 : A ⊆ B) (h2 : A ⊆ C) : A ⊆ B ∩ C := by\n  \n  **done::\n\n\ntheorem Exercise_3_4_4 (U : Type) (A B C : Set U)\n    (h1 : A ⊆ B) (h2 : A ⊈ C) : B ⊈ C := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_12 (U : Type)\n    (F G : Set (Set U)) : F ⊆ G → ⋃₀ F ⊆ ⋃₀ G := by\n\n  **done::\n\n\ntheorem Exercise_3_3_16 (U : Type) (B : Set U)\n    (F : Set (Set U)) : F ⊆ 𝒫 B → ⋃₀ F ⊆ B := by\n  \n  **done::\n\n\ntheorem Exercise_3_3_17 (U : Type) (F G : Set (Set U))\n    (h1 : ∀ (A : Set U), A ∈ F → ∀ (B : Set U), B ∈ G → A ⊆ B) :\n    ⋃₀ F ⊆ ⋂₀ G := by\n  \n  **done::\n\n\ntheorem Exercise_3_4_7 (U : Type) (A B : Set U) :\n    𝒫 (A ∩ B) = 𝒫 A ∩ 𝒫 B := by\n\n  **done::\n\n\ntheorem Exercise_3_4_17 (U : Type) (A : Set U) : A = ⋃₀ (𝒫 A) := by\n\n  **done::\n\n\ntheorem Exercise_3_4_18a (U : Type) (F G : Set (Set U)) :\n    ⋃₀ (F ∩ G) ⊆ (⋃₀ F) ∩ (⋃₀ G) := by\n  \n  **done::\n\n\ntheorem Exercise_3_4_19 (U : Type) (F G : Set (Set U)) :\n    (⋃₀ F) ∩ (⋃₀ G) ⊆ ⋃₀ (F ∩ G) ↔\n      ∀ (A B : Set U), A ∈ F → B ∈ G → A ∩ B ⊆ ⋃₀ (F ∩ G) := by\n  \n  **done::"
  },
  {
    "objectID": "Chap3.html#proofs-involving-disjunctions",
    "href": "Chap3.html#proofs-involving-disjunctions",
    "title": "3  Proofs",
    "section": "3.5. Proofs Involving Disjunctions",
    "text": "3.5. Proofs Involving Disjunctions\nA common proof method for dealing with givens or goals that are disjunctions is proof by cases. Here’s how it works (HTPI p. 143).\n\nTo use a given of the form P ∨ Q:\n\nBreak your proof into cases. For case 1, assume that P is true and use this assumption to prove the goal. For case 2, assume that Q is true and prove the goal.\n\nIn Lean, you can break a proof into cases by using the by_cases tactic. If you have a given h : P ∨ Q, then the tactic by_cases on h will break your proof into two cases. For the first case, the given h will be changed to h : P, and for the second, it will be changed to h : Q; the goal for both cases will be the same as the original goal. Thus, the effect of the by_cases on h tactic is as follows:\n\n\n>> ⋮\nh : P ∨ Q\n⊢ goal\n\n\ncase Case_1\n>> ⋮\nh : P\n⊢ goal\ncase Case_2\n>> ⋮\nh : Q\n⊢ goal\n\n\nNotice that the original given h : P ∨ Q gets replaced by h : P in case 1 and h : Q in case 2. This is usually what is most convenient, but if you write by_cases on h with h1, then the original given h will be preserved, and new givens h1 : P and h1 : Q will be added to cases 1 and 2, respectively. If you want different names for the new givens in the two cases, then use by_cases on h with h1, h2 to add the new given h1 : P in case 1 and h2 : Q in case 2.\nYou can follow by_cases on with any proof of a disjunction, even if that proof is not just a single identifier. In that cases you will want to add with to specify the identifier or identifiers to be used for the new assumptions in the two cases. Another variant is that you can use the tactic by_cases h : P to break your proof into two cases, with the new assumptions being h : P in case 1 and h : ¬P in case 2. In other words, the effect of by_cases h : P is the same as adding the new given h : P ∨ ¬P (which, of course, is a tautology) and then using the tactic by_cases on h.\nThere are several introduction rules that you can use in Lean to prove a goal of the form P ∨ Q. If you have h : P, then Lean will accept Or.intro_left Q h as a proof of P ∨ Q. In most situations Lean can infer the proposition Q from context, and in that case you can use the shorter form Or.inl h as a proof of P ∨ Q. You can see the difference between Or.intro_left and Or.inl by using the #check command:\n\n@Or.intro_left : ∀ {a : Prop} (b : Prop), a → a ∨ b\n\n@Or.inl : ∀ {a b : Prop}, a → a ∨ b\n\nNotice that b is an implicit argument in Or.inl, but not in Or.intro_left.\nSimilarly, if you have h : Q, then Or.intro_right P h is a proof of P ∨ Q. In most situations Lean can infer P from context, and you can use the shorter form Or.inr h.\nOften, when your goal has the form P ∨ Q, you will be unable to prove P, and also unable to prove Q. Proof by cases can help in that situation as well (HTPI p. 145).\n\n\nTo prove a goal of the form P ∨ Q:\n\nBreak your proof into cases. In each case, either prove P or prove Q.\n\nExample 3.5.2 from HTPI illustrates these strategies:\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n\n  **done::\n\n\nU : Type\nA B C : Set U\n⊢ A \\ (B \\ C) ⊆ A \\ B ∪ C\n\n\nThe define tactic would rewrite the goal as ∀ ⦃a : U⦄, a ∈ A \\ (B \\ C) → a ∈ A \\ B ∪ C, which suggests that our next two tactics should be fix x : U and assume h1 : x ∈ A \\ (B \\ C). But as we have seen before, if you know what the result of the define tactic is going to be, then there is usually no need to use it. After introducing x as an arbitrary element of A \\ (B \\ C), we write out the definitions of our new given and goal to help guide our next strategy choice:\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n  **done::\n\n\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ x ∉ B \\ C\n⊢ x ∈ A \\ B ∨ x ∈ C\n\n\nThe goal is now a disjunction, which suggests that proof by cases might be helpful. But what cases should we use? The key is to look at the meaning of the right half of the given h1. The meaning of x ∉ B \\ C is ¬(x ∈ B ∧ x ∉ C), which, by one of the De Morgan laws, is equivalent to x ∉ B ∨ x ∈ C.\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n  have h2 : x ∉ B \\ C := h1.right\n  define at h2; demorgan at h2\n            --h2 : x ∉ B ∨ x ∈ C\n  **done::\n\n\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ x ∉ B \\ C\nh2 : x ∉ B ∨ x ∈ C\n⊢ x ∈ A \\ B ∨ x ∈ C\n\n\nThe new given h2 is now a disjunction, which suggests what cases we should use:\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n  have h2 : x ∉ B \\ C := h1.right\n  define at h2; demorgan at h2\n            --h2 : x ∉ B ∨ x ∈ C\n  by_cases on h2\n  **done::\n\n\ncase Case_1\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ x ∉ B \\ C\nh2 : x ∉ B\n⊢ x ∈ A \\ B ∨ x ∈ C\ncase Case_2\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ x ∉ B \\ C\nh2 : x ∈ C\n⊢ x ∈ A \\ B ∨ x ∈ C\n\n\nOf course, now that we have two goals, we will introduce bullets labeling the two parts of the proof as case 1 and case 2. Looking at the givens h1 and h2 in both cases, it is not hard to see that we should be able to prove x ∈ A \\ B in case 1 and x ∈ C in case 2. Thus, in case 1 we will be able to give a proof of the goal that has the form Or.inl _, where the blank will be filled in with a proof of x ∈ A \\ B, and in case 2 we can use Or.inr _, filling in the blank with a proof of x ∈ C. This suggests that we should use the tactics apply Or.inl in case 1 and apply Or.inr in case 2. Focusing first on case 1, we get:\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n  have h2 : x ∉ B \\ C := h1.right\n  define at h2; demorgan at h2\n            --h2 : x ∉ B ∨ x ∈ C\n  by_cases on h2\n  · -- Case 1. h2 : x ∉ B\n    apply Or.inl\n    **done::\n  · -- Case 2. h2 : x ∈ C\n\n    **done::\n  done\n\n\ncase Case_1.h\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ x ∉ B \\ C\nh2 : x ∉ B\n⊢ x ∈ A \\ B\n\n\nNotice that the tactic apply Or.inl has changed the goal for case 1 to the left half of the original goal, x ∈ A \\ B. Since this means x ∈ A ∧ x ∉ B, we can complete case 1 by combining h1.left with h2, and then we can move on to case 2.\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n  have h2 : x ∉ B \\ C := h1.right\n  define at h2; demorgan at h2\n            --h2 : x ∉ B ∨ x ∈ C\n  by_cases on h2\n  · -- Case 1. h2 : x ∉ B\n    apply Or.inl\n    show x ∈ A \\ B from And.intro h1.left h2\n    done\n  · -- Case 2. h2 : x ∈ C\n\n    **done::\n  done\n\n\ncase Case_2\nU : Type\nA B C : Set U\nx : U\nh1 : x ∈ A ∧ x ∉ B \\ C\nh2 : x ∈ C\n⊢ x ∈ A \\ B ∨ x ∈ C\n\n\nCase 2 is similar, using Or.inr and h2\n\n\ntheorem Example_3_5_2\n    (U : Type) (A B C : Set U) :\n    A \\ (B \\ C) ⊆ (A \\ B) ∪ C := by\n  fix x : U\n  assume h1 : x ∈ A \\ (B \\ C)\n  define; define at h1\n  have h2 : x ∉ B \\ C := h1.right\n  define at h2; demorgan at h2\n            --h2 : x ∉ B ∨ x ∈ C\n  by_cases on h2\n  · -- Case 1. h2 : x ∉ B\n    apply Or.inl\n    show x ∈ A \\ B from And.intro h1.left h2\n    done\n  · -- Case 2. h2 : x ∈ C\n    apply Or.inr\n    show x ∈ C from h2\n    done\n  done\n\n\nNo goals\n\n\nThere is a second strategy that is often useful to prove a goal of the form P ∨ Q. It is motivated by the fact that P ∨ Q is equivalent to both ¬P → Q and ¬Q → P (HTPI p. 147).\n\n\nTo prove a goal of the form P ∨ Q:\n\nAssume that P is false and prove Q, or assume that Q is false and prove P.\n\nIf your goal is P ∨ Q, then the Lean tactic or_left with h will add the new given h : ¬Q to the tactic state and set the goal to be P, and or_right with h will add h : ¬P to the tactic state and set the goal to be Q. For example, here is the effect of the tactic or_left with h:\n\n\n>> ⋮\n⊢ P ∨ Q\n\n\n>> ⋮\nh : ¬Q\n⊢ P\n\n\nNotice that or_left and or_right have the same effect as apply Or.inl and apply Or.inr, except that each adds a new given to the tactic state. Sometimes you can tell in advance that you won’t need the extra given, and in that case the tactics apply Or.inl and apply Or.inr can be useful. For example, that was the case in the example above. But if you think the extra given might be useful, you are better off using or_left or or_right. Here’s an example illustrating this.\n\n\nexample (U : Type) (A B C : Set U)\n    (h1 : A \\ B ⊆ C) : A ⊆ B ∪ C := by\n  \n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : A \\ B ⊆ C\n⊢ A ⊆ B ∪ C\n\n\nOf course, we begin by letting x be an arbitrary element of A. Writing out the meaning of the new goal shows that it is a disjunction.\n\n\nexample (U : Type) (A B C : Set U)\n    (h1 : A \\ B ⊆ C) : A ⊆ B ∪ C := by\n  fix x : U\n  assume h2 : x ∈ A\n  define\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : A \\ B ⊆ C\nx : U\nh2 : x ∈ A\n⊢ x ∈ B ∨ x ∈ C\n\n\nLooking at the givens h1 and h2, we see that if we assume x ∉ B, then we should be able to prove x ∈ C. This suggests that we should use the or_right tactic.\n\n\nexample (U : Type) (A B C : Set U)\n    (h1 : A \\ B ⊆ C) : A ⊆ B ∪ C := by\n  fix x : U\n  assume h2 : x ∈ A\n  define\n  or_right with h3\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : A \\ B ⊆ C\nx : U\nh2 : x ∈ A\nh3 : x ∉ B\n⊢ x ∈ C\n\n\nWe can now complete the proof. Notice that h1 _ will be a proof of the goal x ∈ C, if we can fill in the blank with a proof of x ∈ A \\ B. Since x ∈ A \\ B means x ∈ A ∧ x ∉ B, we can prove it with the expression And.intro h2 h3.\n\n\nexample (U : Type) (A B C : Set U)\n    (h1 : A \\ B ⊆ C) : A ⊆ B ∪ C := by\n  fix x : U\n  assume h2 : x ∈ A\n  define\n  or_right with h3\n  show x ∈ C from h1 (And.intro h2 h3)\n  done\n\n\nNo goals\n\n\nThe fact that P ∨ Q is equivalent to both ¬P → Q and ¬Q → P also suggests another strategy for using a given that is a disjunction (HTPI p. 149).\n\n\nTo use a given of the form P ∨ Q:\n\nIf you are also given ¬P, or you can prove that P is false, then you can use this given to conclude that Q is true. Similarly, if you are given ¬Q or can prove that Q is false, then you can conclude that P is true.\n\nThis strategy is a rule of inference called disjunctive syllogism, and the tactic for using this strategy in Lean is called disj_syll. If you have h1 : P ∨ Q and h2 : ¬P, then the tactic disj_syll h1 h2 will change h1 to h1 : Q; if instead you have h2 : ¬Q, then disj_syll h1 h2 will change h1 to h1 : P. Notice that, as with the by_cases tactic, the given h1 gets replaced with the conclusion of the rule. The tactic disj_syll h1 h2 with h3 will preserve the original h1 and introduce the conclusion as a new given with the identifier h3. Also, as with the by_cases tactic, either h1 or h2 can be a complex proof rather than simply an identifier (although in that case it must be enclosed in parentheses, so that Lean can tell where h1 ends and h2 begins). The only requirement is that h1 must be a proof of a disjunction, and h2 must be a proof of the negation of one side of the disjunction. If h1 is not simply an identifier, then you will want to use with to specify the identifier to be used for the conclusion of the rule.\nHere’s an example illustrating the use of the disjunctive syllogism rule.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ¬∃ (x : U),\n>>  x ∈ A ∩ B\n⊢ A ⊆ C\n\n\nOf course, we begin by introducing an arbitrary element of A. We also rewrite h2 as an equivalent positive statement.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  quant_neg at h2\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ (x : U),\n>>  x ∉ A ∩ B\na : U\nh3 : a ∈ A\n⊢ a ∈ C\n\n\nWe can now make two inferences by combining h1 with h3 and by applying h2 to a. To see how to use the inferred statements, we write out their definitions, and since one of them is a negative statement, we reexpress it as an equivalent positive statement.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  quant_neg at h2\n  have h4 : a ∈ B ∪ C := h1 h3\n  have h5 : a ∉ A ∩ B := h2 a\n  define at h4\n  define at h5; demorgan at h5\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ∀ (x : U),\n>>  x ∉ A ∩ B\na : U\nh3 : a ∈ A\nh4 : a ∈ B ∨ a ∈ C\nh5 : a ∉ A ∨ a ∉ B\n⊢ a ∈ C\n\n\nBoth h4 and h5 are disjunctions, and looking at h3 we see that the disjunctive syllogism rule can be applied. From h3 and h5 we can draw the conclusion a ∉ B, and then combining that conclusion with h4 we can infer a ∈ C. Since that is the goal, we are done.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  quant_neg at h2\n  have h4 : a ∈ B ∪ C := h1 h3\n  have h5 : a ∉ A ∩ B := h2 a\n  define at h4\n  define at h5; demorgan at h5\n  disj_syll h5 h3  --h5 : a ∉ B\n  disj_syll h4 h5  --h4 : a ∈ C\n  show a ∈ C from h4\n  done\n\n\nNo goals\n\n\nWe’re going to redo the last example, to illustrate another useful technique in Lean. We start with some of the same steps as before.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  have h4 : a ∈ B ∪ C := h1 h3\n  define at h4\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ¬∃ (x : U),\n>>  x ∈ A ∩ B\na : U\nh3 : a ∈ A\nh4 : a ∈ B ∨ a ∈ C\n⊢ a ∈ C\n\n\nAt this point, you might see a possible route to the goal: from h2 and h3 we should be able to prove that a ∉ B, and then, combining that with h4 by the disjunctive syllogism rule, we should be able to deduce the goal a ∈ C. Let’s try writing the proof that way.\n\n\n??example::\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  have h4 : a ∈ B ∪ C := h1 h3\n  define at h4\n  have h5 : a ∉ B := sorry\n  disj_syll h4 h5  --h4 : a ∈ C\n  show a ∈ C from h4\n  done\n\n\nNo goals\n\n\nWe have introduced a new idea in this proof. The justification we have given for introducing h5 : a ∉ B is sorry. You might think of this as meaning “Sorry, I’m not going to give a justification for this statement, but please accept it anyway.” Of course, this is cheating; in a complete proof, every step must be justified. Lean accepts sorry as a proof of any statement, but it displays it in red to warn you that you’re cheating. It also puts a brown squiggle under the keyword example and a warning symbol in the left margin, and it puts the message declaration uses 'sorry' in the Infoview, to warn you that, although the proof has reached the goal, it is not fully justified.\nAlthough writing the proof this way is cheating, it is a convenient way to see that our plan of attack for this proof is reasonable. Lean has accepted the proof, except for the warning that we have used sorry. So now we know that if we go back and replace sorry with a proof of a ∉ B, then we will have a complete proof.\nThe proof of a ∉ B is hard enough that it is easier to do it in tactic mode rather than term mode. So we will begin the proof as we always do for tactic-mode proofs: we replace sorry with by, leave a blank line, and then put done, indented further than the surrounding text. When we put the cursor on the blank line before done, we see the tactic state for our “proof within a proof.”\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  have h4 : a ∈ B ∪ C := h1 h3\n  define at h4\n  have h5 : a ∉ B := by\n\n    **done::\n  disj_syll h4 h5  --h4 : a ∈ C\n  show a ∈ C from h4\n  done\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ¬∃ (x : U),\n>>  x ∈ A ∩ B\na : U\nh3 : a ∈ A\nh4 : a ∈ B ∨ a ∈ C\n⊢ a ∉ B\n\n\nNote that h5 : a ∉ B is not a given in the tactic state, because we have not yet justified it; in fact, a ∉ B is the goal. This goal is a negative statement, and h2 is also negative. This suggests that we could try using proof by contradiction, achieving the contradiction by contradicting h2. So we use the tactic contradict h2 with h6.\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  have h4 : a ∈ B ∪ C := h1 h3\n  define at h4\n  have h5 : a ∉ B := by\n    contradict h2 with h6\n    **done::\n  disj_syll h4 h5  --h4 : a ∈ C\n  show a ∈ C from h4\n  done\n\n\nU : Type\nA B C : Set U\nh1 : A ⊆ B ∪ C\nh2 : ¬∃ (x : U),\n>>  x ∈ A ∩ B\na : U\nh3 : a ∈ A\nh4 : a ∈ B ∨ a ∈ C\nh6 : a ∈ B\n⊢ ∃ (x : U), x ∈ A ∩ B\n\n\nLooking at h3 and h6, we see that the right value to plug in for x in the goal is a. In fact, Exists.intro a _ will prove the goal, if we can fill in the blank with a proof of a ∈ A ∩ B. Since this means a ∈ A ∧ a ∈ B, we can prove it with And.intro h3 h6. Thus, we can complete the proof in one more step:\n\n\nexample\n    (U : Type) (A B C : Set U) (h1 : A ⊆ B ∪ C)\n    (h2 : ¬∃ (x : U), x ∈ A ∩ B) : A ⊆ C := by\n  fix a : U\n  assume h3 : a ∈ A\n  have h4 : a ∈ B ∪ C := h1 h3\n  define at h4\n  have h5 : a ∉ B := by\n    contradict h2 with h6\n    show ∃ (x : U), x ∈ A ∩ B from\n      Exists.intro a (And.intro h3 h6)\n    done\n  disj_syll h4 h5  --h4 : a ∈ C\n  show a ∈ C from h4\n  done\n\n\nNo goals\n\n\nThe error has disappeared from the word done and a check mark has appeared in the left margin next to example, indicating that the proof is complete.\nIt was not really necessary for us to use sorry when writing this proof. We could have simply written the steps in order, exactly as they appear above. Any time you use the have tactic with a conclusion that is difficult to justify, you have a choice. You can establish the have with sorry, complete the proof, and then return and fill in a justification for the have, as we did in the example above. Or, you can justify the have right away by typing by after := and then plunging into the “proof within in a proof.” Once you complete the inner proof, you can continue with the original proof.\nAnd in case you were wondering: yes, if the inner proof uses the have tactic with a statement that is hard to justify, then you can write a “proof within a proof within a proof”!\n\n\nExercises\nIn each case, replace sorry with a proof.\n\ntheorem Exercise_3_5_2 (U : Type) (A B C : Set U) :\n    (A ∪ B) \\ C ⊆ A ∪ (B \\ C) := sorry\n\n\ntheorem Exercise_3_5_5 (U : Type) (A B C : Set U)\n    (h1 : A ∩ C ⊆ B ∩ C) (h2 : A ∪ C ⊆ B ∪ C) : A ⊆ B := sorry\n\n\ntheorem Exercise_3_5_7 (U : Type) (A B C : Set U) :\n    A ∪ C ⊆ B ∪ C ↔ A \\ C ⊆ B \\ C := sorry\n\n\ntheorem Exercise_3_5_8 (U : Type) (A B : Set U) :\n    𝒫 A ∪ 𝒫 B ⊆ 𝒫 (A ∪ B) := sorry\n\n\ntheorem Exercise_3_5_17b (U : Type) (F : Set (Set U)) (B : Set U) :\n    B ∪ (⋂₀ F) = {x : U | ∀ (A : Set U), A ∈ F → x ∈ B ∪ A} := sorry\n\n\ntheorem Exercise_3_5_18 (U : Type) (F G H : Set (Set U))\n    (h1 : ∀ (A : Set U), A ∈ F → ∀ (B : Set U), B ∈ G → A ∪ B ∈ H) :\n    ⋂₀ H ⊆ (⋂₀ F) ∪ (⋂₀ G) := sorry\n\n\ntheorem Exercise_3_5_24a (U : Type) (A B C : Set U) :\n    (A ∪ B) ∆ C ⊆ (A ∆ C) ∪ (B ∆ C) := sorry"
  },
  {
    "objectID": "Chap3.html#existence-and-uniqueness-proofs",
    "href": "Chap3.html#existence-and-uniqueness-proofs",
    "title": "3  Proofs",
    "section": "3.6. Existence and Uniqueness Proofs",
    "text": "3.6. Existence and Uniqueness Proofs\nRecall that ∃! (x : U), P x means that there is exactly one x of type U such that P x is true. One way to deal with a given or goal of this form is to use the define tactic to rewrite it as the equivalent statement ∃ (x : U), P x ∧ ∀ (x_1 : U), P x_1 → x_1 = x. You can then apply techniques discussed previously in this chapter. However, there are also proof techniques, and corresponding Lean tactics, for working directly with givens and goals of this form.\nOften a goal of the form ∃! (x : U), P x is proven by using the following strategy. This is a slight rephrasing of the strategy presented in HTPI. The rephrasing is based on the fact that for any propositions A, B, and C, A ∧ B → C is equivalent to A → B → C (you can check this equivalence by making a truth table). The second of these statements is usually easier to work with in Lean than the first one, so we will often rephrase statements that have the form A ∧ B → C as A → B → C. To see why the second statement is easier to use, suppose that you have givens hA : A and hB : B. If you also have h : A → B → C, then h hA is a proof of B → C, and therefore h hA hB is a proof of C. If instead you had h' : (A ∧ B) → C, then to prove C you would have to write h' (And.intro hA hB), which is a bit less convenient.\nWith that preparation, here is our strategy for proving statements of the form ∃! (x : U), P x (HTPI pp. 156–157).\n\nTo prove a goal of the form ∃! (x : U), P x:\n\nProve ∃ (x : U), P x and ∀ (x_1 x_2 : U), P x_1 → P x_2 → x_1 = x_2. The first of these goals says that there exists an x such that P x is true, and the second says that it is unique. The two parts of the proof are therefore sometimes labeled existence and uniqueness.\n\nTo apply this strategy in a Lean proof, we use the tactic exists_unique. We’ll illustrate this with the theorem from Example 3.6.2 in HTPI. Here’s how that theorem and its proof are presented in HTPI (HTPI pp. 157–158):\n\nThere is a unique set \\(A\\) such that for every set \\(B\\), \\(A \\cup B = B\\).\n\n\nProof. Existence: Clearly \\(\\forall B(\\varnothing \\cup B = B)\\), so \\(\\varnothing\\) has the required property.\nUniqueness: Suppose \\(\\forall B(C \\cup B = B)\\) and \\(\\forall B(D \\cup B = B)\\). Applying the first of these assumptions to \\(D\\) we see that \\(C \\cup D = D\\), and applying the second to \\(C\\) we get \\(D \\cup C = C\\). But clearly \\(C \\cup D = D \\cup C\\), so \\(C = D\\).  □\n\nYou will notice that there are two statements in this proof that are described as “clearly” true. This brings up one of the difficulties with proving theorems in Lean: things that are clear to us are not necessarily clear to Lean! There are two ways to deal with such “clear” statements. The first is to see if the statement is in the library of theorems that Lean knows. The second is to prove the statement as a preliminary theorem that can then be used in the proof of our main theorem. We’ll take the second approach here, since proving these “clear” facts will give us more practice with Lean proofs, but later we’ll have more to say about searching for statements in Lean’s theorem library.\nThe first theorem we need says that for every set B, ∅ ∪ B = B, and it brings up a subtle issue: in Lean, the symbol ∅ is ambiguous! The reason for this is Lean’s strict typing rules. For each type U, there is an empty set of type Set U. There is, for example, the set of type Set Nat that contains no natural numbers, and also the set of type Set Real that contains no real numbers. To Lean, these are different sets, because they have different types. Which one does the symbol ∅ denote? The answer will be different in different contexts. Lean can often figure out from context which empty set you have in mind, but if it can’t, then you have to tell it explicitly by writing (∅ : Set U) rather than ∅. Fortunately, in our theorems Lean is able to figure out which empty set we have in mind.\nWith that preparation, we are ready to prove our first preliminary theorem. Since the goal is an equation between sets, our first step is to use the tactic apply Set.ext.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    ∅ ∪ B = B := by\n  apply Set.ext\n  **done::\n\n\ncase h\nU : Type\nB : Set U\n⊢ ∀ (x : U),\n>>  x ∈ ∅ ∪ B ↔ x ∈ B\n\n\nBased on the form of the goal, our next two tactics should be fix x : U and apply Iff.intro. This leaves us with two goals, corresponding to the two directions of the biconditional, but we’ll focus first on just the left-to-right direction.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    ∅ ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  · -- (→)\n\n    **done::\n  · -- (←)\n\n    **done::\n  done\n\n\ncase h.mp\nU : Type\nB : Set U\nx : U\n⊢ x ∈ ∅ ∪ B → x ∈ B\n\n\nOf course, our next step is to assume x ∈ ∅ ∪ B. To help us see how to move forward, we also write out the definition of this assumption.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    ∅ ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  · -- (→)\n    assume h1 : x ∈ ∅ ∪ B\n    define at h1\n    **done::\n  · -- (←)\n\n    **done::\n  done\n\n\ncase h.mp\nU : Type\nB : Set U\nx : U\nh1 : x ∈ ∅ ∨ x ∈ B\n⊢ x ∈ B\n\n\nNow you should see a way to complete the proof: the statement x ∈ ∅ is false, so we should be able to apply the disjunctive syllogism rule to h1 to infer the goal x ∈ B. To carry out this plan, we’ll first have to prove x ∉ ∅. We’ll use the have tactic, and since there’s no obvious term-mode proof to justify it, we’ll try a tactic-mode proof.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    ∅ ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  · -- (→)\n    assume h1 : x ∈ ∅ ∪ B\n    define at h1\n    have h2 : x ∉ ∅ := by\n\n      **done::\n    **done::\n  · -- (←)\n\n    **done::\n  done\n\n\nU : Type\nB : Set U\nx : U\nh1 : x ∈ ∅ ∨ x ∈ B\n⊢ x ∉ ∅\n\n\nThe goal for our “proof within a proof” is a negative statement, so proof by contradiction seems like a good start.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    ∅ ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  · -- (→)\n    assume h1 : x ∈ ∅ ∪ B\n    define at h1\n    have h2 : x ∉ ∅ := by\n      by_contra h3\n      **done::\n    **done::\n  · -- (←)\n\n    **done::\n  done\n\n\nU : Type\nB : Set U\nx : U\nh1 : x ∈ ∅ ∨ x ∈ B\nh3 : x ∈ ∅\n⊢ False\n\n\nTo see how to use the new assumption h3, we use the tactic define at h3. The definition Lean gives for the statement x ∈ ∅ is False. In other words, Lean knows that, by the definition of ∅, the statement x ∈ ∅ is false. Since False is our goal, this completes the inner proof, and we can return to the main proof.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    ∅ ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  · -- (→)\n    assume h1 : x ∈ ∅ ∪ B\n    define at h1\n    have h2 : x ∉ ∅ := by\n      by_contra h3\n      define at h3  --h3 : False\n      show False from h3\n      done\n    **done::\n  · -- (←)\n\n    **done::\n  done\n\n\ncase h.mp\nU : Type\nB : Set U\nx : U\nh1 : x ∈ ∅ ∨ x ∈ B\nh2 : x ∉ ∅\n⊢ x ∈ B\n\n\nNow that we have established the claim h2 : x ∉ ∅, we can apply the disjunctive syllogism rule to h1 and h2 to reach the goal. This completes the left-to-right direction of the biconditional proof, so we move on to the right-to-left direction.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    ∅ ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  · -- (→)\n    assume h1 : x ∈ ∅ ∪ B\n    define at h1\n    have h2 : x ∉ ∅ := by\n      by_contra h3\n      define at h3  --h3 : False\n      show False from h3\n      done\n    disj_syll h1 h2  --h1 : x ∈ B\n    show x ∈ B from h1\n    done\n  · -- (←)\n\n    **done::\n  done\n\n\ncase h.mpr\nU : Type\nB : Set U\nx : U\n⊢ x ∈ B → x ∈ ∅ ∪ B\n\n\nThis direction of the biconditional proof is easier: once we introduce the assumption h1 : x ∈ B, our goal will be x ∈ ∅ ∪ B, which means x ∈ ∅ ∨ x ∈ B, and we can prove it with the proof Or.inr h1.\n\n\ntheorem empty_union {U : Type} (B : Set U) :\n    ∅ ∪ B = B := by\n  apply Set.ext\n  fix x : U\n  apply Iff.intro\n  · -- (→)\n    assume h1 : x ∈ ∅ ∪ B\n    define at h1\n    have h2 : x ∉ ∅ := by\n      by_contra h3\n      define at h3  --h3 : False\n      show False from h3\n      done\n    disj_syll h1 h2  --h1 : x ∈ B\n    show x ∈ B from h1\n    done\n  · -- (←)\n    assume h1 : x ∈ B\n    show x ∈ ∅ ∪ B from Or.inr h1\n    done\n  done\n\n\nNo goals\n\n\nThe second fact that was called “clear” in the proof from Example 3.6.2 was the equation C ∪ D = D ∪ C. This looks like an instance of the commutativity of the union operator. Let’s prove that union is commutative.\n\n\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n  \n  **done::\n\n\nU : Type\nX Y : Set U\n⊢ X ∪ Y = Y ∪ X\n\n\nOnce again, we begin with apply Set.ext, which converts the goal to ∀ (x : U), x ∈ X ∪ Y ↔︎ x ∈ Y ∪ X, and then fix x : U.\n\n\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n  apply Set.ext\n  fix x : U\n  **done::\n\n\ncase h\nU : Type\nX Y : Set U\nx : U\n⊢ x ∈ X ∪ Y ↔ x ∈ Y ∪ X\n\n\nTo understand the goal better, we’ll write out the definitions of the two sides of the biconditional. We use an extension of the define tactic that allows us to write out the definition of just a part of a given or the goal. The tactic define : x ∈ X ∪ Y will replace x ∈ X ∪ Y with its definition wherever it appears in the goal, and then define : x ∈ Y ∪ X will replace x ∈ Y ∪ X with its definition. (Note that define : X ∪ Y produces a result that is not as useful. It is usually best to define a complete statement rather than just a part of a statement. As usual, you can add at to do the replacements in a given rather than the goal.)\n\n\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n  apply Set.ext\n  fix x : U\n  define : x ∈ X ∪ Y\n  define : x ∈ Y ∪ X\n  **done::\n\n\ncase h\nU : Type\nX Y : Set U\nx : U\n⊢ x ∈ X ∨ x ∈ Y ↔\n>>  x ∈ Y ∨ x ∈ X\n\n\nBy the way, there are similar extensions of all of the tactics contrapos, demorgan, conditional, double_neg, bicond_neg, and quant_neg that allow you to use a logical equivalence to rewrite just a part of a formula. For example, if your goal is P ∧ (¬Q → R), then the tactic contrapos : ¬Q → R will change the goal to P ∧ (¬R → Q). If you have a given h : P → ¬∀ (x : U), Q x, then the tactic quant_neg : ¬∀ (x : U), Q x at h will change h to h : P → ∃ (x : U), ¬Q x.\nReturning to our proof of union_comm: the goal is now x ∈ X ∨ x ∈ Y ↔︎ x ∈ Y ∨ x ∈ X. You could prove this by a somewhat tedious application of the rules for biconditionals and disjunctions that were discussed in the last two sections, and we invite you to try it. But there is another possibility. The goal now has the form P ∨ Q ↔︎ Q ∨ P, which is the commutative law for “or” (see Section 1.2 of HTPI). We saw in a previous example that Lean has, in its library, the associative law for “and”; it is called and_assoc. Does Lean also know the commutative law for “or”?\nTry typing #check @or_ in VS Code. After a few seconds, a pop-up window appears with possible completions of this command. You will see or_assoc on the list, as well as or_comm. Select or_comm, and you’ll get this response: @or_comm : ∀ {a b : Prop}, a ∨ b ↔︎ b ∨ a. Since a and b are implicit arguments in this theorem, you can use or_comm to prove any statement of the form a ∨ b ↔︎ b ∨ a, where Lean will figure out for itself what a and b stand for. In particular, or_comm will prove our current goal.\n\n\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n  apply Set.ext\n  fix x : U\n  define : x ∈ X ∪ Y\n  define : x ∈ Y ∪ X\n  show x ∈ X ∨ x ∈ Y ↔ x ∈ Y ∨ x ∈ X from or_comm\n  done\n\n\nNo goals\n\n\nWe have now proven the two statements that were said to be “clearly” true in the proof in Example 3.6.2 of HTPI, and we have given them names. And that means that we can now use these theorems, in the file containing these proofs, to prove other theorems. As with any theorem in Lean’s library, you can use the #check command to confirm what these theorems say. If you type #check @empty_union and #check @union_comm, you will get these results:\n\n@empty_union : ∀ {U : Type} (B : Set U), ∅ ∪ B = B\n\n@union_comm : ∀ {U : Type} (X Y : Set U), X ∪ Y = Y ∪ X\n\nNotice that in both theorems we used curly braces when we introduced the type U, so it is an implicit argument and will not need to be specified when we apply the theorems. (Why did we decide to make U an implicit argument? Well, when we apply the theorem empty_union we will be specifying the set B, and when we apply union_comm we will be specifying the sets X and Y. Lean can figure out what U is by examining the types of these sets, so there is no need to specify it separately.)\nWe are finally ready to prove the theorem from Example 3.6.2. Here is the theorem:\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n\n  **done::\n\n\nU : Type\n⊢ ∃! (A : Set U),\n>>  ∀ (B : Set U),\n>>    A ∪ B = B\n\n\nThe goal starts with ∃!, so we use our new tactic, exists_unique.\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n  **done::\n\n\ncase Existence\nU : Type\n⊢ ∃ (A : Set U),\n>>  ∀ (B : Set U),\n>>    A ∪ B = B\ncase Uniqueness\nU : Type\n⊢ ∀ (A_1 A_2 : Set U),\n>>  (∀ (B : Set U),\n>>      A_1 ∪ B = B) →\n>>  (∀ (B : Set U),\n>>      A_2 ∪ B = B) →\n>>  A_1 = A_2\n\n\nWe have two goals, labeled Existence and Uniqueness. Imitating the proof from HTPI, we prove existence by using the value ∅ for A.\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n  · -- Existence\n    apply Exists.intro ∅\n    **done::\n  · -- Uniqueness\n\n    **done::\n  done\n\n\ncase Existence\nU : Type\n⊢ ∀ (B : Set U),\n>>  ∅ ∪ B = B\n\n\nThe goal is now precisely the statement of the theorem empty_union, so we can prove it by simply citing that theorem.\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n  · -- Existence\n    apply Exists.intro ∅\n    show ∀ (B : Set U), ∅ ∪ B = B from empty_union\n    done\n  · -- Uniqueness\n\n    **done::\n  done\n\n\ncase Uniqueness\nU : Type\n⊢ ∀ (A_1 A_2 : Set U),\n>>  (∀ (B : Set U),\n>>      A_1 ∪ B = B) →\n>>  (∀ (B : Set U),\n>>      A_2 ∪ B = B) →\n>>  A_1 = A_2\n\n\nFor the uniqueness proof, we begin by introducing arbitrary sets C and D and assuming ∀ (B : Set U), C ∪ B = B and ∀ (B : Set U), D ∪ B = B, exactly as in the HTPI proof.\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n  · -- Existence\n    apply Exists.intro ∅\n    show ∀ (B : Set U), ∅ ∪ B = B from empty_union\n    done\n  · -- Uniqueness\n    fix C : Set U; fix D : Set U\n    assume h1 : ∀ (B : Set U), C ∪ B = B\n    assume h2 : ∀ (B : Set U), D ∪ B = B\n    **done::\n  done\n\n\ncase Uniqueness\nU : Type\nC D : Set U\nh1 : ∀ (B : Set U),\n>>  C ∪ B = B\nh2 : ∀ (B : Set U),\n>>  D ∪ B = B\n⊢ C = D\n\n\nThe next step in HTPI was to apply h1 to D, and h2 to C. We do the same thing in Lean.\n\n\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n  · -- Existence\n    apply Exists.intro ∅\n    show ∀ (B : Set U), ∅ ∪ B = B from empty_union\n    done\n  · -- Uniqueness\n    fix C : Set U; fix D : Set U\n    assume h1 : ∀ (B : Set U), C ∪ B = B\n    assume h2 : ∀ (B : Set U), D ∪ B = B\n    have h3 : C ∪ D = D := h1 D\n    have h4 : D ∪ C = C := h2 C \n    **done::\n  done\n\n\ncase Uniqueness\nU : Type\nC D : Set U\nh1 : ∀ (B : Set U),\n>>  C ∪ B = B\nh2 : ∀ (B : Set U),\n>>  D ∪ B = B\nh3 : C ∪ D = D\nh4 : D ∪ C = C\n⊢ C = D\n\n\nThe goal can now be achieved by stringing together a sequence of equations: C = D ∪ C = C ∪ D = D. The first of these equations is h4.symm—that is, h4 read backwards; the second follows from the commutative law for union; and the third is h3. We saw in Section 3.4 that you can prove a biconditional statement in Lean by stringing together a sequence of biconditionals in a calculational proof. Exactly the same method applies to equations. Here is the complete proof of the theorem:\ntheorem Example_3_6_2 (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U),\n    A ∪ B = B := by\n  exists_unique\n  · -- Existence\n    apply Exists.intro ∅\n    show ∀ (B : Set U), ∅ ∪ B = B from empty_union\n    done\n  · -- Uniqueness\n    fix C : Set U; fix D : Set U\n    assume h1 : ∀ (B : Set U), C ∪ B = B\n    assume h2 : ∀ (B : Set U), D ∪ B = B\n    have h3 : C ∪ D = D := h1 D\n    have h4 : D ∪ C = C := h2 C \n    show C = D from\n      calc C\n        _ = D ∪ C := h4.symm\n        _ = C ∪ D := union_comm D C\n        _ = D := h3\n    done\n  done\nSince the statement ∃! (x : U), P x asserts both the existence and the uniqueness of an object satisfying the predicate P, we have the following strategy for using a given of this form (HTPI p. 159):\n\n\nTo use a given of the form ∃! (x : U), P x:\n\nIntroduce a new variable, say a, into the proof to stand for an object of type U for which P a is true. You may also assert that ∀ (x_1 x_2 : U), P x_1 → P x_2 → x_1 = x2.\n\nIf you have a given h : ∃! (x : U), P x, then the tactic obtain (a : U) (h1 : P a) (h2 : ∀ (x_1 x_2 : U), P x_1 → P x_2 → x_1 = x_2) from h will introduce into the tactic state a new variable a of type U and new givens (h1 : P a) and (h2 : ∀ (x_1 x_2 : U), P x_1 → P x_2 → x_1 = x_2). To illustrate the use of this tactic, let’s prove the theorem in Example 3.6.4 of HTPI.\n\n\ntheorem Example_3_6_4 (U : Type) (A B C : Set U)\n    (h1 : ∃ (x : U), x ∈ A ∩ B)\n    (h2 : ∃ (x : U), x ∈ A ∩ C)\n    (h3 : ∃! (x : U), x ∈ A) :\n    ∃ (x : U), x ∈ B ∩ C := by\n\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : ∃ (x : U),\n>>  x ∈ A ∩ B\nh2 : ∃ (x : U),\n>>  x ∈ A ∩ C\nh3 : ∃! (x : U), x ∈ A\n⊢ ∃ (x : U), x ∈ B ∩ C\n\n\nWe begin by applying the obtain tactic to h1, h2, and h3. In the case of h3, we get an extra given asserting the uniqueness of the element of A. We also write out the definitions of two of the new givens we obtain.\n\n\ntheorem Example_3_6_4 (U : Type) (A B C : Set U)\n    (h1 : ∃ (x : U), x ∈ A ∩ B)\n    (h2 : ∃ (x : U), x ∈ A ∩ C)\n    (h3 : ∃! (x : U), x ∈ A) :\n    ∃ (x : U), x ∈ B ∩ C := by\n  obtain (b : U) (h4 : b ∈ A ∩ B) from h1\n  obtain (c : U) (h5 : c ∈ A ∩ C) from h2\n  obtain (a : U) (h6 : a ∈ A) (h7 : ∀ (y z : U),\n    y ∈ A → z ∈ A → y = z)  from h3\n  define at h4; define at h5\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : ∃ (x : U),\n>>  x ∈ A ∩ B\nh2 : ∃ (x : U),\n>>  x ∈ A ∩ C\nh3 : ∃! (x : U), x ∈ A\nb : U\nh4 : b ∈ A ∧ b ∈ B\nc : U\nh5 : c ∈ A ∧ c ∈ C\na : U\nh6 : a ∈ A\nh7 : ∀ (y z : U),\n>>  y ∈ A → z ∈ A → y = z\n⊢ ∃ (x : U), x ∈ B ∩ C\n\n\nThe key to the rest of the proof is the observation that, by the uniqueness of the element of A, b must be equal to c. To justify this conclusion, note that by two applications of universal instantiation, h7 b c is a proof of b ∈ A → c ∈ A → b = c, and therefore by two applications of modus ponens, h7 b c h4.left h5.left is a proof of b = c.\n\n\ntheorem Example_3_6_4 (U : Type) (A B C : Set U)\n    (h1 : ∃ (x : U), x ∈ A ∩ B)\n    (h2 : ∃ (x : U), x ∈ A ∩ C)\n    (h3 : ∃! (x : U), x ∈ A) :\n    ∃ (x : U), x ∈ B ∩ C := by\n  obtain (b : U) (h4 : b ∈ A ∩ B) from h1\n  obtain (c : U) (h5 : c ∈ A ∩ C) from h2\n  obtain (a : U) (h6 : a ∈ A) (h7 : ∀ (y z : U),\n    y ∈ A → z ∈ A → y = z)  from h3\n  define at h4; define at h5\n  have h8 : b = c := h7 b c h4.left h5.left\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : ∃ (x : U),\n>>  x ∈ A ∩ B\nh2 : ∃ (x : U),\n>>  x ∈ A ∩ C\nh3 : ∃! (x : U), x ∈ A\nb : U\nh4 : b ∈ A ∧ b ∈ B\nc : U\nh5 : c ∈ A ∧ c ∈ C\na : U\nh6 : a ∈ A\nh7 : ∀ (y z : U),\n>>  y ∈ A → z ∈ A → y = z\nh8 : b = c\n⊢ ∃ (x : U), x ∈ B ∩ C\n\n\nFor our next step, we will need a new tactic. Since we have h8 : b = c, we should be able to replace b with c anywhere it appears. The tactic that allows us to do this called rewrite. If h is a proof of any equation s = t, then rewrite [h] will replace all occurrences of s in the goal with t. Notice that it is the left side of the equation that is replaced with the right side; if you want the replacement to go in the other direction, so that t is replaced with s, you can use rewrite [←h]. (Alternatively, since h.symm is a proof of t = s, you can use rewrite [h.symm].) You can also apply the rewrite tactic to biconditional statements. If you have h : P ↔︎ Q, then rewrite [h] will cause all occurrences of P in the goal to be replaced with Q (and rewrite [←h] will replace Q with P).\nAs with many other tactics, you can add at h' to specify that the replacement should be done in the given h' rather than the goal. In our case, rewrite [h8] at h4 will change both occurrences of b in h4 to c.\n\n\ntheorem Example_3_6_4 (U : Type) (A B C : Set U)\n    (h1 : ∃ (x : U), x ∈ A ∩ B)\n    (h2 : ∃ (x : U), x ∈ A ∩ C)\n    (h3 : ∃! (x : U), x ∈ A) :\n    ∃ (x : U), x ∈ B ∩ C := by\n  obtain (b : U) (h4 : b ∈ A ∩ B) from h1\n  obtain (c : U) (h5 : c ∈ A ∩ C) from h2\n  obtain (a : U) (h6 : a ∈ A) (h7 : ∀ (y z : U),\n    y ∈ A → z ∈ A → y = z)  from h3\n  define at h4; define at h5\n  have h8 : b = c := h7 b c h4.left h5.left\n  rewrite [h8] at h4\n  **done::\n\n\nU : Type\nA B C : Set U\nh1 : ∃ (x : U),\n>>  x ∈ A ∩ B\nh2 : ∃ (x : U),\n>>  x ∈ A ∩ C\nh3 : ∃! (x : U), x ∈ A\nb c : U\nh4 : c ∈ A ∧ c ∈ B\nh5 : c ∈ A ∧ c ∈ C\na : U\nh6 : a ∈ A\nh7 : ∀ (y z : U),\n>>  y ∈ A → z ∈ A → y = z\nh8 : b = c\n⊢ ∃ (x : U), x ∈ B ∩ C\n\n\nNow the right sides of h4 and h5 tell us that we can prove the goal by plugging in c for x. Here is the complete proof:\ntheorem Example_3_6_4 (U : Type) (A B C : Set U)\n    (h1 : ∃ (x : U), x ∈ A ∩ B)\n    (h2 : ∃ (x : U), x ∈ A ∩ C)\n    (h3 : ∃! (x : U), x ∈ A) :\n    ∃ (x : U), x ∈ B ∩ C := by\n  obtain (b : U) (h4 : b ∈ A ∩ B) from h1\n  obtain (c : U) (h5 : c ∈ A ∩ C) from h2\n  obtain (a : U) (h6 : a ∈ A) (h7 : ∀ (y z : U),\n    y ∈ A → z ∈ A → y = z)  from h3\n  define at h4; define at h5\n  have h8 : b = c := h7 b c h4.left h5.left\n  rewrite [h8] at h4\n  show ∃ (x : U), x ∈ B ∩ C from\n    Exists.intro c (And.intro h4.right h5.right)\n  done\nYou might want to compare the Lean proof above to the proof of this theorem as it appears in HTPI (HTPI p. 160):\n\nSuppose \\(A\\), \\(B\\), and \\(C\\) are sets, \\(A\\) and \\(B\\) are not disjoint, \\(A\\) and \\(C\\) are not disjoint, and \\(A\\) has exactly one element. Then \\(B\\) and \\(C\\) are not disjoint\n\n\nProof. Since \\(A\\) and \\(B\\) are not disjoint, we can let \\(b\\) be something such that \\(b \\in A\\) and \\(b \\in B\\). Similarly, since \\(A\\) and \\(C\\) are not disjoint, there is some object \\(c\\) such that \\(c \\in A\\) and \\(c \\in C\\). Since \\(A\\) has only one element, we must have \\(b = c\\). Thus \\(b = c \\in B \\cap C\\) and therefore \\(B\\) and \\(C\\) are not disjoint.  □\n\nBefore ending this section, we return to the question of how you can tell if a theorem you want to use is in Lean’s library. In an earlier example, we guessed that the commutative law for “or” might be in Lean’s library, and we were then able to use the #check command to confirm it. But there is another technique that we could have used: the tactic apply?, which asks Lean to search through its library of theorems to see if there is one that could be applied to prove the goal. Let’s return to our proof of the theorem union_comm, which started like this:\n\n\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n  apply Set.ext\n  fix x : U\n  define : x ∈ X ∪ Y\n  define : x ∈ Y ∪ X\n  **done::\n\n\ncase h\nU : Type\nX Y : Set U\nx : U\n⊢ x ∈ X ∨ x ∈ Y ↔\n>>  x ∈ Y ∨ x ∈ X\n\n\nNow let’s give the apply? tactic a try.\ntheorem union_comm {U : Type} (X Y : Set U) :\n    X ∪ Y = Y ∪ X := by\n  apply Set.ext\n  fix x : U\n  define : x ∈ X ∪ Y\n  define : x ∈ Y ∪ X\n  ++apply?::\n  done\nIt takes a few seconds for Lean to search its library of theorems, but eventually a blue squiggle appears under apply?, indicating that the tactic has produced an answer. You will find the answer in the Infoview pane: Try this: exact Or.comm. The word exact is the name of a tactic that we have not discussed; it is a shorthand for show _ from, where the blank gets filled in with the goal. Thus, you can think of apply?’s answer as a shortened form of the tactic\n\nshow x ∈ X ∨ x ∈ Y ↔ x ∈ Y ∨ x ∈ X from Or.comm\n\nThe command #check @Or.comm will tell you that Or.comm is just an alternative name for the theorem or_comm. So the step suggested by the apply? tactic is essentially the same as the step we used earlier to complete the proof.\nUsually your proof will be more readable if you use the show tactic to state explicitly the goal that is being proven. This also gives Lean a chance to correct you if you have become confused about what goal you are proving. But sometimes—for example, if the goal is very long—it is convenient to use the exact tactic instead. You might think of exact as meaning “the following is a term-mode proof that is exactly what is needed to prove the goal.”\nThe apply? tactic has not only come up with a suggested tactic, it has applied that tactic, and the proof is now complete. You can confirm that the tactic completes the proof by replacing the line apply? in the proof with apply?’s suggested exact tactic.\nThe apply? tactic is somewhat unpredictable; sometimes it is able to find the right theorem in the library, and sometimes it isn’t. But it is always worth a try. There are also tools available on the internet for searching Lean’s library, including LeanSearch, Moogle, and Loogle. Another way to try to find theorems is to visit the documentation page for Lean’s mathematics library, which can be found at https://leanprover-community.github.io/mathlib4_docs/.\n\n\nExercises\n\ntheorem Exercise_3_4_15 (U : Type) (B : Set U) (F : Set (Set U)) :\n    ⋃₀ {X : Set U | ∃ (A : Set U), A ∈ F ∧ X = A \\ B}\n      ⊆ ⋃₀ (F \\ 𝒫 B) := sorry\n\n\ntheorem Exercise_3_5_9 (U : Type) (A B : Set U)\n    (h1 : 𝒫 (A ∪ B) = 𝒫 A ∪ 𝒫 B) : A ⊆ B ∨ B ⊆ A := by\n  --Hint:  Start like this:\n  have h2 : A ∪ B ∈ 𝒫 (A ∪ B) := sorry\n  sorry\n\n\ntheorem Exercise_3_6_6b (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U), A ∪ B = A := sorry\n\n\ntheorem Exercise_3_6_7b (U : Type) :\n    ∃! (A : Set U), ∀ (B : Set U), A ∩ B = A := sorry\n\n\ntheorem Exercise_3_6_8a (U : Type) : ∀ (A : Set U),\n    ∃! (B : Set U), ∀ (C : Set U), C \\ A = C ∩ B := sorry\n\n\ntheorem Exercise_3_6_10 (U : Type) (A : Set U)\n    (h1 : ∀ (F : Set (Set U)), ⋃₀ F = A → A ∈ F) :\n    ∃! (x : U), x ∈ A := by\n  --Hint:  Start like this:\n  set F0 : Set (Set U) := {X : Set U | X ⊆ A ∧ ∃! (x : U), x ∈ X}\n  --Now F0 is in the tactic state, with the definition above\n  have h2 : ⋃₀ F0 = A := sorry\n  sorry"
  },
  {
    "objectID": "Chap3.html#more-examples-of-proofs",
    "href": "Chap3.html#more-examples-of-proofs",
    "title": "3  Proofs",
    "section": "3.7. More Examples of Proofs",
    "text": "3.7. More Examples of Proofs\nIt is finally time to discuss proofs involving algebraic reasoning. Lean has types for several different kinds of numbers. Nat is the type of natural numbers—that is, the numbers 0, 1, 2, …. Int is the type of integers, Rat is the type of rational numbers, Real is the type of real numbers, and Complex is the type of complex numbers. Lean also uses the notation ℕ, ℤ, ℚ, ℝ, and ℂ for these types. (If you want to use those names for the number types, you can enter them by typing \\N, \\Z, \\Q, \\R, and \\C.) To write formulas involving arithmetic operations, you should use the symbols + for addition, - for subtraction, * for multiplication, / for division, and ^ for exponentiation. You can enter the symbols ≤, ≥, and ≠ by typing \\le, \\ge, and \\ne, respectively. We will discuss some of the more subtle points of algebraic reasoning in Chapter 6. For the moment, you are best off avoiding subtraction and division when working with natural numbers and avoiding division when working with integers.\nTo see what’s involved in proving theorems about numbers in Lean, we’ll turn to a few examples from earlier in Chapter 3 of HTPI. We begin with Theorem 3.3.7, which concerns divisibility of integers. As in HTPI, for integers x and y, we will write x ∣ y to mean that x divides y, or y is divisible by x. The formal definition is that x ∣ y means that there is an integer k such that y = x * k. For example, 3 ∣ 12, since 12 = 3 * 4. Lean knows this notation, but there is an important warning: to type the vertical line that means “divides,” you must type \\|, not simply |. (There are two slightly different vertical line symbols, and you have to look closely to see that they are different: | and ∣. It is the second one that means “divides” in Lean, and to enter it you must type \\|.) Here is Theorem 3.3.7, written using our usual rephrasing of a statement of the form A ∧ B → C as A → B → C.\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  \n  **done::\n\n\n⊢ ∀ (a b c : ℤ),\n>>  a ∣ b → b ∣ c → a ∣ c\n\n\nOf course, we begin the proof by introducing arbitrary integers a, b, and c, and assuming a ∣ b and b ∣ c. We also write out the definitions of our assumptions and the goal.\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n  **done::\n\n\na b c : ℤ\nh1 : ∃ (c : ℤ),\n>>  b = a * c\nh2 : ∃ (c_1 : ℤ),\n>>  c = b * c_1\n⊢ ∃ (c_1 : ℤ),\n>>  c = a * c_1\n\n\nWe always use existential givens right away, so we use h1 and h2 to introduce two new variables, m and n.\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n  obtain (m : Int) (h3 : b = a * m) from h1\n  obtain (n : Int) (h4 : c = b * n) from h2\n  **done::\n\n\na b c : ℤ\nh1 : ∃ (c : ℤ),\n>>  b = a * c\nh2 : ∃ (c_1 : ℤ),\n>>  c = b * c_1\nm : ℤ\nh3 : b = a * m\nn : ℤ\nh4 : c = b * n\n⊢ ∃ (c_1 : ℤ),\n>>  c = a * c_1\n\n\nIf we substitute the value for b given in h3 into h4, we will see how to reach the goal. Of course, the rewrite tactic is what we need for this.\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n  obtain (m : Int) (h3 : b = a * m) from h1\n  obtain (n : Int) (h4 : c = b * n) from h2\n  rewrite [h3] at h4   --h4 : c = a * m * n\n  **done::\n\n\na b c : ℤ\nh1 : ∃ (c : ℤ),\n>>  b = a * c\nh2 : ∃ (c_1 : ℤ),\n>>  c = b * c_1\nm : ℤ\nh3 : b = a * m\nn : ℤ\nh4 : c = a * m * n\n⊢ ∃ (c_1 : ℤ),\n>>  c = a * c_1\n\n\nLooking at h4, we see that the value we should use for c_1 in the goal is m * n.\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n  obtain (m : Int) (h3 : b = a * m) from h1\n  obtain (n : Int) (h4 : c = b * n) from h2\n  rewrite [h3] at h4   --h4 : c = a * m * n\n  apply Exists.intro (m * n)\n  **done::\n\n\na b c : ℤ\nh1 : ∃ (c : ℤ),\n>>  b = a * c\nh2 : ∃ (c_1 : ℤ),\n>>  c = b * c_1\nm : ℤ\nh3 : b = a * m\nn : ℤ\nh4 : c = a * m * n\n⊢ c = a * (m * n)\n\n\nNote that in the application of Exists.intro, the parentheses around m * n are necessary to help Lean parse the line correctly. Comparing h4 to the goal, you might think that we can finish the proof with show c = a * (m * n) from h4. But if you try it, you will get an error message. What’s the problem? The difference in the parentheses is the clue. Lean groups the arithmetic operations +, -, *, and / to the left, so h4 means h4 : c = (a * m) * n, which is not quite the same as the goal. To prove the goal, we will need to apply the associative law for multiplication.\nWe have already seen that and_assoc is Lean’s name for the associative law for “and”. Perhaps you can guess that the name for the associative law for multiplication is mul_assoc. If you type #check @mul_assoc, Lean’s response will be:\n\n@mul_assoc : ∀ {G : Type u_1} [inst : Semigroup G] (a b c : G),\n              a * b * c = a * (b * c)\n\nThe implicit arguments in this cases are a little complicated (the expression [inst : Semigroup G] represents yet another kind of implicit argument). But what they mean is that mul_assoc can be used to prove any statement of the form ∀ (a b c : G), a * b * c = a * (b * c), as long as G is a type that has an associative multiplication operation. In particular, mul_assoc can be used as a proof of ∀ (a b c : Int), a * b * c = a * (b * c). (There are also versions of this theorem for particular number types. You can use the #check command to verify the theorems Nat.mul_assoc : ∀ (a b c : ℕ), a * b * c = a * (b * c), Int.mul_assoc : ∀ (a b c : ℤ), a * b * c = a * (b * c), and so on.)\nReturning to our proof of Theorem 3.3.7, by three applications of universal instantiation, mul_assoc a m n is a proof of a * m * n = a * (m * n), and that is exactly what we need to finish the proof. The tactic rewrite [mul_assoc a m n] at h4 will replace a * m * n in h4 with a * (m * n).\n\n\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n  obtain (m : Int) (h3 : b = a * m) from h1\n  obtain (n : Int) (h4 : c = b * n) from h2\n  rewrite [h3] at h4   --h4 : c = a * m * n\n  apply Exists.intro (m * n)\n  rewrite [mul_assoc a m n] at h4\n  **done::\n\n\na b c : ℤ\nh1 : ∃ (c : ℤ),\n>>  b = a * c\nh2 : ∃ (c_1 : ℤ),\n>>  c = b * c_1\nm : ℤ\nh3 : b = a * m\nn : ℤ\nh4 : c = a * (m * n)\n⊢ c = a * (m * n)\n\n\nBy the way, this is a case in which Lean could have figured out some details on its own. If we had used rewrite [mul_assoc _ _ _] at h4, then Lean would have figured out that the blanks had to be filled in with a, m, and n. And as with the apply tactic, blanks at the end of rewrite rules can be left out, so even rewrite [mul_assoc] at h4 would have worked.\nOf course, now h4 really does match the goal exactly, so we can use it to complete the proof.\ntheorem Theorem_3_3_7 :\n    ∀ (a b c : Int), a ∣ b → b ∣ c → a ∣ c := by\n  fix a : Int; fix b : Int; fix c : Int\n  assume h1 : a ∣ b; assume h2 : b ∣ c\n  define at h1; define at h2; define\n  obtain (m : Int) (h3 : b = a * m) from h1\n  obtain (n : Int) (h4 : c = b * n) from h2\n  rewrite [h3] at h4   --h4 : c = a * m * n\n  apply Exists.intro (m * n)\n  rewrite [mul_assoc a m n] at h4\n  show c = a * (m * n) from h4\n  done\nAs usual, you might find it instructive to compare the Lean proof above to the proof of this theorem in HTPI.\nFor our next example, we’ll do a somewhat more complex proof concerning divisibility. Here is the proof from HTPI (HTPI p. 139).\n\nFor every integer \\(n\\), \\(6 \\mid n\\) iff \\(2 \\mid n\\) and \\(3 \\mid n\\).\n\n\nProof. Let \\(n\\) be an arbitrary integer.\n(\\(\\to\\)) Suppose \\(6 \\mid n\\). Then we can choose an integer \\(k\\) such that \\(6k=n\\). Therefore \\(n = 6k = 2(3k)\\), so \\(2 \\mid n\\), and similarly \\(n = 6k = 3(2k)\\), so \\(3 \\mid n\\).\n(\\(\\leftarrow\\)) Suppose \\(2 \\mid n\\) and \\(3 \\mid n\\). Then we can choose integers \\(j\\) and \\(k\\) such that \\(n = 2j\\) and \\(n = 3k\\). Therefore \\(6(j-k) = 6j - 6k = 3(2j) - 2(3k) = 3n - 2n = n\\), so \\(6 \\mid n\\).  □\n\nLet’s try writing the proof in Lean. We use exactly the same strategy as in the HTPI proof: we begin by fixing an arbitrary integer n, and then we prove the two directions of the biconditional separately.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  **done::\n\n\ncase mp\nn : ℤ\n⊢ 6 ∣ n → 2 ∣ n ∧ 3 ∣ n\ncase mpr\nn : ℤ\n⊢ 2 ∣ n ∧ 3 ∣ n → 6 ∣ n\n\n\nFor the left-to-right direction, we assume 6 ∣ n, and since the definition of this assumption is an existential statement, we immediately apply existential instantiation.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  · -- (→)\n    assume h1 : 6 ∣ n; define at h1\n    obtain (k : Int) (h2 : n = 6 * k) from h1\n    **done::\n  · -- (←)\n\n    **done::\n  done\n\n\ncase mp\nn : ℤ\nh1 : ∃ (c : ℤ),\n>>  n = 6 * c\nk : ℤ\nh2 : n = 6 * k\n⊢ 2 ∣ n ∧ 3 ∣ n\n\n\nOur goal is now a conjunction, so we prove the two conjuncts separately. Focusing just on the first one, 2 ∣ n, we write out the definition to decide how to proceed.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  · -- (→)\n    assume h1 : 6 ∣ n; define at h1\n    obtain (k : Int) (h2 : n = 6 * k) from h1\n    apply And.intro\n    · -- Proof that 2 ∣ n\n      define\n      **done::\n    · -- Proof that 3 ∣ n\n\n      **done::\n    done\n  · -- (←)\n\n    **done::\n  done\n\n\ncase mp.left\nn : ℤ\nh1 : ∃ (c : ℤ),\n>>  n = 6 * c\nk : ℤ\nh2 : n = 6 * k\n⊢ ∃ (c : ℤ), n = 2 * c\n\n\nSince we have n = 6 * k = 2 * 3 * k, it looks like 3 * k is the value we should use for c.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  · -- (→)\n    assume h1 : 6 ∣ n; define at h1\n    obtain (k : Int) (h2 : n = 6 * k) from h1\n    apply And.intro\n    · -- Proof that 2 ∣ n\n      define\n      apply Exists.intro (3 * k)\n      **done::\n    · -- Proof that 3 ∣ n\n\n      **done::\n    done\n  · -- (←)\n\n    **done::\n  done\n\n\ncase mp.left\nn : ℤ\nh1 : ∃ (c : ℤ),\n>>  n = 6 * c\nk : ℤ\nh2 : n = 6 * k\n⊢ n = 2 * (3 * k)\n\n\nOnce again, if you think carefully about it, you will see that in order to deduce the goal from h2, we will need to use the associativity of multiplication to rewrite the goal as n = 2 * 3 * k. As we have already seen, mul_assoc 2 3 k is a proof of 2 * 3 * k = 2 * (3 * k). Since we want to replace the right side of this equation with the left in the goal, we’ll use the tactic rewrite [←mul_assoc 2 3 k].\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  · -- (→)\n    assume h1 : 6 ∣ n; define at h1\n    obtain (k : Int) (h2 : n = 6 * k) from h1\n    apply And.intro\n    · -- Proof that 2 ∣ n\n      define\n      apply Exists.intro (3 * k)\n      rewrite [←mul_assoc 2 3 k]\n      **done::\n    · -- Proof that 3 ∣ n\n\n      **done::\n    done\n  · -- (←)\n\n    **done::\n  done\n\n\ncase mp.left\nn : ℤ\nh1 : ∃ (c : ℤ),\n>>  n = 6 * c\nk : ℤ\nh2 : n = 6 * k\n⊢ n = 2 * 3 * k\n\n\nDo we have to convince Lean that 2 * 3 = 6? No, remember that Lean works out definitions on its own. Lean knows the definition of multiplication, and it knows that, according to that definition, 2 * 3 is equal to 6. So it regards n = 6 * k and n = 2 * 3 * k as definitionally equal, and therefore it will recognize h2 as a proof of the goal.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  · -- (→)\n    assume h1 : 6 ∣ n; define at h1\n    obtain (k : Int) (h2 : n = 6 * k) from h1\n    apply And.intro\n    · -- Proof that 2 ∣ n\n      define\n      apply Exists.intro (3 * k)\n      rewrite [←mul_assoc 2 3 k]\n      show n = 2 * 3 * k from h2\n      done\n    · -- Proof that 3 ∣ n\n\n      **done::\n    done\n  · -- (←)\n\n    **done::\n  done\n\n\ncase mp.right\nn : ℤ\nh1 : ∃ (c : ℤ),\n>>  n = 6 * c\nk : ℤ\nh2 : n = 6 * k\n⊢ 3 ∣ n\n\n\nThe proof of the next goal, 3 ∣ n, is similar, and it completes the left-to-right direction of the biconditional.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  · -- (→)\n    assume h1 : 6 ∣ n; define at h1\n    obtain (k : Int) (h2 : n = 6 * k) from h1\n    apply And.intro\n    · -- Proof that 2 ∣ n\n      define\n      apply Exists.intro (3 * k)\n      rewrite [←mul_assoc 2 3 k]\n      show n = 2 * 3 * k from h2\n      done\n    · -- Proof that 3 ∣ n\n      define\n      apply Exists.intro (2 * k)\n      rewrite [←mul_assoc 3 2 k]\n      show n = 3 * 2 * k from h2\n      done\n    done\n  · -- (←)\n\n    **done::\n  done\n\n\ncase mpr\nn : ℤ\n⊢ 2 ∣ n ∧ 3 ∣ n → 6 ∣ n\n\n\nFor the right-to-left direction, we begin by assuming 2 ∣ n ∧ 3 ∣ n. We write out the definitions of 2 ∣ n and 3 ∣ n, and since this gives us two existential givens, we apply existential instantiation twice. To save space, we won’t repeat the proof of the first half of the proof in the displays below.\n\n\ntheorem Theorem_3_4_7 :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  · -- (→)\n    ...\n  · -- (←)\n    assume h1 : 2 ∣ n ∧ 3 ∣ n\n    have h2 : 2 ∣ n := h1.left\n    have h3 : 3 ∣ n := h1.right\n    define at h2; define at h3; define\n    obtain (j : Int) (h4 : n = 2 * j) from h2\n    obtain (k : Int) (h5 : n = 3 * k) from h3\n    **done::\n  done\n\n\ncase mpr\nn : ℤ\nh1 : 2 ∣ n ∧ 3 ∣ n\nh2 : ∃ (c : ℤ),\n>>  n = 2 * c\nh3 : ∃ (c : ℤ),\n>>  n = 3 * c\nj : ℤ\nh4 : n = 2 * j\nk : ℤ\nh5 : n = 3 * k\n⊢ ∃ (c : ℤ),\n>>  n = 6 * c\n\n\nThe next step in the HTPI proof is a string of equations that proves \\(6(j - k) = n\\), which establishes that \\(6 \\mid n\\). Let’s try to do the same thing in Lean, using a calculational proof:\n\n\ntheorem ??Theorem_3_4_7:: :\n    ∀ (n : Int), 6 ∣ n ↔ 2 ∣ n ∧ 3 ∣ n := by\n  fix n : Int\n  apply Iff.intro\n  · -- (→)\n    ...\n  · -- (←)\n    assume h1 : 2 ∣ n ∧ 3 ∣ n\n    have h2 : 2 ∣ n := h1.left\n    have h3 : 3 ∣ n := h1.right\n    define at h2; define at h3; define\n    obtain (j : Int) (h4 : n = 2 * j) from h2\n    obtain (k : Int) (h5 : n = 3 * k) from h3\n    have h6 : 6 * (j - k) = n :=\n      calc 6 * (j - k)\n        _ = 6 * j - 6 * k := sorry\n        _ = 3 * (2 * j) - 2 * (3 * k) := sorry\n        _ = 3 * n - 2 * n := sorry\n        _ = (3 - 2) * n := sorry\n        _ = n := sorry\n    show ∃ (c : Int), n = 6 * c from\n      Exists.intro (j - k) h6.symm\n    done\n  done\n\n\nNo goals\n\n\nSometimes the easiest way to write a calculational proof is to justify each line with sorry and then go back and fill in real justifications. Lean has accepted the proof above, so we know that we’ll have a complete proof if we can replace each sorry with a justification.\nTo justify the first line of the calculational proof, try replacing sorry with by apply?. Lean comes up with a justification: Int.mul_sub 6 j k. The command #check @Int.mul_sub tells us that the theorem Int.mul_sub means\n\nInt.mul_sub : ∀ (a b c : ℤ), a * (b - c) = a * b - a * c\n\nThus, we can fill in Int.mul_sub 6 j k as a proof of the first equation.\nIt looks like we’ll have to use the associativity of multiplication again to prove the second equation, but it will take more than one step. Let’s try writing a tactic-mode proof. In the display below, we’ll just focus on the calculational proof.\n\n\nhave h6 : 6 * (j - k) = n :=\n  calc 6 * (j - k)\n    _ = 6 * j - 6 * k := Int.mul_sub 6 j k\n    _ = 3 * (2 * j) - 2 * (3 * k) := by\n\n          **done::\n    _ = 3 * n - 2 * n := sorry\n    _ = (3 - 2) * n := sorry\n    _ = n := sorry\n\n\nn : ℤ\nh1 : 2 ∣ n ∧ 3 ∣ n\nh2 : ∃ (c : ℤ),\n>>  n = 2 * c\nh3 : ∃ (c : ℤ),\n>>  n = 3 * c\nj : ℤ\nh4 : n = 2 * j\nk : ℤ\nh5 : n = 3 * k\n⊢ 6 * j - 6 * k =\n>>  3 * (2 * j) -\n>>    2 * (3 * k)\n\n\nTo justify the second equation, we’ll have to use associativity to rewrite both 3 * (2 * j) as 3 * 2 * j and also 2 * (3 * k) as 2 * 3 * k. So we apply the rewrite tactic to both of the proofs mul_assoc 3 2 j : 3 * 2 * j = 3 * (2 * j) and mul_assoc 2 3 k : 2 * 3 * k = 2 * (3 * k):\n\n\nhave h6 : 6 * (j - k) = n :=\n  calc 6 * (j - k)\n    _ = 6 * j - 6 * k := Int.mul_sub 6 j k\n    _ = 3 * (2 * j) - 2 * (3 * k) := by\n          rewrite [←mul_assoc 3 2 j]\n          rewrite [←mul_assoc 2 3 k]\n          **done::\n    _ = 3 * n - 2 * n := sorry\n    _ = (3 - 2) * n := sorry\n    _ = n := sorry\n\n\nn : ℤ\nh1 : 2 ∣ n ∧ 3 ∣ n\nh2 : ∃ (c : ℤ),\n>>  n = 2 * c\nh3 : ∃ (c : ℤ),\n>>  n = 3 * c\nj : ℤ\nh4 : n = 2 * j\nk : ℤ\nh5 : n = 3 * k\n⊢ 6 * j - 6 * k =\n>>  3 * 2 * j - \n>>    2 * 3 * k\n\n\nTo finish off the justification of the second equation, we’ll use the theorem Eq.refl. The command #check @Eq.refl gives the result\n\n@Eq.refl : ∀ {α : Sort u_1} (a : α), a = a\n\nIgnoring the implicit argument α, this should remind you of the theorem Iff.refl : ∀ (a : Prop), a ↔︎ a. Recall that we were able to use Iff.refl _ to prove not only any statement of the form a ↔︎ a, but also statements of the form a ↔︎ a', where a and a' are definitionally equal. Similarly, Eq.refl _ will prove any equation of the form a = a', where a and a' are definitionally equal. Since Lean knows that, by definition, 3 * 2 = 6 and 2 * 3 = 6, the goal has this form. Thus we can complete the proof with the tactic show 6 * j - 6 * k = 3 * 2 * j - 2 * 3 * k from Eq.refl _. As we saw earlier, a shorter version of this would be exact Eq.refl _. But this situation comes up often enough that there is an even shorter version: the tactic rfl can be used as a shorthand for either exact Eq.refl _ or exact Iff.refl _. In other words, in a tactic-mode proof, if the goal has one of the forms a = a' or a ↔︎ a', where a and a' are definitionally equal, then the tactic rfl will prove the goal. So rfl will finish off the justification of the second equation, and we can move on to the third.\n\n\nhave h6 : 6 * (j - k) = n :=\n  calc 6 * (j - k)\n    _ = 6 * j - 6 * k := Int.mul_sub 6 j k\n    _ = 3 * (2 * j) - 2 * (3 * k) := by\n          rewrite [←mul_assoc 3 2 j]\n          rewrite [←mul_assoc 2 3 k]\n          rfl\n          done\n    _ = 3 * n - 2 * n := by\n\n          **done::\n    _ = (3 - 2) * n := sorry\n    _ = n := sorry\n\n\nn : ℤ\nh1 : 2 ∣ n ∧ 3 ∣ n\nh2 : ∃ (c : ℤ),\n>>  n = 2 * c\nh3 : ∃ (c : ℤ),\n>>  n = 3 * c\nj : ℤ\nh4 : n = 2 * j\nk : ℤ\nh5 : n = 3 * k\n⊢ 3 * (2 * j) -\n>>  2 * (3 * k) =\n>>    3 * n - 2 * n\n\n\nTo justify the third equation we have to substitute n for both 2 * j and 3 * k. We can use h4 and h5 in the rewrite tactic to do this. In fact, we can do it in one step: you can put a list of proofs of equations or biconditionals inside the brackets, and the rewrite tactic will perform all of the replacements, one after another. In our case, the tactic rewrite [←h4, ←h5] will first replace 2 * j in the goal with n, and then it will replace 3 * k with n.\n\n\nhave h6 : 6 * (j - k) = n :=\n  calc 6 * (j - k)\n    _ = 6 * j - 6 * k := Int.mul_sub 6 j k\n    _ = 3 * (2 * j) - 2 * (3 * k) := by\n          rewrite [←mul_assoc 3 2 j]\n          rewrite [←mul_assoc 2 3 k]\n          rfl\n          done\n    _ = 3 * n - 2 * n := by\n          rewrite [←h4, ←h5]\n          **done::\n    _ = (3 - 2) * n := sorry\n    _ = n := sorry\n\n\nn : ℤ\nh1 : 2 ∣ n ∧ 3 ∣ n\nh2 : ∃ (c : ℤ),\n>>  n = 2 * c\nh3 : ∃ (c : ℤ),\n>>  n = 3 * c\nj : ℤ\nh4 : n = 2 * j\nk : ℤ\nh5 : n = 3 * k\n⊢ 3 * n - 2 * n =\n>>  3 * n - 2 * n\n\n\nOf course, the rfl tactic will now finish off the justification of the third equation.\nThe fourth equation is 3 * n - 2 * n = (3 - 2) * n. It looks like the algebraic law we need to justify this is a lot like the one that was used in the first equation, but with the multiplication to the right of the subtraction rather than to the left. It shouldn’t be surprising, therefore, that the name of the theorem we need is Int.sub_mul. The command #check @Int.sub_mul gives the response\n\nInt.sub_mul : ∀ (a b c : ℤ), (a - b) * c = a * c - b * c\n\nso Int.sub_mul 3 2 n is a proof of (3 - 2) * n = 3 * n - 2 * n. But the fourth equation has the sides of this equation reversed, so to justify it we need (Int.sub_mul 3 2 n).symm.\nFinally, the fifth equation is (3 - 2) * n = n. Why is this true? Because it is definitionally equal to 1 * n = n. Is there a theorem to justify this last equation? One way to find the answer is to type in this example:\nexample (n : Int) : 1 * n = n := by ++apply?::\nLean responds with exact Int.one_mul n, and #check @Int.one_mul yields\n\nInt.one_mul : ∀ (a : ℤ), 1 * a = a\n\nSo Int.one_mul n should justify the last equation. Here’s the complete calculational proof, where we have shortened the second step a bit by doing both rewrites in one step. When a tactic proof is short enough that it can be written on one line, we generally leave off done.\nhave h6 : 6 * (j - k) = n :=\n  calc 6 * (j - k)\n    _ = 6 * j - 6 * k := Int.mul_sub 6 j k\n    _ = 3 * (2 * j) - 2 * (3 * k) := by\n          rewrite [←mul_assoc 3 2 j, ←mul_assoc 2 3 k]; rfl\n    _ = 3 * n - 2 * n := by rewrite [←h4, ←h5]; rfl\n    _ = (3 - 2) * n := (Int.sub_mul 3 2 n).symm\n    _ = n := Int.one_mul n\nWhew! This example illustrates why algebraic reasoning in Lean can be difficult. But one reason why this proof was challenging is that we justified all of our steps from basic algebraic principles. Fortunately, there are more powerful tactics that can automate some algebraic reasoning. For example, the tactic ring can combine algebraic laws involving addition, subtraction, multiplication, and exponentiation with natural number exponents to prove many equations in one step. Also, the tactic rw is a variant of rewrite that automatically applies rfl after the rewriting if it can be used to finish the proof. Here’s a shortened version of our calculational proof that uses these tactics.\nhave h6 : 6 * (j - k) = n :=\n  calc 6 * (j - k)\n    _ = 3 * (2 * j) - 2 * (3 * k) := by ring\n    _ = 3 * n - 2 * n := by rw [←h4, ←h5]\n    _ = n := by ring\nBy the way, the theorems Int.mul_sub, Int.sub_mul, and Int.one_mul that we used earlier are the integer versions of more general theorems mul_sub, sub_mul, and one_mul. The #check command tells us what these general theorems say:\n\n@mul_sub : ∀ {α : Type u_1} [inst : NonUnitalNonAssocRing α]\n            (a b c : α), a * (b - c) = a * b - a * c\n\n@sub_mul : ∀ {α : Type u_1} [inst : NonUnitalNonAssocRing α]\n            (a b c : α), (a - b) * c = a * c - b * c\n\n@one_mul : ∀ {M : Type u_1} [inst : MulOneClass M]\n            (a : M), 1 * a = a\n\nThe implicit arguments say that these theorems apply in any number system with the appropriate algebraic properties. We’ll use the third theorem in our next example, which involves algebraic reasoning about real numbers. You can use the #check command to find the meanings of the other theorems we use in this proof.\ntheorem Example_3_5_4 (x : Real) (h1 : x ≤ x ^ 2) : x ≤ 0 ∨ 1 ≤ x := by\n  or_right with h2     --h2 : ¬x ≤ 0;  Goal : 1 ≤ x\n  have h3 : 0 < x := lt_of_not_le h2\n  have h4 : 1 * x ≤ x * x :=\n    calc 1 * x\n      _ = x := one_mul x\n      _ ≤ x ^ 2 := h1\n      _ = x * x := by ring\n  show 1 ≤ x from le_of_mul_le_mul_right h4 h3\n  done\n\nExercises\n\ntheorem Exercise_3_3_18a (a b c : Int)\n    (h1 : a ∣ b) (h2 : a ∣ c) : a ∣ (b + c) := sorry\n\n2. Complete the following proof by justifying the steps in the calculational proof. Remember that you can use the tactic demorgan : ... to apply one of De Morgan’s laws to just a part of the goal. You may also find the theorem and_or_left useful. (Use #check to see what the theorem says.)\ntheorem Exercise_3_4_6 (U : Type) (A B C : Set U) :\n    A \\ (B ∩ C) = (A \\ B) ∪ (A \\ C) := by\n  apply Set.ext\n  fix x : U\n  show x ∈ A \\ (B ∩ C) ↔ x ∈ A \\ B ∪ A \\ C from\n    calc x ∈ A \\ (B ∩ C)\n      _ ↔ x ∈ A ∧ ¬(x ∈ B ∧ x ∈ C) := sorry\n      _ ↔ x ∈ A ∧ (x ∉ B ∨ x ∉ C) := sorry  \n      _ ↔ (x ∈ A ∧ x ∉ B) ∨ (x ∈ A ∧ x ∉ C) := sorry\n      _ ↔ x ∈ (A \\ B) ∪ (A \\ C) := sorry\n  done\n\n\n\nFor the next exercise you will need the following definitions:\ndef even (n : Int) : Prop := ∃ (k : Int), n = 2 * k\n\ndef odd (n : Int) : Prop := ∃ (k : Int), n = 2 * k + 1\nThese definitions tell Lean that if n has type Int, then even n means ∃ (k : Int), n = 2 * k and odd n means ∃ (k : Int), n = 2 * k + 1.\n\ntheorem Exercise_3_4_10 (x y : Int)\n    (h1 : odd x) (h2 : odd y) : even (x - y) := sorry\n\n\ntheorem Exercise_3_4_27a :\n    ∀ (n : Int), 15 ∣ n ↔ 3 ∣ n ∧ 5 ∣ n := sorry\n\n\ntheorem Like_Exercise_3_7_5 (U : Type) (F : Set (Set U))\n    (h1 : 𝒫 (⋃₀ F) ⊆ ⋃₀ {𝒫 A | A ∈ F}) :\n    ∃ (A : Set U), A ∈ F ∧ ∀ (B : Set U), B ∈ F → B ⊆ A := sorry"
  },
  {
    "objectID": "Chap4.html",
    "href": "Chap4.html",
    "title": "4  Relations",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\mathbin{∆}}\n$$"
  },
  {
    "objectID": "Chap4.html#ordered-pairs-and-cartesian-products",
    "href": "Chap4.html#ordered-pairs-and-cartesian-products",
    "title": "4  Relations",
    "section": "4.1. Ordered Pairs and Cartesian Products",
    "text": "4.1. Ordered Pairs and Cartesian Products\nSection 4.1 of How To Prove It defines the Cartesian product \\(A \\times B\\) of two sets \\(A\\) and \\(B\\) to be the set of all ordered pairs \\((a, b)\\), where \\(a \\in A\\) and \\(b \\in B\\). However, in Lean, Cartesian product is an operation on types, not sets. If A and B are types, then A × B is the type of ordered pairs (a, b), where a has type A and b has type B. (To enter the symbol × in Lean, type \\times or \\x.) In other words, if you have a : A and b : B, then (a, b) is an object of type A × B. There is also notation for the first and second coordinates of an ordered pair. If p has type A × B, then p.fst is the first coordinate of p, and p.snd is the second coordinate. You can also use the notation p.1 for the first coordinate of p and p.2 for the second coordinate. This means that p = (p.fst, p.snd) = (p.1, p.2)."
  },
  {
    "objectID": "Chap4.html#relations",
    "href": "Chap4.html#relations",
    "title": "4  Relations",
    "section": "4.2. Relations",
    "text": "4.2. Relations\nSection 4.2 of HTPI defines a relation from \\(A\\) to \\(B\\) to be a subset of \\(A \\times B\\). In other words, if \\(R\\) is a relation from \\(A\\) to \\(B\\), then \\(R\\) is a set whose element are ordered pairs \\((a, b)\\), where \\(a \\in A\\) and \\(b \\in B\\). We will see in the next section that in Lean, it is convenient to use a somewhat different definition of relations. Nevertheless, we will take some time in this section to study sets of ordered pairs. If A and B are types, and R has type Set (A × B), then R is a set whose elements are ordered pairs (a, b), where a has type A and b has type B.\nSection 4.2 of HTPI discusses several concepts concerning relations. Here is how these concepts are defined in HTPI (HTPI p. 183):\n\nSuppose \\(R\\) is a relation from \\(A\\) to \\(B\\). Then the domain of \\(R\\) is the set\n\n\\(\\text{Dom}(R) = \\{a \\in A \\mid \\exists b \\in B((a, b) \\in R)\\}\\).\n\nThe range of \\(R\\) is the set\n\n\\(\\text{Ran}(R) = \\{b \\in B \\mid \\exists a \\in A((a, b) \\in R)\\}\\).\n\nThe inverse of \\(R\\) is the relation \\(R^{-1}\\) from \\(B\\) to \\(A\\) define as follows:\n\n\\(R^{-1} = \\{(b, a) \\in B \\times A \\mid (a, b) \\in R\\}\\).\n\nFinally, suppose \\(R\\) is a relation from \\(A\\) to \\(B\\) and \\(S\\) is a relation from \\(B\\) to \\(C\\). Then the composition of \\(S\\) and \\(R\\) is the relation \\(S \\circ R\\) from \\(A\\) to \\(C\\) defined as follows:\n\n\\(S \\circ R = \\{(a, c) \\in A \\times C \\mid \\exists b \\in B((a, b) \\in R \\text{ and } (b, c) \\in S)\\}\\).\n\n\nThere are several examples in HTPI that illustrate these definitions. We will focus here on seeing how to work with these concepts in Lean.\nWe can write corresponding definitions in Lean as follows:\ndef Dom {A B : Type} (R : Set (A × B)) : Set A :=\n  {a : A | ∃ (b : B), (a, b) ∈ R}\n\ndef Ran {A B : Type} (R : Set (A × B)) : Set B :=\n  {b : B | ∃ (a : A), (a, b) ∈ R}\n\ndef inv {A B : Type} (R : Set (A × B)) : Set (B × A) :=\n  {(b, a) : B × A | (a, b) ∈ R}\n\ndef comp {A B C : Type}\n    (S : Set (B × C)) (R : Set (A × B)) : Set (A × C) :=\n  {(a, c) : A × C | ∃ (x : B), (a, x) ∈ R ∧ (x, c) ∈ S}\nDefinitions in Lean are introduced with the keyword def. In the definition of Dom, we have declared that A and B are implicit arguments and R is an explicit argument. That means that, in a Lean file containing these definitions, if we have R : Set (A × B), then we can just write Dom R for the domain of R, and Lean will figure out for itself what A and B are. After the list of arguments there is a colon and then the type of Dom R, which is Set A. This is followed by := and then the definition of Dom R. The definition says that Dom R is the set of all objects a of type A such that there is some b of type B with (a, b) ∈ R. This is a direct translation, into Lean’s type-theory language, of the first part of Definition 4.2.3. The other three definitions are similar; they define Ran R to be the range of R, inv R to be the inverse of R, and comp S R to be the composition of S and R.\nHere is the main theorem about these concepts, as stated in HTPI (HTPI p. 187):\n\nSuppose \\(R\\) is a relation from \\(A\\) to \\(B\\), \\(S\\) is a relation from \\(B\\) to \\(C\\), and \\(T\\) is a relation from \\(C\\) to \\(D\\). Then:\n\n\\((R^{-1})^{-1} = R\\).\n\\(\\mathrm{Dom}(R^{-1}) = \\mathrm{Ran}(R)\\).\n\\(\\mathrm{Ran}(R^{-1}) = \\mathrm{Dom}(R)\\).\n\\(T \\circ (S \\circ R) = (T \\circ S) \\circ R\\).\n\\((S \\circ R)^{-1} = R^{-1} \\circ S^{-1}\\).\n\n\nAll five parts of this theorem follow directly from the definitions of the relevant concepts. In fact, in the first three parts, Lean recognizes the two sides of the equation as being definitionally equal, and therefore the tactic rfl proves those parts:\ntheorem Theorem_4_2_5_1 {A B : Type}\n    (R : Set (A × B)) : inv (inv R) = R := by rfl\n\ntheorem Theorem_4_2_5_2 {A B : Type}\n    (R : Set (A × B)) : Dom (inv R) = Ran R := by rfl\n\ntheorem Theorem_4_2_5_3 {A B : Type}\n    (R : Set (A × B)) : Ran (inv R) = Dom R := by rfl\nThe fourth part will take a little more work to prove. We start the proof like this:\ntheorem Theorem_4_2_5_4 {A B C D : Type}\n    (R : Set (A × B)) (S : Set (B × C)) (T : Set (C × D)) :\n    comp T (comp S R) = comp (comp T S) R := by\n  apply Set.ext\n  fix (a, d) : A × D\n  **done::\nAfter the apply Set.ext tactic, the goal is\n\n∀ (x : A × D), x ∈ comp T (comp S R) ↔︎ x ∈ comp (comp T S) R\n\nThe next step should be to introduce an arbitrary object of type A × D. We could just call this object x, but Lean lets us use a shortcut here. An object of type A × D must have the form of an ordered pair, where the first coordinate has type A and the second has type D. So Lean lets us write it as an ordered pair right away. That’s what we’ve done in the second step, fix (a, d) : A × D. This tactic introduces two new variables into the proof, a : A and d : D. (The proof in HTPI uses a similar shortcut. And we used a similar shortcut in the definitions of inv R and comp R, where the elements of these sets were written as ordered pairs.)\nHere is the complete proof.\ntheorem Theorem_4_2_5_4 {A B C D : Type}\n    (R : Set (A × B)) (S : Set (B × C)) (T : Set (C × D)) :\n    comp T (comp S R) = comp (comp T S) R := by\n  apply Set.ext\n  fix (a, d) : A × D\n  apply Iff.intro\n  · -- (→)\n    assume h1 : (a, d) ∈ comp T (comp S R)\n                     --Goal : (a, d) ∈ comp (comp T S) R\n    define           --Goal : ∃ (x : B), (a, x) ∈ R ∧ (x, d) ∈ comp T S\n    define at h1     --h1 : ∃ (x : C), (a, x) ∈ comp S R ∧ (x, d) ∈ T\n    obtain (c : C) (h2 : (a, c) ∈ comp S R ∧ (c, d) ∈ T) from h1\n    have h3 : (a, c) ∈ comp S R := h2.left\n    define at h3     --h3 : ∃ (x : B), (a, x) ∈ R ∧ (x, c) ∈ S\n    obtain (b : B) (h4 : (a, b) ∈ R ∧ (b, c) ∈ S) from h3\n    apply Exists.intro b    --Goal : (a, b) ∈ R ∧ (b, d) ∈ comp T S\n    apply And.intro h4.left --Goal : (b, d) ∈ comp T S\n    define                  --Goal : ∃ (x : C), (b, x) ∈ S ∧ (x, d) ∈ T\n    show ∃ (x : C), (b, x) ∈ S ∧ (x, d) ∈ T from\n      Exists.intro c (And.intro h4.right h2.right)\n    done\n  · -- (←)\n    assume h1 : (a, d) ∈ comp (comp T S) R\n    define; define at h1\n    obtain (b : B) (h2 : (a, b) ∈ R ∧ (b, d) ∈ comp T S) from h1\n    have h3 : (b, d) ∈ comp T S := h2.right\n    define at h3\n    obtain (c : C) (h4 : (b, c) ∈ S ∧ (c, d) ∈ T) from h3\n    apply Exists.intro c\n    apply And.intro _ h4.right\n    define\n    show ∃ (x : B), (a, x) ∈ R ∧ (x, c) ∈ S from\n      Exists.intro b (And.intro h2.left h4.left)\n    done\n  done\nOf course, if you have trouble reading this proof, you can enter it into Lean and see how the tactic state changes over the course of the proof.\nHere is a natural way to start the proof of part 5:\ntheorem Theorem_4_2_5_5 {A B C : Type}\n    (R : Set (A × B)) (S : Set (B × C)) :\n    inv (comp S R) = comp (inv R) (inv S) := by\n  apply Set.ext\n  fix (c, a) : C × A\n  apply Iff.intro\n  · -- (→)\n    assume h1 : (c, a) ∈ inv (comp S R)\n                      --Goal : (c, a) ∈ comp (inv R) (inv S)\n    define at h1      --h1 : ∃ (x : B), (a, x) ∈ R ∧ (x, c) ∈ S\n    define            --Goal : ∃ (x : B), (c, x) ∈ inv S ∧ (x, a) ∈ inv R\n    obtain (b : B) (h2 : (a, b) ∈ R ∧ (b, c) ∈ S) from h1\n    apply Exists.intro b         --Goal : (c, b) ∈ inv S ∧ (b, a) ∈ inv R\n    **done::\n  · -- (←)\n\n    **done::\n  done\nAfter the tactics apply Set.ext and fix (c, a) : C × A, the goal is (c, a) ∈ inv (comp S R) ↔︎ (c, a) ∈ comp (inv R) (inv S). For the proof of the left-to-right direction, we assume h1 : (c, a) ∈ inv (comp S R), and we must prove (c, a) ∈ comp (inv R) (inv S). The definition of h1 is an existential statement, so we apply existential instantiation to obtain b : B and h2 : (a, b) ∈ R ∧ (b, c) ∈ S. The definition of the goal is also an existential statement, and after the tactic apply Exists.intro b, the goal is (c, b) ∈ inv S ∧ (b, a) ∈ inv R. It looks like this goal will follow easily from h2, using the definitions of the inverses of S and R.\nOne way to write out these definitions would be to use the tactics define : (c, b) ∈ inv S and define : (b, a) ∈ inv R. But we’re going to use this example to illustrate another way to proceed. To use this alternative method, we’ll need to prove a preliminary theorem before proving part 5 of Theorem 4.2.5:\ntheorem inv_def {A B : Type} (R : Set (A × B)) (a : A) (b : B) :\n    (b, a) ∈ inv R ↔ (a, b) ∈ R := by rfl\nNow, any time we have a relation R : Set (A × B) and objects a : A and b : B, the expression inv_def R a b will be a proof of the statement (b, a) ∈ inv R ↔︎ (a, b) ∈ R. (Note that A and B are implicit arguments and don’t need to be specified.) And that means that the tactic rewrite [inv_def R a b] will change (b, a) ∈ inv R to (a, b) ∈ R. In fact, as we’ve seen before, you can just write rewrite [inv_def], and Lean will figure out how to apply the theorem inv_def to rewrite some part of the goal.\nReturning to our proof of part 5 of Theorem 4.2.5, recall that after the step apply Exists.intro b, the goal is (c, b) ∈ inv S ∧ (b, a) ∈ inv R. Rather than using the define tactic to write out the definitions of the inverses, we’ll use the tactic rewrite [inv_def, inv_def]. Why do we list inv_def twice in the rewrite tactic? When we ask Lean to use the theorem inv_def as a rewriting rule, it figures out that inv_def S b c is a proof of the statement (c, b) ∈ inv S ↔︎ (b, c) ∈ S, which can be used to rewrite the left half of the goal. To rewrite the right half, we need a different application of the inv_def theorem, inv_def R a b. So we have to ask Lean to apply the theorem a second time. After the rewrite tactic, the goal is (b, c) ∈ S ∧ (a, b) ∈ R, which will follow easily from h2.\nThe rest of the proof of straightforward. Here is the complete proof.\ntheorem Theorem_4_2_5_5 {A B C : Type}\n    (R : Set (A × B)) (S : Set (B × C)) :\n    inv (comp S R) = comp (inv R) (inv S) := by\n  apply Set.ext\n  fix (c, a) : C × A\n  apply Iff.intro\n  · -- (→)\n    assume h1 : (c, a) ∈ inv (comp S R)\n                      --Goal : (c, a) ∈ comp (inv R) (inv S)\n    define at h1      --h1 : ∃ (x : B), (a, x) ∈ R ∧ (x, c) ∈ S\n    define            --Goal : ∃ (x : B), (c, x) ∈ inv S ∧ (x, a) ∈ inv R\n    obtain (b : B) (h2 : (a, b) ∈ R ∧ (b, c) ∈ S) from h1\n    apply Exists.intro b         --Goal : (c, b) ∈ inv S ∧ (b, a) ∈ inv R\n    rewrite [inv_def, inv_def] --Goal : (b, c) ∈ S ∧ (a, b) ∈ R\n    show (b, c) ∈ S ∧ (a, b) ∈ R from And.intro h2.right h2.left\n    done\n  · -- (←)\n    assume h1 : (c, a) ∈ comp (inv R) (inv S)\n    define at h1\n    define\n    obtain (b : B) (h2 : (c, b) ∈ inv S ∧ (b, a) ∈ inv R) from h1\n    apply Exists.intro b\n    rewrite [inv_def, inv_def] at h2\n    show (a, b) ∈ R ∧ (b, c) ∈ S from And.intro h2.right h2.left\n    done\n  done\nBy the way, an alternative way to complete both directions of this proof would have been to apply the commutativity of “and”. See if you can guess the name of that theorem (you can use #check to confirm your guess) and apply it as a third rewriting rule in the rewrite steps.\n\nExercises\n\ntheorem Exercise_4_2_9a {A B C : Type} (R : Set (A × B))\n    (S : Set (B × C)) : Dom (comp S R) ⊆ Dom R := sorry\n\n\ntheorem Exercise_4_2_9b {A B C : Type} (R : Set (A × B))\n    (S : Set (B × C)) : Ran R ⊆ Dom S → Dom (comp S R) = Dom R := sorry\n\n\n--Fill in the blank to get a correct theorem and then prove the theorem\ntheorem Exercise_4_2_9c {A B C : Type} (R : Set (A × B))\n    (S : Set (B × C)) : ___ → Ran (comp S R) = Ran S := sorry\n\n\ntheorem Exercise_4_2_12a {A B C : Type}\n    (R : Set (A × B)) (S T : Set (B × C)) :\n    (comp S R) \\ (comp T R) ⊆ comp (S \\ T) R := sorry\n\n5. Here is an incorrect theorem with an incorrect proof.\n\nSuppose \\(R\\) is a relation from \\(A\\) to \\(B\\) and \\(S\\) and \\(T\\) are relations from \\(B\\) to \\(C\\). Then \\((S \\setmin T) \\circ R \\subseteq (S \\circ R) \\setmin (T \\circ R)\\).\n\n\nSuppose \\((a, c) \\in (S \\setmin T) \\circ R\\). Then we can choose some \\(b \\in B\\) such that \\((a, b) \\in R\\) and \\((b, c) \\in S \\setmin T\\), so \\((b, c) \\in S\\) and \\((b, c) \\notin T\\). Since \\((a, b) \\in R\\) and \\((b, c) \\in S\\), \\((a, c) \\in S \\circ R\\). Similarly, since \\((a, b) \\in R\\) and \\((b, c) \\notin T\\), \\((a, c) \\notin T \\circ R\\). Therefore \\((a, c) \\in (S \\circ R) \\setmin (T \\circ R)\\). Since \\((a, c)\\) was arbitrary, this shows that \\((S \\setmin T) \\circ R \\subseteq (S \\circ R) \\setmin (T \\circ R)\\).  □\n\nFind the mistake in the proof by attempting to write the proof in Lean:\n--You won't be able to complete this proof\ntheorem Exercise_4_2_12b {A B C : Type}\n    (R : Set (A × B)) (S T : Set (B × C)) :\n    comp (S \\ T) R ⊆ (comp S R) \\ (comp T R) := sorry\n6. Is the following theorem correct? Try to prove it in Lean. If you can’t prove it, see if you can find a counterexample.\n--You might not be able to complete this proof\ntheorem Exercise_4_2_14c {A B C : Type}\n    (R : Set (A × B)) (S T : Set (B × C)) :\n    comp (S ∩ T) R = (comp S R) ∩ (comp T R) := sorry\n7. Is the following theorem correct? Try to prove it in Lean. If you can’t prove it, see if you can find a counterexample.\n--You might not be able to complete this proof\ntheorem Exercise_4_2_14d {A B C : Type}\n    (R : Set (A × B)) (S T : Set (B × C)) :\n    comp (S ∪ T) R = (comp S R) ∪ (comp T R) := sorry"
  },
  {
    "objectID": "Chap4.html#more-about-relations",
    "href": "Chap4.html#more-about-relations",
    "title": "4  Relations",
    "section": "4.3. More About Relations",
    "text": "4.3. More About Relations\nSection 4.3 of HTPI introduces new notation for working with relations. If \\(R \\subseteq A \\times B\\), \\(a \\in A\\), and \\(b \\in B\\), then HTPI introduces the notation \\(aRb\\) as an alternative way of saying \\((a, b) \\in R\\).\nThe notation we will use in Lean is slightly different. Corresponding to the notation \\(aRb\\) in HTPI, in Lean we will use the notation R a b. And we cannot use this notation when R has type Set (A × B). Rather, we will need to introduce a new type for the variable R in the notation R a b. The name we will use for this new type is Rel A B. Thus, if R has type Rel A B, a has type A, and b has type B, then R a b is a proposition. This should remind you of the way predicates work in Lean. If we have P : Pred A, then we think of P as representing a property that an object of type A might have, and if we also have a : A, then P a is the proposition asserting that a has the property represented by P. Similarly, if we have R : Rel A B, then we can think of R as representing a relationship that might hold between an object of type A and an object of type B, and if we also have a : A and b : B, then R a b is the proposition asserting that the relationship represented by R holds between a and b.\nNotice that in HTPI, the same variable \\(R\\) is used in both the notation \\(aRb\\) and \\((a, b) \\in R\\). But in Lean, the notation R a b is used when R has type Rel A B, and the notation (a, b) ∈ R is used when R has type Set (A × B). The types Rel A B and Set (A × B) are different, so we cannot use the same variable R in the two notations. However, there is a correspondence between the two types. Suppose R has type Rel A B. If we let R' denote the set of all ordered pairs (a, b) : A × B such that the proposition R a b is true, then R' has type Set (A × B). And there is then a simple relationship between R and R': for any objects a : A and b : B, the propositions R a b and (a, b) ∈ R' are equivalent. For our work in Lean, we will say that R is a relation from A to B, and R' is the extension of R.\nWe can define the extension of a relation, and state the correspondence between a relation and its extension, in Lean as follows:\ndef extension {A B : Type} (R : Rel A B) : Set (A × B) :=\n  {(a, b) : A × B | R a b}\n\ntheorem ext_def {A B : Type} (R : Rel A B) (a : A) (b : B) :\n    (a, b) ∈ extension R ↔ R a b := by rfl\nThe rest of Chapter 4 of HTPI focuses on relations from a set to itself; in Lean, the corresponding idea is a relation from a type to itself. If A is any type and R has type Rel A A, then we will say that R is a binary relation on A. The notation BinRel A denotes the type of binary relations on A. In other words, BinRel A is just an abbreviation for Rel A A. If R is a binary relation on A, then we say that R is reflexive if for every x of type A, R x x holds. It is symmetric if for all x and y of type A, if R x y then R y x. And it is transitive if for all x, y, and z of type A, if R x y and R y z then R x z. Of course, we can tell Lean about these definitions, which correspond to Definition 4.3.2 in HTPI:\ndef reflexive {A : Type} (R : BinRel A) : Prop :=\n  ∀ (x : A), R x x\n\ndef symmetric {A : Type} (R : BinRel A) : Prop :=\n  ∀ (x y : A), R x y → R y x\n\ndef transitive {A : Type} (R : BinRel A) : Prop :=\n  ∀ (x y z : A), R x y → R y z → R x z\nOnce again, we refer you to HTPI to see examples of these concepts, and we focus here on proving theorems about these concepts in Lean. The main theorem about these concepts in Section 4.3 of HTPI is Theorem 4.3.4. Here is what it says (HTPI p. 196):\n\nSuppose \\(R\\) is a relation on a set \\(A\\).\n\n\\(R\\) is reflexive iff \\(\\{(x, y) \\in A \\times A \\mid x = y\\} \\subseteq R\\).\n\\(R\\) is symmetric iff \\(R = R^{-1}\\).\n\\(R\\) is transitive iff \\(R \\circ R \\subseteq R\\).\n\n\nWe can prove corresponding statements in Lean, but we’ll have to be careful to distinguish between the types BinRel A and Set (A × A). In HTPI, each of the three statements in the theorem uses the same letter \\(R\\) on both sides of the “iff”, but we can’t write the statements that way in Lean. In each statement, the part before “iff” uses a concept that was defined for objects of type BinRel A, whereas the part after “iff” uses concepts that only make sense for objects of type Set (A × A). So we’ll have to rephrase the statements by using the correspondence between a relation of type BinRel A and its extension, which has type Set (A × A). Here’s the Lean theorem corresponding to statement 2 of Theorem 4.3.4:\ntheorem Theorem_4_3_4_2 {A : Type} (R : BinRel A) :\n    symmetric R ↔ extension R = inv (extension R) := by\n  apply Iff.intro\n  · -- (→)\n    assume h1 : symmetric R\n    define at h1             --h1 : ∀ (x y : A), R x y → R y x\n    apply Set.ext\n    fix (a, b) : A × A\n    show (a, b) ∈ extension R ↔ (a, b) ∈ inv (extension R) from\n      calc (a, b) ∈ extension R\n        _ ↔ R a b := by rfl\n        _ ↔ R b a := Iff.intro (h1 a b) (h1 b a)\n        _ ↔ (a, b) ∈ inv (extension R) := by rfl\n    done\n  · -- (←)\n    assume h1 : extension R = inv (extension R)\n    define                   --Goal : ∀ (x y : A), R x y → R y x\n    fix a : A; fix b : A\n    assume h2 : R a b        --Goal : R b a\n    rewrite [←ext_def R, h1, inv_def, ext_def] at h2\n    show R b a from h2\n    done\n  done\nNote that near the end of the proof, we assume h2 : R a b, and our goal is R b a. We convert R a b to R b a by a sequence of rewrites. Applying the right-to-left direction of the theorem ext_def R a b converts R a b to (a, b) ∈ extension R. Then rewriting with h1 converts this to (a, b) ∈ inv (extension R), using inv_def (extension R) b a converts this to (b, a) ∈ extension R, and finally ext_def R b a produces R b a. Usually we can leave out the arguments when we use a theorem as a rewriting rule, and Lean will figure them out for itself. But in this case, if you try using ←ext_def as the first rewriting rule, you will see that Lean is unable to figure out that it should use the right-to-left direction of ext_def R a b. Supplying the first argument turns out to be enough of a hint for Lean to figure out the rest. That’s why our first rewriting rule is ←ext_def R.\nWe’ll leave the proofs of the other two statements in Theorem 4.3.4 as exercises for you.\nFor any types A and B, if we want to define a particular relation R from A to B, we can do it by specifying, for any a : A and b : B, what proposition is represented by R a b. For example, for any type A, we can define a relation elementhood A from A to Set A as follows:\ndef elementhood (A : Type) (a : A) (X : Set A) : Prop := a ∈ X\nThis definition says that if A is a type, a has type A, and X has type Set A, then elementhood A a X is the proposition a ∈ X. Thus, if elementhood A is followed by objects of type A and Set A, the result is a proposition, so elementhood A is functioning as a relation from A to Set A. For example, elementhood Int is a relation from integers to sets of integers, and elementhood Int 6 {n : Int | ∃ (k : Int), n = 2 * k} is the (true) statement that 6 is an element of the set of even integers. (You are asked to prove it in the exercises.)\nWe can also use this method to define an operation that reverses the process of forming the extension of a relation. If R has type Set (A × B), then we define RelFromExt R to be the relation whose extension is R. A few simple theorems, which follow directly from the definition, clarify the meaning of RelFromExt R.\ndef RelFromExt {A B : Type}\n    (R : Set (A × B)) (a : A) (b : B) : Prop := (a, b) ∈ R\n\ntheorem RelFromExt_def {A B : Type}\n    (R : Set (A × B)) (a : A) (b : B) :\n    RelFromExt R a b ↔ (a, b) ∈ R := by rfl\n\nexample {A B : Type} (R : Rel A B) :\n    RelFromExt (extension R) = R := by rfl\n\nexample {A B : Type} (R : Set (A × B)) :\n    extension (RelFromExt R) = R := by rfl\n\nExercises\n\nexample :\n    elementhood Int 6 {n : Int | ∃ (k : Int), n = 2 * k} := sorry\n\n\ntheorem Theorem_4_3_4_1 {A : Type} (R : BinRel A) :\n    reflexive R ↔ {(x, y) : A × A | x = y} ⊆ extension R := sorry\n\n\ntheorem Theorem_4_3_4_3 {A : Type} (R : BinRel A) :\n    transitive R ↔\n      comp (extension R) (extension R) ⊆ extension R := sorry\n\n\ntheorem Exercise_4_3_12a {A : Type} (R : BinRel A) (h1 : reflexive R) :\n    reflexive (RelFromExt (inv (extension R))) := sorry\n\n\ntheorem Exercise_4_3_12c {A : Type} (R : BinRel A) (h1 : transitive R) :\n    transitive (RelFromExt (inv (extension R))) := sorry\n\n\ntheorem Exercise_4_3_18 {A : Type}\n    (R S : BinRel A) (h1 : transitive R) (h2 : transitive S)\n    (h3 : comp (extension S) (extension R) ⊆\n      comp (extension R) (extension S)) :\n    transitive (RelFromExt (comp (extension R) (extension S))) := sorry\n\n\ntheorem Exercise_4_3_20 {A : Type} (R : BinRel A) (S : BinRel (Set A))\n    (h : ∀ (X Y : Set A), S X Y ↔ X ≠ ∅ ∧ Y ≠ ∅ ∧\n    ∀ (x y : A), x ∈ X → y ∈ Y → R x y) :\n    transitive R → transitive S := sorry\n\n\n\n\nIn the next three exercises, determine whether or not the theorem is correct.\n\n--You might not be able to complete this proof\ntheorem Exercise_4_3_13b {A : Type}\n    (R1 R2 : BinRel A) (h1 : symmetric R1) (h2 : symmetric R2) :\n    symmetric (RelFromExt ((extension R1) ∪ (extension R2))) := sorry\n\n\n--You might not be able to complete this proof\ntheorem Exercise_4_3_13c {A : Type}\n    (R1 R2 : BinRel A) (h1 : transitive R1) (h2 : transitive R2) :\n    transitive (RelFromExt ((extension R1) ∪ (extension R2))) := sorry\n\n\n--You might not be able to complete this proof\ntheorem Exercise_4_3_19 {A : Type} (R : BinRel A) (S : BinRel (Set A))\n    (h : ∀ (X Y : Set A), S X Y ↔ ∃ (x y : A), x ∈ X ∧ y ∈ Y ∧ R x y) :\n    transitive R → transitive S := sorry"
  },
  {
    "objectID": "Chap4.html#ordering-relations",
    "href": "Chap4.html#ordering-relations",
    "title": "4  Relations",
    "section": "4.4. Ordering Relations",
    "text": "4.4. Ordering Relations\nSection 4.4 of HTPI begins by defining several new concepts about binary relations. Here are the definitions, written in Lean:\ndef antisymmetric {A : Type} (R : BinRel A) : Prop :=\n  ∀ (x y : A), R x y → R y x → x = y\n\ndef partial_order {A : Type} (R : BinRel A) : Prop :=\n  reflexive R ∧ transitive R ∧ antisymmetric R\n\ndef total_order {A : Type} (R : BinRel A) : Prop :=\n  partial_order R ∧ ∀ (x y : A), R x y ∨ R y x\nThese definitions say that if R is a binary relation on A, then R is antisymmetric if R x y and R y x cannot both be true unless x = y. R is a partial order on A—or just a partial order, if A is clear from context—if it is reflexive, transitive, and antisymmetric. And R is a total order on A if it is a partial order and also, for any x and y of type A, either R x y or R y x. Note that, since Lean groups the connective ∧ to the right, partial_order R means reflexive R ∧ (transitive R ∧ antisymmetric R), and therefore if h is a proof of partial_order R, then h.left is a proof of reflexive R, h.right.left is a proof of transitive R, and h.right.right is a proof of antisymmetric R.\nExample 4.4.3 in HTPI gives several examples of partial orders and total orders. We’ll give one of those examples here. For any type A, we define sub A to be the subset relation on sets of objects of type A:\ndef sub (A : Type) (X Y : Set A) : Prop := X ⊆ Y\nAccording to this definition, sub A is a binary relation on Set A, and for any two sets X and Y of type Set A, sub A X Y is the proposition X ⊆ Y. We will leave it as an exercise for you to prove that sub A is a partial order on the type Set A.\nNotice that X ⊆ Y could be thought of as expressing a sense in which Y is “at least as large as” X. Often, if R is a partial order on A and a and b have type A, then R a b can be thought of as meaning that b is in some sense “at least as large as” a. Many of the concepts we study for partial and total orders are motivated by this interpretation of R.\nFor example, if R is a partial order on A, B has type Set A, and b has type A, then we say that b is an R-smallest element of B if it is an element of B, and every element of B is at least as large as b, according to this interpretation of the ordering R. We say that b is an R-minimal element of B if it is an element of B, and there is no other element of B that is smaller than b, according to the ordering R. We can state these precisely as definitions in Lean:\ndef smallestElt {A : Type} (R : BinRel A) (b : A) (B : Set A) : Prop :=\n  b ∈ B ∧ ∀ x ∈ B, R b x\n\ndef minimalElt {A : Type} (R : BinRel A) (b : A) (B : Set A) : Prop :=\n  b ∈ B ∧ ¬∃ x ∈ B, R x b ∧ x ≠ b\nNotice that, as in HTPI, in Lean we can write ∀ x ∈ B, P x as an abbreviation for ∀ (x : A), x ∈ B → P x, and ∃ x ∈ B, P x as an abbreviation for ∃ (x : A), x ∈ B ∧ P x. According to these definitions, if R is a partial order then smallestElt R b B is the proposition that b is an R-smallest element of B, and minimalElt R b B means that b is an R-minimal element of B. Although the definitions do not explicitly say that R must be a partial order, we will only use them in situations in which that is the case.\nTheorem 4.4.6 in HTPI asserts three statements about these concepts. We’ll prove the second and third, and leave the first as an exercise for you. The first statement in Theorem 4.4.6 says that if B has an R-smallest element, then that R-smallest element is unique. Thus, we can talk about the R-smallest element of B rather than an R-smallest element. The second says that if b is the R-smallest element of B, then it is also an R-minimal element, and it is the only R-minimal element. Here is how you might start the proof. (Although Lean sometimes uses bounded quantifiers as abbreviations in the Infoview, we have written out the unabbreviated statements in the comments, to make the logic of some steps easier to follow.)\ntheorem Theorem_4_4_6_2 {A : Type} (R : BinRel A) (B : Set A) (b : A)\n    (h1 : partial_order R) (h2 : smallestElt R b B) :\n    minimalElt R b B ∧ ∀ (c : A), minimalElt R c B → b = c := by\n  define at h1     --h1 : reflexive R ∧ transitive R ∧ antisymmetric R\n  define at h2     --h2 : b ∈ B ∧ ∀ (x : A), x ∈ B → R b x\n  apply And.intro\n  · -- Proof that b is minimal\n    define           --Goal : b ∈ B ∧ ¬∃ (x : A), x ∈ B ∧ R x b ∧ x ≠ b\n    apply And.intro h2.left\n    quant_neg        --Goal : ∀ (x : A), ¬(x ∈ B ∧ R x b ∧ x ≠ b)\n    **demorgan : ¬(x ∈ B ∧ R x b ∧ x ≠ b)::\n    done\n  · -- Proof that b is only minimal element\n\n    **done::\n  done\nWhen the goal is ∀ (x : A), ¬(x ∈ B ∧ R x b ∧ x ≠ b), it is tempting to apply the demorgan tactic to ¬(x ∈ B ∧ R x b ∧ x ≠ b), but unfortunately this generates an error in Lean: unknown identifier 'x'. The problem is that x is not defined in the tactic state, so without the quantifier ∀ (x : A) in front of it, ¬(x ∈ B ∧ R x b ∧ x ≠ b) doesn’t mean anything to Lean. The solution to the problem is to deal with the universal quantifier first by introducing an arbitrary x of type A. Once x has been introduced, we can apply the demorgan tactic.\ntheorem Theorem_4_4_6_2 {A : Type} (R : BinRel A) (B : Set A) (b : A)\n    (h1 : partial_order R) (h2 : smallestElt R b B) :\n    minimalElt R b B ∧ ∀ (c : A), minimalElt R c B → b = c := by\n  define at h1     --h1 : reflexive R ∧ transitive R ∧ antisymmetric R\n  define at h2     --h2 : b ∈ B ∧ ∀ (x : A), x ∈ B → R b x\n  apply And.intro\n  · -- Proof that b is minimal\n    define           --Goal : b ∈ B ∧ ¬∃ (x : A), x ∈ B ∧ R x b ∧ x ≠ b\n    apply And.intro h2.left\n    quant_neg        --Goal : ∀ (x : A), ¬(x ∈ B ∧ R x b ∧ x ≠ b)\n    fix x : A\n    demorgan         --Goal : ¬x ∈ B ∨ ¬(R x b ∧ x ≠ b)\n    or_right with h3 --h3 : x ∈ B; Goal : ¬(R x b ∧ x ≠ b)\n    demorgan         --Goal : ¬R x b ∨ x = b\n    or_right with h4 --h4 : R x b; Goal : x = b\n    have h5 : R b x := h2.right x h3\n    have h6 : antisymmetric R := h1.right.right\n    define at h6     --h6 : ∀ (x y : A), R x y → R y x → x = y\n    show x = b from h6 x b h4 h5\n    done\n  · -- Proof that b is only minimal element\n    fix c : A\n    assume h3 : minimalElt R c B\n    define at h3    --h3 : c ∈ B ∧ ¬∃ (x : A), x ∈ B ∧ R x c ∧ x ≠ c\n    contradict h3.right with h4\n                  --h4 : ¬b = c; Goal : ∃ (x : A), x ∈ B ∧ R x c ∧ x ≠ c\n    have h5 : R b c := h2.right c h3.left\n    show ∃ (x : A), x ∈ B ∧ R x c ∧ x ≠ c from\n      Exists.intro b (And.intro h2.left (And.intro h5 h4))\n    done\n  done\nFinally, the third statement in Theorem 4.4.6 says that if R is a total order, then any R-minimal element of a set B must be the R-smallest element of B. The beginning of the proof is straightforward:\ntheorem Theorem_4_4_6_3 {A : Type} (R : BinRel A) (B : Set A) (b : A)\n    (h1 : total_order R) (h2 : minimalElt R b B) : smallestElt R b B := by\n  define at h1         --h1 : partial_order R ∧ ∀ (x y : A), R x y ∨ R y x\n  define at h2         --h2 : b ∈ B ∧ ¬∃ (x : A), x ∈ B ∧ R x b ∧ x ≠ b\n  define               --Goal : b ∈ B ∧ ∀ (x : A), x ∈ B → R b x\n  apply And.intro h2.left  --Goal : ∀ (x : A), x ∈ B → R b x\n  fix x : A\n  assume h3 : x ∈ B        --Goal : R b x\n  **done::\nSurprisingly, at this point it is difficult to find a way to reach the goal R b x. See HTPI for an explanation of why it turns out to be helpful to split the proof into two cases, depending on whether or not x = b. Of course, we use the by_cases tactic for this.\ntheorem Theorem_4_4_6_3 {A : Type} (R : BinRel A) (B : Set A) (b : A)\n    (h1 : total_order R) (h2 : minimalElt R b B) : smallestElt R b B := by\n  define at h1         --h1 : partial_order R ∧ ∀ (x y : A), R x y ∨ R y x\n  define at h2         --h2 : b ∈ B ∧ ¬∃ (x : A), x ∈ B ∧ R x b ∧ x ≠ b\n  define               --Goal : b ∈ B ∧ ∀ (x : A), x ∈ B → R b x\n  apply And.intro h2.left  --Goal : ∀ (x : A), x ∈ B → R b x\n  fix x : A\n  assume h3 : x ∈ B        --Goal : R b x\n  by_cases h4 : x = b\n  · -- Case 1. h4 : x = b\n    rewrite [h4]           --Goal : R b b\n    have h5 : partial_order R := h1.left\n    define at h5\n    have h6 : reflexive R := h5.left\n    define at h6\n    show R b b from h6 b\n    done\n  · -- Case 2. h4 : x ≠ b\n    have h5 : ∀ (x y : A), R x y ∨ R y x := h1.right\n    have h6 : R x b ∨ R b x := h5 x b\n    have h7 : ¬R x b := by\n      contradict h2.right with h8\n      show ∃ (x : A), x ∈ B ∧ R x b ∧ x ≠ b from\n        Exists.intro x (And.intro h3 (And.intro h8 h4))\n      done\n    disj_syll h6 h7\n    show R b x from h6\n    done\n  done\nImitating the definitions above, you should be able to formulate definitions of R-largest and R-maximal elements. Section 4.4 of HTPI defines four more terms: upper bound, lower bound, least upper bound, and greatest lower bound. We will discuss upper bounds and least upper bounds, and leave lower bounds and greatest lower bounds for you to figure out on your own.\nIf R is a partial order on A, B has type Set A, and a has type A, then a is called an upper bound for B if it is at least as large as every element of B. If it is the smallest element of the set of upper bounds, then it is called the least upper bound of B. The phrase “least upper bound” is often abbreviated “lub”. Here are these definitions, written in Lean:\ndef upperBd {A : Type} (R : BinRel A) (a : A) (B : Set A) : Prop :=\n  ∀ x ∈ B, R x a\n\ndef lub {A : Type} (R : BinRel A) (a : A) (B : Set A) : Prop :=\n  smallestElt R a {c : A | upperBd R c B}\nAs usual, we will let you consult HTPI for examples of these concepts. But we will mention one example: If A is a type and F has type Set (Set A)—that is, F is a set whose elements are sets of objects of type A—then the least upper bound of F, with respect to the partial order sub A, is ⋃₀ F. We leave the proof of this fact as an exercise.\n\nExercises\n\ntheorem Example_4_4_3_1 {A : Type} : partial_order (sub A) := sorry\n\n\ntheorem Theorem_4_4_6_1 {A : Type} (R : BinRel A) (B : Set A) (b : A)\n    (h1 : partial_order R) (h2 : smallestElt R b B) :\n    ∀ (c : A), smallestElt R c B → b = c := sorry\n\n\n--If F is a set of sets, then ⋃₀ F is the lub of F in the subset ordering\ntheorem Theorem_4_4_11 {A : Type} (F : Set (Set A)) :\n    lub (sub A) (⋃₀ F) F := sorry\n\n\ntheorem Exercise_4_4_8 {A B : Type} (R : BinRel A) (S : BinRel B)\n    (T : BinRel (A × B)) (h1 : partial_order R) (h2 : partial_order S)\n    (h3 : ∀ (a a' : A) (b b' : B),\n      T (a, b) (a', b') ↔ R a a' ∧ S b b') :\n    partial_order T := sorry\n\n\ntheorem Exercise_4_4_9_part {A B : Type} (R : BinRel A) (S : BinRel B)\n    (L : BinRel (A × B)) (h1 : total_order R) (h2 : total_order S)\n    (h3 : ∀ (a a' : A) (b b' : B),\n      L (a, b) (a', b') ↔ R a a' ∧ (a = a' → S b b')) :\n    ∀ (a a' : A) (b b' : B),\n      L (a, b) (a', b') ∨ L (a', b') (a, b) := sorry\n\n\ntheorem Exercise_4_4_15a {A : Type}\n    (R1 R2 : BinRel A) (B : Set A) (b : A)\n    (h1 : partial_order R1) (h2 : partial_order R2)\n    (h3 : extension R1 ⊆ extension R2) :\n    smallestElt R1 b B → smallestElt R2 b B := sorry\n\n\ntheorem Exercise_4_4_15b {A : Type}\n    (R1 R2 : BinRel A) (B : Set A) (b : A)\n    (h1 : partial_order R1) (h2 : partial_order R2)\n    (h3 : extension R1 ⊆ extension R2) :\n    minimalElt R2 b B → minimalElt R1 b B := sorry\n\n\ntheorem Exercise_4_4_18a {A : Type}\n    (R : BinRel A) (B1 B2 : Set A) (h1 : partial_order R)\n    (h2 : ∀ x ∈ B1, ∃ y ∈ B2, R x y) (h3 : ∀ x ∈ B2, ∃ y ∈ B1, R x y) :\n    ∀ (x : A), upperBd R x B1 ↔ upperBd R x B2 := sorry\n\n\ntheorem Exercise_4_4_22 {A : Type}\n    (R : BinRel A) (B1 B2 : Set A) (x1 x2 : A)\n    (h1 : partial_order R) (h2 : lub R x1 B1) (h3 : lub R x2 B2) :\n    B1 ⊆ B2 → R x1 x2 := sorry\n\n\ntheorem Exercise_4_4_24 {A : Type} (R : Set (A × A)) :\n    smallestElt (sub (A × A)) (R ∪ (inv R))\n    {T : Set (A × A) | R ⊆ T ∧ symmetric (RelFromExt T)} := sorry"
  },
  {
    "objectID": "Chap4.html#equivalence-relations",
    "href": "Chap4.html#equivalence-relations",
    "title": "4  Relations",
    "section": "4.5. Equivalence Relations",
    "text": "4.5. Equivalence Relations\nChapter 4 of HTPI concludes with the study of one more important combination of properties that a relation might have. A binary relation \\(R\\) on a set \\(A\\) is called an equivalence relation if it is reflexive, symmetric, and transitive. If \\(x \\in A\\), then the equivalence class of \\(x\\) with respect to \\(R\\) is the set of all \\(y \\in A\\) such that \\(yRx\\). In HTPI, this equivalence class is denoted \\([x]_R\\), so we have \\[\n[x]_R = \\{y \\in A \\mid yRx\\}.\n\\] The set whose elements are all of these equivalence classes is called \\(A\\) mod \\(R\\). It is written \\(A/R\\), so \\[\nA/R = \\{[x]_R \\mid x \\in A\\}.\n\\] Note that \\(A/R\\) is a set whose elements are sets: for each \\(x \\in A\\), \\([x]_R\\) is a subset of \\(A\\), and \\([x]_R \\in A/R\\).\nTo define these concepts in Lean, we write:\ndef equiv_rel {A : Type} (R : BinRel A) : Prop :=\n  reflexive R ∧ symmetric R ∧ transitive R\n\ndef equivClass {A : Type} (R : BinRel A) (x : A) : Set A :=\n  {y : A | R y x}\n\ndef mod (A : Type) (R : BinRel A) : Set (Set A) :=\n  {equivClass R x | x : A}\nThus, equiv_rel R is the proposition that R is an equivalence relation, equivClass R x is the equivalence class of x with respect to R, and mod A R is A mod R. Note that equivClass R x has type Set A, while mod A R has type Set (Set A). The definition of mod A R is shorthand for {X : Set A | ∃ (x : A), equivClass R x = X}.\nHTPI gives several examples of equivalence relations, and these examples illustrate that equivalence classes always have certain properties. The most important of these are that each equivalence class is a nonempty set, the equivalence classes do not overlap, and their union is all of A. We say that the equivalence classes form a partition of A. To state and prove these properties in Lean we will need some definitions. We start with these:\ndef empty {A : Type} (X : Set A) : Prop := ¬∃ (x : A), x ∈ X \n\ndef pairwise_disjoint {A : Type} (F : Set (Set A)) : Prop :=\n  ∀ X ∈ F, ∀ Y ∈ F, X ≠ Y → empty (X ∩ Y)\nTo say that a set X is empty, we could write X = ∅, but it is more convenient to have a statement that says more explicitly what it means for a set to be empty. Thus, we have defined empty X to be the proposition saying that X has no elements. If F has type Set (Set A), then pairwise_disjoint F is the proposition that no two distinct elements of F have any element in common—in other words, the elements of F do not overlap. We can now give the precise definition of a partition:\ndef partition {A : Type} (F : Set (Set A)) : Prop :=\n  (∀ (x : A), x ∈ ⋃₀ F) ∧ pairwise_disjoint F ∧ ∀ X ∈ F, ¬empty X\nThe main theorem about equivalence relations in HTPI is Theorem 4.5.4, which says that mod A R is a partition of A. The proof of this theorem is hard enough that HTPI proves two facts about equivalence classes first. A fact that is proven just for the purpose of using it to prove something else is often called a lemma. We can use this term in Lean as well. Here is the first part of Lemma 4.5.5 from HTPI\nlemma Lemma_4_5_5_1 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    ∀ (x : A), x ∈ equivClass R x := by\n  fix x : A\n  define           --Goal : R x x\n  define at h      --h : reflexive R ∧ symmetric R ∧ transitive R\n  have Rref : reflexive R := h.left\n  show R x x from Rref x\n  done\nThe command #check @Lemma_4_5_5_1 produces the result\n\n@Lemma_4_5_5_1 : ∀ {A : Type} (R : BinRel A),\n                  equiv_rel R → ∀ (x : A), x ∈ equivClass R x\n\nThus, if we have R : BinRel A, h : equiv_rel R, and x : A, then Lemma_4_5_5_1 R h x is a proof of x ∈ equivClass R x. We will use this at the end of the proof of our next lemma:\nlemma Lemma_4_5_5_2 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    ∀ (x y : A), y ∈ equivClass R x ↔\n      equivClass R y = equivClass R x := by\n  have Rsymm : symmetric R := h.right.left\n  have Rtrans : transitive R := h.right.right\n  fix x : A; fix y : A\n  apply Iff.intro\n  · -- (→)\n    assume h2 :\n      y ∈ equivClass R x    --Goal : equivClass R y = equivClass R x\n    define at h2                        --h2 : R y x\n    apply Set.ext\n    fix z : A\n    apply Iff.intro\n    · -- Proof that z ∈ equivClass R y → z ∈ equivClass R x\n      assume h3 : z ∈ equivClass R y\n      define                            --Goal : R z x\n      define at h3                      --h3 : R z y\n      show R z x from Rtrans z y x h3 h2\n      done\n    · -- Proof that z ∈ equivClass R x → z ∈ equivClass R y\n      assume h3 : z ∈ equivClass R x\n      define                            --Goal : R z y\n      define at h3                      --h3 : R z x\n      have h4 : R x y := Rsymm y x h2\n      show R z y from Rtrans z x y h3 h4\n      done\n    done\n  · -- (←)\n    assume h2 :\n      equivClass R y = equivClass R x   --Goal : y ∈ equivClass R x\n    rewrite [←h2]                       --Goal : y ∈ equivClass R y\n    show y ∈ equivClass R y from Lemma_4_5_5_1 R h y\n    done\n  done\nThe definition of “partition” has three parts, so to prove Theorem 4.5.4 we will have to prove three statements. It will make the proof easier to read if we prove the three statements separately.\nlemma Theorem_4_5_4_part_1 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    ∀ (x : A), x ∈ ⋃₀ (mod A R) := by\n  fix x : A\n  define        --Goal : ∃ (t : Set A), t ∈ mod A R ∧ x ∈ t\n  apply Exists.intro (equivClass R x)\n  apply And.intro _ (Lemma_4_5_5_1 R h x)\n                --Goal : equivClass R x ∈ mod A R\n  define        --Goal : ∃ (x_1 : A), equivClass R x_1 = equivClass R x\n  apply Exists.intro x\n  rfl\n  done\n\nlemma Theorem_4_5_4_part_2 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    pairwise_disjoint (mod A R) := by\n  define\n  fix X : Set A\n  assume h2 : X ∈ mod A R\n  fix Y : Set A\n  assume h3 : Y ∈ mod A R           --Goal : X ≠ Y → empty (X ∩ Y)\n  define at h2; define at h3\n  obtain (x : A) (h4 : equivClass R x = X) from h2\n  obtain (y : A) (h5 : equivClass R y = Y) from h3\n  contrapos\n  assume h6 : ∃ (x : A), x ∈ X ∩ Y  --Goal : X = Y\n  obtain (z : A) (h7 : z ∈ X ∩ Y) from h6\n  define at h7\n  rewrite [←h4, ←h5] at h7 --h7 : z ∈ equivClass R x ∧ z ∈ equivClass R y\n  have h8 : equivClass R z = equivClass R x :=\n    (Lemma_4_5_5_2 R h x z).ltr h7.left\n  have h9 : equivClass R z = equivClass R y :=\n    (Lemma_4_5_5_2 R h y z).ltr h7.right\n  show X = Y from\n    calc X\n      _ = equivClass R x := h4.symm\n      _ = equivClass R z := h8.symm\n      _ = equivClass R y := h9\n      _ = Y              := h5\n  done\n\nlemma Theorem_4_5_4_part_3 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    ∀ X ∈ mod A R, ¬empty X := by\n  fix X : Set A\n  assume h2 : X ∈ mod A R  --Goal : ¬empty X\n  define; double_neg       --Goal : ∃ (x : A), x ∈ X\n  define at h2             --h2 : ∃ (x : A), equivClass R x = X\n  obtain (x : A) (h3 : equivClass R x = X) from h2\n  rewrite [←h3]\n  show ∃ (x_1 : A), x_1 ∈ equivClass R x from\n    Exists.intro x (Lemma_4_5_5_1 R h x)\n  done\nIt’s easy now to put everything together to prove Theorem 4.5.4.\ntheorem Theorem_4_5_4 {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    partition (mod A R) := And.intro (Theorem_4_5_4_part_1 R h)\n      (And.intro (Theorem_4_5_4_part_2 R h) (Theorem_4_5_4_part_3 R h))\nTheorem 4.5.4 shows that an equivalence relation on A determines a partition of A, namely mod A R. Our next project will be to prove Theorem 4.5.6 in HTPI, which says that every partition of A arises in this way; that is, every partition is mod A R for some equivalence relation R. To prove this, we must show how to use a partition F to define an equivalence relation R for which mod A R = F. The proof in HTPI defines the required equivalence relation R as a set of ordered pairs, but in Lean we will need to define it instead as a binary relation on A. Translating HTPI’s set-theoretic definition into Lean’s notation for binary relations leads to the following definition:\ndef EqRelFromPart {A : Type} (F : Set (Set A)) (x y : A) : Prop :=\n  ∃ X ∈ F, x ∈ X ∧ y ∈ X\nIn other words, EqRelFromPart F is the binary relation on A that is true of any two objects x and y of type A if and only if x and y belong to the same set in F. Our plan now is to show that if F is a partition of A, then EqRelFromPart F is an equivalence relation on A, and mod A (EqRelFromPart F) = F.\nOnce again, HTPI breaks the proof up by proving some lemmas first, and we will find it convenient to break the proof into even smaller pieces. We will leave the proofs of most of these lemmas as exercises for you.\nlemma overlap_implies_equal {A : Type}\n    (F : Set (Set A)) (h : partition F) :\n    ∀ X ∈ F, ∀ Y ∈ F, ∀ (x : A), x ∈ X → x ∈ Y → X = Y := sorry\n\nlemma Lemma_4_5_7_ref {A : Type} (F : Set (Set A)) (h : partition F):\n    reflexive (EqRelFromPart F) := sorry\n  \nlemma Lemma_4_5_7_symm {A : Type} (F : Set (Set A)) (h : partition F):\n    symmetric (EqRelFromPart F) := sorry\n\nlemma Lemma_4_5_7_trans {A : Type} (F : Set (Set A)) (h : partition F):\n    transitive (EqRelFromPart F) := sorry\nWe can now put these pieces together to prove Lemma 4.5.7 in HTPI:\nlemma Lemma_4_5_7 {A : Type} (F : Set (Set A)) (h : partition F) :\n    equiv_rel (EqRelFromPart F) := And.intro (Lemma_4_5_7_ref F h)\n      (And.intro (Lemma_4_5_7_symm F h) (Lemma_4_5_7_trans F h))\nWe need one more lemma before we can prove Theorem 4.5.6:\nlemma Lemma_4_5_8 {A : Type} (F : Set (Set A)) (h : partition F) :\n    ∀ X ∈ F, ∀ x ∈ X, equivClass (EqRelFromPart F) x = X := sorry\nWe are finally now ready to address Theorem 4.5.6. Here is the statement of the theorem:\ntheorem Theorem_4_5_6 {A : Type} (F : Set (Set A)) (h: partition F) :\n    ∃ (R : BinRel A), equiv_rel R ∧ mod A R = F\nOf course, the relation R that we will use to prove the theorem is EqRelFromPart F, so we could start the proof with the tactic apply Exists.intro (EqRelFromPart F). But this means that the rest of the proof will involve many statements about the relation EqRelFromPart F. When a complicated object appears multiple times in a proof, it can make the proof a little easier to read if we give that object a name. We can do that by using a new tactic. The tactic set R : BinRel A := EqRelFromPart F introduces the new variable R into the tactic state. The variable R has type BinRel A, and it is definitionally equal to EqRelFromPart F. That means that, when necessary, Lean will fill in this definition of R. For example, one of our first steps will be to apply Lemma_4_5_7 to F and h. The conclusion of that lemma is equiv_rel (EqRelFromPart F), but Lean will recognize this as meaning the same thing as equiv_rel R. Here is the proof of the theorem:\ntheorem Theorem_4_5_6 {A : Type} (F : Set (Set A)) (h: partition F) :\n    ∃ (R : BinRel A), equiv_rel R ∧ mod A R = F := by\n  set R : BinRel A := EqRelFromPart F\n  apply Exists.intro R               --Goal : equiv_rel R ∧ mod A R = F\n  apply And.intro (Lemma_4_5_7 F h)  --Goal : mod A R = F\n  apply Set.ext\n  fix X : Set A                      --Goal :  X ∈ mod A R ↔ X ∈ F\n  apply Iff.intro\n  · -- (→)\n    assume h2 : X ∈ mod A R          --Goal : X ∈ F\n    define at h2                     --h2 : ∃ (x : A), equivClass R x = X\n    obtain (x : A) (h3 : equivClass R x = X) from h2\n    have h4 : x ∈ ⋃₀ F := h.left x\n    define at h4\n    obtain (Y : Set A) (h5 : Y ∈ F ∧ x ∈ Y) from h4\n    have h6 : equivClass R x = Y :=\n      Lemma_4_5_8 F h Y h5.left x h5.right\n    rewrite [←h3, h6]\n    show Y ∈ F from h5.left\n    done\n  · -- (←)\n    assume h2 : X ∈ F                --Goal : X ∈ mod A R\n    have h3 : ¬empty X := h.right.right X h2\n    define at h3; double_neg at h3   --h3 : ∃ (x : A), x ∈ X\n    obtain (x : A) (h4 : x ∈ X) from h3\n    define                       --Goal : ∃ (x : A), equivClass R x = X\n    show ∃ (x : A), equivClass R x = X from\n      Exists.intro x (Lemma_4_5_8 F h X h2 x h4)\n    done\n  done\n\nExercises\n\nlemma overlap_implies_equal {A : Type}\n    (F : Set (Set A)) (h : partition F) :\n    ∀ X ∈ F, ∀ Y ∈ F, ∀ (x : A), x ∈ X → x ∈ Y → X = Y := sorry\n\n\nlemma Lemma_4_5_7_ref {A : Type} (F : Set (Set A)) (h : partition F) :\n    reflexive (EqRelFromPart F) := sorry\n\n\nlemma Lemma_4_5_7_symm {A : Type} (F : Set (Set A)) (h : partition F) :\n    symmetric (EqRelFromPart F) := sorry\n\n\nlemma Lemma_4_5_7_trans {A : Type} (F : Set (Set A)) (h : partition F) :\n    transitive (EqRelFromPart F) := sorry\n\n\nlemma Lemma_4_5_8 {A : Type} (F : Set (Set A)) (h : partition F) :\n    ∀ X ∈ F, ∀ x ∈ X, equivClass (EqRelFromPart F) x = X := sorry\n\n\nlemma elt_mod_equiv_class_of_elt\n    {A : Type} (R : BinRel A) (h : equiv_rel R) :\n    ∀ X ∈ mod A R, ∀ x ∈ X, equivClass R x = X := sorry\n\n\n\n\nThe next three exercises use the following definitions:\ndef dot {A : Type} (F G : Set (Set A)) : Set (Set A) :=\n  {Z : Set A | ¬empty Z ∧ ∃ X ∈ F, ∃ Y ∈ G, Z = X ∩ Y}\n\ndef conj {A : Type} (R S : BinRel A) (x y : A) : Prop :=\n  R x y ∧ S x y\n\ntheorem Exercise_4_5_20a {A : Type} (R S : BinRel A)\n    (h1 : equiv_rel R) (h2 : equiv_rel S) :\n    equiv_rel (conj R S) := sorry\n\n\ntheorem Exercise_4_5_20b {A : Type} (R S : BinRel A)\n    (h1 : equiv_rel R) (h2 : equiv_rel S) :\n    ∀ (x : A), equivClass (conj R S) x =\n      equivClass R x ∩ equivClass S x := sorry\n\n\ntheorem Exercise_4_5_20c {A : Type} (R S : BinRel A)\n    (h1 : equiv_rel R) (h2 : equiv_rel S) :\n    mod A (conj R S) = dot (mod A R) (mod A S) := sorry\n\n\n\n\nThe next exercise uses the following definition:\ndef equiv_mod (m x y : Int) : Prop := m ∣ (x - y)\n\ntheorem Theorem_4_5_10 : ∀ (m : Int), equiv_rel (equiv_mod m) := sorry"
  },
  {
    "objectID": "Chap5.html",
    "href": "Chap5.html",
    "title": "5  Functions",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\mathbin{∆}}\n$$"
  },
  {
    "objectID": "Chap5.html#functions",
    "href": "Chap5.html#functions",
    "title": "5  Functions",
    "section": "5.1. Functions",
    "text": "5.1. Functions\nThe first definition in Chapter 5 of HTPI says that if \\(F \\subseteq A \\times B\\), then \\(F\\) is called a function from \\(A\\) to \\(B\\) if for every \\(a \\in A\\) there is exactly one \\(b \\in B\\) such that \\((a, b) \\in F\\). The notation \\(F : A \\to B\\) means that \\(F\\) is a function from \\(A\\) to \\(B\\). If \\(F\\) is a function from \\(A\\) to \\(B\\) and \\(a \\in A\\), then HTPI introduces the notation \\(F(a)\\) for the unique \\(b \\in B\\) such that \\((a, b) \\in F\\). Thus, if \\(F : A \\to B\\), \\(a \\in A\\), and \\(b \\in B\\), then \\(F(a) = b\\) means the same thing as \\((a, b) \\in F\\). We sometimes think of \\(F\\) as representing an operation that can be applied to an element \\(a\\) of \\(A\\) to produce a corresponding element \\(F(a)\\) of \\(B\\), and we call \\(F(a)\\) the value of \\(F\\) at \\(a\\), or the result of applying \\(F\\) to \\(a\\).\nThis might remind you of the situation we faced in Chapter 4. If \\(R \\subseteq A \\times B\\), \\(a \\in A\\), and \\(b \\in B\\), then Chapter 4 of HTPI uses the notation \\(aRb\\) to mean the same thing as \\((a, b) \\in R\\). But in Lean, we found it necessary to change this notation. Instead of using HTPI’s notation \\(aRb\\), we introduced the notation R a b, which we use when R has type Rel A B, a has type A, and b has type B. (The notation (a, b) ∈ R, in contrast, can be used only when R has type Set (A × B).) If R has type Rel A B, then we think of R as representing some relationship that might hold between a and b, and R a b as the proposition saying that this relationship holds. And although R is not a set of ordered pairs, there is a corresponding set, extension R, of type Set (A × B), with the property that (a, b) ∈ extension R if and only if R a b.\nWe will take a similar approach to functions in this chapter. For any types A and B, we introduce a new type A → B. If f has type A → B, then we think of f as representing some operation that can be applied to an object of type A to produce a corresponding object of type B. We will say that f is a function from A to B, and A is the domain of f. If a has type A, then we write f a (with a space but no parentheses) for the result of applying the operation represented by f to the object a. Thus, if we have f : A → B and a : A, then f a has type B. As with relations, if f has type A → B, then f is not a set of ordered pairs. But there is a corresponding set of ordered pairs, which we will call the graph of f, whose elements are the ordered pairs (a, b) for which f a = b:\ndef graph {A B : Type} (f : A → B) : Set (A × B) :=\n  {(a, b) : A × B | f a = b}\n\ntheorem graph_def {A B : Type} (f : A → B) (a : A) (b : B) :\n    (a, b) ∈ graph f ↔ f a = b := by rfl\nEvery set of type Set (A × B) is the extension of some relation from A to B, but not every such set is the graph of a function from A to B. To be the graph of a function, it must have the property that was used to define functions in HTPI: each object of type A must be paired in the set with exactly one object of type B. Let’s give this property a name:\ndef is_func_graph {A B : Type} (F : Set (A × B)) : Prop :=\n  ∀ (x : A), ∃! (y : B), (x, y) ∈ F\nAnd now we can say that the sets of type Set (A × B) that are graphs of functions from A to B are precisely the ones that have the property is_func_graph:\ntheorem func_from_graph {A B : Type} (F : Set (A × B)) :\n    (∃ (f : A → B), graph f = F) ↔ is_func_graph F\nWe will ask you to prove the left-to-right direction of this theorem in the exercises. The proof of the right-to-left direction in Lean is tricky; it requires an idea that we won’t introduce until Section 8.2. We’ll give the proof then, but we’ll go ahead and use the theorem in this chapter when we find it useful.\nSection 5.1 of HTPI proves two theorems about functions. The first gives a convenient way of proving that two functions are equal (HTPI p. 232):\n\nSuppose \\(f\\) and \\(g\\) are functions from \\(A\\) to \\(B\\). If \\(\\forall a \\in A(f(a) = g(a))\\), then \\(f = g\\).\n\nThe proof of this theorem in HTPI is based on the axiom of extensionality for sets. But in Lean, functions aren’t sets of ordered pairs, so this method of proof won’t work. Fortunately, Lean has a similar axiom of extensionality for functions. The axiom is called funext, and it proves Theorem 5.1.4.\ntheorem Theorem_5_1_4 {A B : Type} (f g : A → B) :\n    (∀ (a : A), f a = g a) → f = g := funext\nWe saw previously that if we are trying to prove X = Y, where X and Y both have type Set U, then often the best first step is the tactic apply Set.ext, which converts the goal to ∀ (x : U), x ∈ X ↔︎ x ∈ Y. Similarly, if we are trying to prove f = g, where f and g both have type A → B, then we will usually start with the tactic apply funext, which will convert the goal to ∀ (x : A), f x = g x. By Theorem_5_1_4, this implies the original goal f = g. For example, here is a proof that if two functions have the same graph, then they are equal:\nexample {A B : Type} (f g : A → B) :\n    graph f = graph g → f = g := by\n  assume h1 : graph f = graph g  --Goal : f = g\n  apply funext                   --Goal : ∀ (x : A), f x = g x\n  fix x : A\n  have h2 : (x, f x) ∈ graph f := by\n    define                       --Goal : f x = f x\n    rfl\n    done\n  rewrite [h1] at h2             --h2 : (x, f x) ∈ graph g\n  define at h2                   --h2 : g x = f x\n  show f x = g x from h2.symm\n  done\nThe axiom of extensionality for sets says that a set is completely determined by its elements. This is what justifies our usual method of defining a set: we specify what its elements are, using notation like {0, 1, 2} or {x : Nat | x < 3}. Similarly, the axiom of extensionality for functions says that a function is completely determined by its values, and therefore we can define a function by specifying its values. For instance, we can define a function from Nat to Nat by specifying, for any n : Nat, the result of applying the function to n. As an example of this, we could define the “squaring function” from Nat to Nat to be the function that, when applied to any n : Nat, produces the result n ^ 2. Here are two ways to define this function in Lean:\ndef square1 (n : Nat) : Nat := n ^ 2\n\ndef square2 : Nat → Nat := fun (n : Nat) => n ^ 2\nThe first of these definitions uses notation we have used before; it says that if n has type Nat, then the expression square1 n also has type Nat, and it is definitionally equal to n ^ 2. The second definition introduces new Lean notation. It says that square2 has type Nat → Nat, and it defines it to be the function that, when applied to any n of type Nat, yields the result n ^ 2. Of course, this also means that square2 n is definitionally equal to n ^ 2. In general, the notation fun (x : A) => ... means “the function which, when applied to any x of type A, yields the result …” The two definitions above are equivalent. You can ask Lean to confirm this, and try out the squaring function, as follows (the #eval command asks Lean to evaluate an expression):\nexample : square1 = square2 := by rfl\n\n++#eval:: square1 7     --Answer: 49\nThere is one more theorem in Section 5.1 of HTPI. Theorem 5.1.5 says that if \\(f\\) is a function from \\(A\\) to \\(B\\) and \\(g\\) is a function from \\(B\\) to \\(C\\), then the composition of \\(g\\) and \\(f\\) is a function from \\(A\\) to \\(C\\). To state this theorem in Lean, we will have to make adjustments for the differences between the treatment of functions in HTPI and Lean. In Chapter 4, we defined comp S R to be the composition of S and R, where R has type Set (A × B) and S had type Set (B × C). But functions in Lean are not sets of ordered pairs, so we cannot apply the operation comp to them. We can, however, apply it to their graphs. So the theorem corresponding to Theorem 5.1.5 in HTPI is this:\ntheorem Theorem_5_1_5 {A B C : Type} (f : A → B) (g : B → C) :\n    ∃ (h : A → C), graph h = comp (graph g) (graph f) := by\n  set h : A → C := fun (x : A) => g (f x)\n  apply Exists.intro h\n  apply Set.ext\n  fix (a, c) : A × C\n  apply Iff.intro\n  · -- Proof that (a, c) ∈ graph h → (a, c) ∈ comp (graph g) (graph f)\n    assume h1 : (a, c) ∈ graph h\n    define at h1  --h1 : h a = c\n    define        --Goal : ∃ (x : B), (a, x) ∈ graph f ∧ (x, c) ∈ graph g\n    apply Exists.intro (f a)\n    apply And.intro\n    · -- Proof that (a, f a) ∈ graph f\n      define\n      rfl\n      done\n    · -- Proof that (f a, c) ∈ graph g\n      define\n      show g (f a) = c from h1\n      done\n    done\n  · -- Proof that (a, c) ∈ comp (graph g) (graph f) → (a, c) ∈ graph h\n    assume h1 : (a, c) ∈ comp (graph g) (graph f)\n    define        --Goal : h a = c\n    define at h1  --h1 : ∃ (x : B), (a, x) ∈ graph f ∧ (x, c) ∈ graph g\n    obtain (b : B) (h2 : (a, b) ∈ graph f ∧ (b, c) ∈ graph g) from h1\n    have h3 : (a, b) ∈ graph f := h2.left\n    have h4 : (b, c) ∈ graph g := h2.right\n    define at h3          --h3 : f a = b\n    define at h4          --h4 : g b = c\n    rewrite [←h3] at h4   --h4 : g (f a) = c\n    show h a = c from h4\n    done\n  done\nNotice that the proof of Theorem_5_1_5 begins by defining the function h for which graph h = comp (graph g) (graph f). The definition says that for all x of type A, h x = g (f x). This function h is called the composition of g and f, and it is denoted g ∘ f. (To type ∘ in VS Code, type \\comp or \\circ.) In other words, g ∘ f has type A → C, and for all x of type A, (g ∘ f) x is definitionally equal to g (f x). In HTPI, functions are sets of ordered pairs, and the operation of composition of functions is literally the same as the operation comp that we used in Chapter 4. But in Lean, we distinguish among functions, relations, and sets of ordered pairs, so all we can say is that the operation of composition of functions corresponds to the operation comp from Chapter 4. The correspondence is that, as shown in the proof of Theorem_5_1_5, if h = g ∘ f, then graph h = comp (graph g) (graph f).\nWe saw in part 4 of Theorem 4.2.5 that composition of relations is associative. Composition of functions is also associative. In fact, if f : A → B, g : B → C, and h : C → D, then h ∘ (g ∘ f) and (h ∘ g) ∘ f are definitionally equal, since they both mean the same thing as fun (x : A) => h (g (f a)). As a result, the tactic rfl proves the associativity of composition of functions:\nexample {A B C D : Type} (f : A → B) (g : B → C) (h : C → D) :\n    h ∘ (g ∘ f) = (h ∘ g) ∘ f := by rfl\nHTPI defines the identity function on a set \\(A\\) to be the function \\(i_A\\) from \\(A\\) to \\(A\\) such that \\(\\forall x \\in A(i_A(x) = x)\\), and Exercise 9 from Section 4.3 of HTPI implies that if \\(f : A \\to B\\), then \\(f \\circ i_A = f\\) and \\(i_B \\circ f = f\\). We say, therefore, that the identity functions are identity elements for composition of functions. Similarly, in Lean, for each type A there is an identity function from A to A. This identity function is denoted id; there is no need to specify A in the notation, because A is an implicit argument to id. Thus, when you use id to denote an identity function, Lean will figure out what type A to use as the domain of the function. (If, for some reason, you want to specify that the domain is some type A, you can write @id A instead of id.) For any x, of any type, id x is definitionally equal to x, and as a result the proof that id is an identity element for composition of functions can also be done with the rfl tactic:\nexample {A B : Type} (f : A → B) : f ∘ id = f := by rfl\n\nexample {A B : Type} (f : A → B) : id ∘ f = f := by rfl\n\nExercises\n\ntheorem func_from_graph_ltr {A B : Type} (F : Set (A × B)) :\n    (∃ (f : A → B), graph f = F) → is_func_graph F := sorry\n\n\ntheorem Exercise_5_1_13a\n    {A B C : Type} (R : Set (A × B)) (S : Set (B × C)) (f : A → C)\n    (h1 : ∀ (b : B), b ∈ Ran R ∧ b ∈ Dom S) (h2 : graph f = comp S R) :\n    is_func_graph S := sorry\n\n\ntheorem Exercise_5_1_14a\n    {A B : Type} (f : A → B) (R : BinRel A) (S : BinRel B)\n    (h : ∀ (x y : A), R x y ↔ S (f x) (f y)) :\n    reflexive S → reflexive R := sorry\n\n4. Here is a putative theorem:\n\nSuppose \\(f : A \\to B\\), \\(R\\) is a binary relation on \\(A\\), and \\(S\\) is the binary relation on \\(B\\) defined as follows: \\[\n\\forall x \\in B \\forall y \\in B(xSy \\leftrightarrow \\exists u \\in A\\exists v \\in A(f(u) = x \\wedge f(v) = y \\wedge uRv)).\n\\] If \\(R\\) is reflexive then \\(S\\) is reflexive.\n\nIs the theorem correct? Try to prove it in Lean. If you can’t prove it, see if you can find a counterexample.\n--You might not be able to complete this proof\ntheorem Exercise_5_1_15a\n    {A B : Type} (f : A → B) (R : BinRel A) (S : BinRel B)\n    (h : ∀ (x y : B), S x y ↔ ∃ (u v : A), f u = x ∧ f v = y ∧ R u v) :\n    reflexive R → reflexive S := sorry\n5. Here is a putative theorem with an incorrect proof:\n\nSuppose \\(f : A \\to B\\), \\(R\\) is a binary relation on \\(A\\), and \\(S\\) is the binary relation on \\(B\\) defined as follows: \\[\n\\forall x \\in B \\forall y \\in B(xSy \\leftrightarrow \\exists u \\in A\\exists v \\in A(f(u) = x \\wedge f(v) = y \\wedge uRv)).\n\\] If \\(R\\) is transitive then \\(S\\) is transitive.\n\n\nSuppose \\(R\\) is transitive. Let \\(x\\), \\(y\\), and \\(z\\) be arbitrary elements of \\(B\\). Assume that \\(xSy\\) and \\(ySz\\). By the definition of \\(S\\), this means that there are \\(u\\), \\(v\\), and \\(w\\) in \\(A\\) such that \\(f(u) = x\\), \\(f(v) = y\\), \\(f(w) = z\\), \\(uRv\\), and \\(vRw\\). Since \\(R\\) is transitive, it follows that \\(uRw\\). Since \\(f(u) = x\\), \\(f(w) = z\\), and \\(uRw\\), \\(xSz\\). Therefore \\(S\\) is transitive.  □\n\nFind the mistake in the proof by attempting to write the proof in Lean. Is the theorem correct?\n--You might not be able to complete this proof\ntheorem Exercise_5_1_15c\n    {A B : Type} (f : A → B) (R : BinRel A) (S : BinRel B)\n    (h : ∀ (x y : B), S x y ↔ ∃ (u v : A), f u = x ∧ f v = y ∧ R u v) :\n    transitive R → transitive S := sorry\n\ntheorem Exercise_5_1_16b\n    {A B : Type} (R : BinRel B) (S : BinRel (A → B))\n    (h : ∀ (f g : A → B), S f g ↔ ∀ (x : A), R (f x) (g x)) :\n    symmetric R → symmetric S := sorry\n\n\ntheorem Exercise_5_1_17a {A : Type} (f : A → A) (a : A)\n    (h : ∀ (x : A), f x = a) : ∀ (g : A → A), f ∘ g = f := sorry\n\n\ntheorem Exercise_5_1_17b {A : Type} (f : A → A) (a : A)\n    (h : ∀ (g : A → A), f ∘ g = f) :\n    ∃ (y : A), ∀ (x : A), f x = y := sorry"
  },
  {
    "objectID": "Chap5.html#one-to-one-and-onto",
    "href": "Chap5.html#one-to-one-and-onto",
    "title": "5  Functions",
    "section": "5.2. One-to-One and Onto",
    "text": "5.2. One-to-One and Onto\nSection 5.2 of HTPI introduces two important properties that a function might have. A function f : A → B is called onto if for every b of type B there is at least one a of type A such that f a = b:\ndef onto {A B : Type} (f : A → B) : Prop :=\n  ∀ (y : B), ∃ (x : A), f x = y\nIt is called one-to-one if there do not exist distinct a1 and a2 of type A such that f a1 = f a2. This phrasing of the definition makes it clear what is at issue: Are there distinct objects in the domain to which the function assigns the same value? But it is a negative statement, and that would make it difficult to work with it in proofs. Fortunately, it is not hard to rephrase the definition as an equivalent positive statement, using quantifier negation, De Morgan, and conditional laws. The resulting equivalent positive statement is given in Theorem 5.2.3 of HTPI, and we take it as our official definition of one_to_one in Lean:\ndef one_to_one {A B : Type} (f : A → B) : Prop :=\n  ∀ (x1 x2 : A), f x1 = f x2 → x1 = x2\nThere is only one more theorem about these properties in Section 5.2 of HTPI. It says that a composition of one-to-one functions is one-to-one, and a composition of onto functions is onto. It is straightforward to carry out these proofs in Lean by simply applying the definitions of the relevant concepts.\ntheorem Theorem_5_2_5_1 {A B C : Type} (f : A → B) (g : B → C) :\n    one_to_one f → one_to_one g → one_to_one (g ∘ f) := by\n  assume h1 : one_to_one f\n  assume h2 : one_to_one g\n  define at h1  --h1 : ∀ (x1 x2 : A), f x1 = f x2 → x1 = x2\n  define at h2  --h2 : ∀ (x1 x2 : B), g x1 = g x2 → x1 = x2\n  define        --Goal : ∀ (x1 x2 : A), (g ∘ f) x1 = (g ∘ f) x2 → x1 = x2\n  fix a1 : A\n  fix a2 : A    --Goal : (g ∘ f) a1 = (g ∘ f) a2 → a1 = a2\n  define : (g ∘ f) a1; define : (g ∘ f) a2\n                --Goal : g (f a1) = g (f a2) → a1 = a2\n  assume h3 : g (f a1) = g (f a2)\n  have h4 : f a1 = f a2 := h2 (f a1) (f a2) h3\n  show a1 = a2 from h1 a1 a2 h4\n  done\nNotice that the tactic define : (g ∘ f) a1 replaces (g ∘ f) a1 with its definition, g (f a1). As usual, this step isn’t really needed—Lean will apply the definition on its own when necessary, without being told. But using this tactic makes the proof easier to read. Also, notice that define : g ∘ f produces a result that is much less useful. As we have observed before, the define tactic works best when applied to a complete expression, rather than just a part of an expression.\nAn alternative way to apply the definition of composition of functions is to prove a lemma that can be used in the rewrite tactic. We try out this approach for the second part of the theorem.\nlemma comp_def {A B C : Type} (f : A → B) (g : B → C) (x : A) :\n    (g ∘ f) x = g (f x) := by rfl\n\ntheorem Theorem_5_2_5_2 {A B C : Type} (f : A → B) (g : B → C) :\n    onto f → onto g → onto (g ∘ f) := by\n  assume h1 : onto f\n  assume h2 : onto g\n  define at h1           --h1 : ∀ (y : B), ∃ (x : A), f x = y\n  define at h2           --h2 : ∀ (y : C), ∃ (x : B), g x = y\n  define                 --Goal : ∀ (y : C), ∃ (x : A), (g ∘ f) x = y\n  fix c : C\n  obtain (b : B) (h3 : g b = c) from h2 c\n  obtain (a : A) (h4 : f a = b) from h1 b\n  apply Exists.intro a   --Goal : (g ∘ f) a = c\n  rewrite [comp_def]     --Goal : g (f a) = c\n  rewrite [←h4] at h3\n  show g (f a) = c from h3\n  done\n\nExercises\n\ntheorem Exercise_5_2_10a {A B C : Type} (f: A → B) (g : B → C) :\n    onto (g ∘ f) → onto g := sorry\n\n\ntheorem Exercise_5_2_10b {A B C : Type} (f: A → B) (g : B → C) :\n    one_to_one (g ∘ f) → one_to_one f := sorry\n\n\ntheorem Exercise_5_2_11a {A B C : Type} (f: A → B) (g : B → C) :\n    onto f → ¬(one_to_one g) → ¬(one_to_one (g ∘ f)) := sorry\n\n\ntheorem Exercise_5_2_11b {A B C : Type} (f: A → B) (g : B → C) :\n    ¬(onto f) → one_to_one g → ¬(onto (g ∘ f)) := sorry\n\n\ntheorem Exercise_5_2_12 {A B : Type} (f : A → B) (g : B → Set A)\n    (h : ∀ (b : B), g b = {a : A | f a = b}) :\n    onto f → one_to_one g := sorry\n\n\ntheorem Exercise_5_2_16 {A B C : Type}\n    (R : Set (A × B)) (S : Set (B × C)) (f : A → C) (g : B → C)\n    (h1 : graph f = comp S R) (h2 : graph g = S) (h3 : one_to_one g) :\n    is_func_graph R := sorry\n\n\ntheorem Exercise_5_2_17a\n    {A B : Type} (f : A → B) (R : BinRel A) (S : BinRel B)\n    (h1 : ∀ (x y : B), S x y ↔ ∃ (u v : A), f u = x ∧ f v = y ∧ R u v)\n    (h2 : onto f) : reflexive R → reflexive S := sorry\n\n\ntheorem Exercise_5_2_17b\n    {A B : Type} (f : A → B) (R : BinRel A) (S : BinRel B)\n    (h1 : ∀ (x y : B), S x y ↔ ∃ (u v : A), f u = x ∧ f v = y ∧ R u v)\n    (h2 : one_to_one f) : transitive R → transitive S := sorry\n\n\ntheorem Exercise_5_2_21a {A B C : Type} (f : B → C) (g h : A → B)\n    (h1 : one_to_one f) (h2 : f ∘ g = f ∘ h) : g = h := sorry\n\n\ntheorem Exercise_5_2_21b {A B C : Type} (f : B → C) (a : A)\n    (h1 : ∀ (g h : A → B), f ∘ g = f ∘ h → g = h) :\n    one_to_one f := sorry"
  },
  {
    "objectID": "Chap5.html#inverses-of-functions",
    "href": "Chap5.html#inverses-of-functions",
    "title": "5  Functions",
    "section": "5.3. Inverses of Functions",
    "text": "5.3. Inverses of Functions\nSection 5.3 of HTPI is motivated by the following question: If \\(f\\) is a function from \\(A\\) to \\(B\\), is \\(f^{-1}\\) a function from \\(B\\) to \\(A\\)? Here is the first theorem in that section (HTPI p. 250):\n\nSuppose \\(f : A \\to B\\). If \\(f\\) is one-to-one and onto, then \\(f^{-1} : B \\to A\\).\n\nOf course, we will have to rephrase this theorem slightly to prove it in Lean. If f has type A → B, then the inverse operation inv cannot be applied to f, but it can be applied to graph f. So we must rephrase the theorem like this:\ntheorem Theorem_5_3_1 {A B : Type}\n    (f : A → B) (h1 : one_to_one f) (h2 : onto f) :\n    ∃ (g : B → A), graph g = inv (graph f)\nTo prove this theorem, we will use the theorem func_from_graph that was stated in Section 5.1. We can remind ourselves of what that theorem says by using the command #check @func_from_graph, which gives the result:\n\n@func_from_graph : ∀ {A B : Type} (F : Set (A × B)),\n                    (∃ (f : A → B), graph f = F) ↔ is_func_graph F\n\nThis means that, in the context of the proof of Theorem_5_3_1, func_from_graph (inv (graph f)) is a proof of the statement\n\n∃ (g : B → A), graph g = inv (graph f) ↔ is_func_graph (inv (graph f)).\n\nTherefore the tactic rewrite [func_from_graph (inv (graph f))] will change the goal to is_func_graph (inv (graph f)). In fact, we can just use rewrite [func_from_graph], and Lean will figure out how to apply the theorem to rewrite the goal. The rest of the proof is straightforward.\ntheorem Theorem_5_3_1 {A B : Type}\n    (f : A → B) (h1 : one_to_one f) (h2 : onto f) :\n    ∃ (g : B → A), graph g = inv (graph f) := by\n  rewrite [func_from_graph]   --Goal : is_func_graph (inv (graph f))\n  define        --Goal : ∀ (x : B), ∃! (y : A), (x, y) ∈ inv (graph f)\n  fix b : B\n  exists_unique\n  · -- Existence\n    define at h2          --h2 : ∀ (y : B), ∃ (x : A), f x = y\n    obtain (a : A) (h4 : f a = b) from h2 b\n    apply Exists.intro a  --Goal : (b, a) ∈ inv (graph f)\n    define                --Goal : f a = b\n    show f a = b from h4\n    done\n  · -- Uniqueness\n    fix a1 : A; fix a2 : A\n    assume h3 : (b, a1) ∈ inv (graph f)\n    assume h4 : (b, a2) ∈ inv (graph f) --Goal : a1 = a2\n    define at h3          --h3 : f a1 = b\n    define at h4          --h4 : f a2 = b\n    rewrite [←h4] at h3   --h3 : f a1 = f a2\n    define at h1          --h1 : ∀ (x1 x2 : A), f x1 = f x2 → x1 = x2\n    show a1 = a2 from h1 a1 a2 h3\n    done\n  done\nSuppose, now, that we have f : A → B, g : B → A, and graph g = inv (graph f), as in Theorem_5_3_1. What can we say about the relationship between f and g? One answer is that g ∘ f = id and f ∘ g = id, as shown in Theorem 5.3.2 of HTPI. We’ll prove one of these facts, and leave the other as an exercise for you.\ntheorem Theorem_5_3_2_1 {A B : Type} (f : A → B) (g : B → A)\n    (h1 : graph g = inv (graph f)) : g ∘ f = id := by\n  apply funext           --Goal : ∀ (x : A), (g ∘ f) x = id x\n  fix a : A              --Goal : (g ∘ f) a = id a\n  have h2 : (f a, a) ∈ graph g := by\n    rewrite [h1]         --Goal : (f a, a) ∈ inv (graph f)\n    define               --Goal : f a = f a\n    rfl\n    done\n  define at h2           --h2 : g (f a) = a\n  show (g ∘ f) a = id a from h2\n  done\n\ntheorem Theorem_5_3_2_2 {A B : Type} (f : A → B) (g : B → A)\n    (h1 : graph g = inv (graph f)) : f ∘ g = id := sorry\nCombining the theorems above, we have shown that if f is one-to-one and onto, then there is a function g such that g ∘ f = id and f ∘ g = id. In fact, the converse is true as well: if such a function g exists, then f must be one-to-one and onto. Again, we’ll prove one statement and leave the second as an exercise.\ntheorem Theorem_5_3_3_1 {A B : Type} (f : A → B) (g : B → A)\n    (h1 : g ∘ f = id) : one_to_one f := by\n  define              --Goal : ∀ (x1 x2 : A), f x1 = f x2 → x1 = x2\n  fix a1 : A; fix a2 : A\n  assume h2 : f a1 = f a2\n  show a1 = a2 from\n    calc a1\n      _ = id a1 := by rfl\n      _ = (g ∘ f) a1 := by rw [h1]\n      _ = g (f a1) := by rfl\n      _ = g (f a2) := by rw [h2]\n      _ = (g ∘ f) a2 := by rfl\n      _ = id a2 := by rw [h1]\n      _ = a2 := by rfl\n  done\n\ntheorem Theorem_5_3_3_2 {A B : Type} (f : A → B) (g : B → A)\n    (h1 : f ∘ g = id) : onto f := sorry\nWe can combine the theorems above to show that if we have f : A → B, g : B → A, g ∘ f = id, and f ∘ g = id, then graph g must be the inverse of graph f. Compare the proof below to the proof of Theorem 5.3.5 in HTPI.\ntheorem Theorem_5_3_5 {A B : Type} (f : A → B) (g : B → A)\n    (h1 : g ∘ f = id) (h2 : f ∘ g = id) : graph g = inv (graph f) := by\n  have h3 : one_to_one f := Theorem_5_3_3_1 f g h1\n  have h4 : onto f := Theorem_5_3_3_2 f g h2\n  obtain (g' : B → A) (h5 : graph g' = inv (graph f))\n    from Theorem_5_3_1 f h3 h4\n  have h6 : g' ∘ f = id := Theorem_5_3_2_1 f g' h5\n  have h7 : g = g' :=\n    calc g\n      _ = id ∘ g := by rfl\n      _ = (g' ∘ f) ∘ g := by rw [h6]\n      _ = g' ∘ (f ∘ g) := by rfl\n      _ = g' ∘ id := by rw [h2]\n      _ = g' := by rfl\n  rewrite [←h7] at h5\n  show graph g = inv (graph f) from h5\n  done\n\nExercises\n\ntheorem Theorem_5_3_2_2 {A B : Type} (f : A → B) (g : B → A)\n    (h1 : graph g = inv (graph f)) : f ∘ g = id := sorry\n\n\ntheorem Theorem_5_3_3_2 {A B : Type} (f : A → B) (g : B → A)\n    (h1 : f ∘ g = id) : onto f := sorry\n\n\ntheorem Exercise_5_3_11a {A B : Type} (f : A → B) (g : B → A) :\n    one_to_one f → f ∘ g = id → graph g = inv (graph f) := sorry\n\n\ntheorem Exercise_5_3_11b {A B : Type} (f : A → B) (g : B → A) :\n    onto f → g ∘ f = id → graph g = inv (graph f) := sorry\n\n\ntheorem Exercise_5_3_14a {A B : Type} (f : A → B) (g : B → A)\n    (h : f ∘ g = id) : ∀ x ∈ Ran (graph g), g (f x) = x := sorry\n\n\ntheorem Exercise_5_3_18 {A B C : Type} (f : A → C) (g : B → C)\n    (h1 : one_to_one g) (h2 : onto g) :\n    ∃ (h : A → B), g ∘ h = f := sorry\n\n\n\n\nThe next two exercises will use the following definition:\ndef conjugate (A : Type) (f1 f2 : A → A) : Prop :=\n  ∃ (g g' : A → A), (f1 = g' ∘ f2 ∘ g) ∧ (g ∘ g' = id) ∧ (g' ∘ g = id)\n\ntheorem Exercise_5_3_17a {A : Type} : symmetric (conjugate A) := sorry\n\n\ntheorem Exercise_5_3_17b {A : Type} (f1 f2 : A → A)\n    (h1 : conjugate A f1 f2) (h2 : ∃ (a : A), f1 a = a) :\n    ∃ (a : A), f2 a = a := sorry"
  },
  {
    "objectID": "Chap5.html#closures",
    "href": "Chap5.html#closures",
    "title": "5  Functions",
    "section": "5.4. Closures",
    "text": "5.4. Closures\nSuppose we have f : A → A and C : Set A. We say that C is closed under f if the value of f at any element of C is again an element of C:\ndef closed {A : Type} (f : A → A) (C : Set A) : Prop := ∀ x ∈ C, f x ∈ C\nAccording to this definition, closed f C means that C is closed under f. Sometimes, if we have a set B of type Set A that is not closed under f, we are interested in adding more elements to the set to make it closed. The closure of B under f is the smallest set containing B that is closed under f. That is, it is the smallest element of {D : Set A | B ⊆ D ∧ closed f D}, where we use the subset partial ordering on Set A to determine which element is smallest. We will write closure f B C to mean that the closure of B under f is C. We can define this as follows:\ndef closure {A : Type} (f : A → A) (B C : Set A) : Prop :=\n  smallestElt (sub A) C {D : Set A | B ⊆ D ∧ closed f D}\nWe know that smallest elements, when they exist, are unique, so it makes sense to talk about the closure of B under f. But not every set has a smallest element. Does every set have a closure? Theorem 5.4.5 in HTPI says that the answer is yes. The idea behind the proof is that, for any family of sets F, if F has a smallest element under the subset partial order, then that smallest element is equal to ⋂₀ F. (We’ll ask you to prove this in the exercises.)\ntheorem Theorem_5_4_5 {A : Type} (f : A → A) (B : Set A) :\n    ∃ (C : Set A), closure f B C := by\n  set F : Set (Set A) := {D : Set A | B ⊆ D ∧ closed f D}\n  set C : Set A := ⋂₀ F\n  apply Exists.intro C    --Goal : closure f B C\n  define                  --Goal : C ∈ F ∧ ∀ x ∈ F, C ⊆ x\n  apply And.intro\n  · -- Proof that C ∈ F\n    define                  --Goal : B ⊆ C ∧ closed f C\n    apply And.intro\n    · -- Proof that B ⊆ C\n      fix a : A\n      assume h1 : a ∈ B       --Goal : a ∈ C\n      define                  --Goal : ∀ t ∈ F, a ∈ t\n      fix D : Set A\n      assume h2 : D ∈ F\n      define at h2            --h2 : B ⊆ D ∧ closed f D\n      show a ∈ D from h2.left h1\n      done\n    · -- Proof that C is closed under f\n      define                  --Goal : ∀ x ∈ C, f x ∈ C\n      fix a : A\n      assume h1 : a ∈ C       --Goal : f a ∈ C\n      define                  --Goal : ∀ t ∈ F, f a ∈ t\n      fix D : Set A\n      assume h2 : D ∈ F       --Goal : f a ∈ D\n      define at h1            --h1 : ∀ t ∈ F, a ∈ t\n      have h3 : a ∈ D := h1 D h2\n      define at h2            --h2 : B ⊆ D ∧ closed f D\n      have h4 : closed f D := h2.right\n      define at h4            --h4 : ∀ x ∈ D, f x ∈ D\n      show f a ∈ D from h4 a h3\n      done\n    done\n  · -- Proof that C is smallest\n    fix D : Set A\n    assume h1 : D ∈ F      --Goal : sub A C D\n    define\n    fix a : A\n    assume h2 : a ∈ C       --Goal : a ∈ D\n    define at h2            --h2 : ∀ t ∈ F, a ∈ t\n    show a ∈ D from h2 D h1\n    done\n  done\nThe idea of the closure of a set under a function can also be applied to functions of two variables. One way to represent a function of two variables on a type A would be to use a function g of type (A × A) → A. If a and b have type A, then (a, b) has type A × A, and the result of applying the function g to the pair of values a and b would be written g (a, b).\nHowever, there is another way to represent a function of two variables that turns out to be more convenient in Lean. Suppose f has type A → A → A. As with the arrow used in conditional propositions, the arrow for function types groups to the right, so A → A → A means A → (A → A). Thus, if a has type A, then f a has type A → A. In other words, f a is a function from A to A, and therefore if b has type A then f a b has type A. The upshot is that if f is followed by two objects of type A, then the resulting expression has type A, so f can be thought of as a function that applies to a pair of objects of type A and gives a value of type A.\nFor example, we can think of addition of integers as a function of two variables. Here are three ways to define this function in Lean.\ndef plus (m n : Int) : Int := m + n\n\ndef plus' : Int → Int → Int := fun (m n : Int) => m + n\n\ndef plus'' : Int → Int → Int := fun (m : Int) => (fun (n : Int) => m + n)\nThe third definition matches the description above most closely: plus'' is a function that, when applied to an integer m, produces a new function plus'' m : Int → Int. The function plus'' m is defined to be the function that, when applied to an integer n, produces the value m + n. In other words, plus'' m n = m + n. The first two definitions are more convenient ways of defining exactly the same function. Let’s have Lean confirm this, and try out the function:\nexample : plus = plus'' := by rfl\n\nexample : plus' = plus'' := by rfl\n\n++#eval:: plus 3 2     --Answer: 5\nThere are two reasons why this way of representing functions of two variables in Lean is more convenient. First, it saves us the trouble of grouping the arguments of the function together into an ordered pair before applying the function. If we have f : A → A → A and a b : A, then to apply the function f to the arguments a and b we can just write f a b. Second, it allows for the possibility of “partially applying” the function f. The expression f a is meaningful, and denotes the function that, when applied to any b : A, produces the result f a b. For example, if m is an integer, then plus m denotes the function that, when applied to an integer n, produces the result m + n. We might call plus m the “add to m” function.\nWe have actually been using these ideas for a long time. In Chapter 3, we introduced the type Pred U of predicates applying to objects of type U, but we did not explain how such predicates are represented internally in Lean. In fact, Pred U is defined to be the type U → Prop, so if P has type Pred U, then P is a function from U to Prop, and if x has type U, then the proposition P x is the result of applying the function P to x. Similarly, Rel A B stands for A → B → Prop, so if R has type Rel A B, then R is a function of two variables, one of type A and one of type B. Earlier in this section, we defined closed f C to be the proposition asserting that C is closed under f. This means that closed is a function of two variables, the first a function f of type A → A and the second a set C of type Set A (where the type A is an implicit argument of closed). But that means that the partial application closed f denotes a function from Set A to Prop. In other words, closed f is a predicate applying to sets of type Set A; we could think of it as the “is closed under f” predicate. Similarly, in Section 4.4 we defined sub to be a function of three variables: if A is a type and X and Y have type Set A, then sub A X Y is the proposition X ⊆ Y. Since then, we have used the partial application sub A, which is the subset relation on Set A. For example, we used it earlier in this section in the definition of closure.\nReturning to the subject of closures, here’s how we can extend the idea of closures to functions of two variables:\ndef closed2 {A : Type} (f : A → A → A) (C : Set A) : Prop :=\n  ∀ x ∈ C, ∀ y ∈ C, f x y ∈ C\n\ndef closure2 {A : Type} (f : A → A → A) (B C : Set A) : Prop := \n  smallestElt (sub A) C {D : Set A | B ⊆ D ∧ closed2 f D}\nWe will leave it as an exercise for you to prove that closures under functions of two variables also exist.\ntheorem Theorem_5_4_9 {A : Type} (f : A → A → A) (B : Set A) :\n    ∃ (C : Set A), closure2 f B C := sorry\n\nExercises\n\nexample {A : Type} (F : Set (Set A)) (B : Set A) :\n    smallestElt (sub A) B F → B = ⋂₀ F := sorry\n\n2. If B has type Set A, then complement B is the set {a : A | a ∉ B}. Thus, for any a of type A, a ∈ complement B if and only if a ∉ B:\ndef complement {A : Type} (B : Set A) : Set A := {a : A | a ∉ B}\nUse this definition to prove the following theorem:\ntheorem Exercise_5_4_7 {A : Type} (f g : A → A) (C : Set A)\n    (h1 : f ∘ g = id) (h2 : closed f C) : closed g (complement C) := sorry\n\ntheorem Exercise_5_4_9a {A : Type} (f : A → A) (C1 C2 : Set A)\n    (h1 : closed f C1) (h2 : closed f C2) : closed f (C1 ∪ C2) := sorry\n\n\ntheorem Exercise_5_4_10a {A : Type} (f : A → A) (B1 B2 C1 C2 : Set A)\n    (h1 : closure f B1 C1) (h2 : closure f B2 C2) :\n    B1 ⊆ B2 → C1 ⊆ C2 := sorry\n\n\ntheorem Exercise_5_4_10b {A : Type} (f : A → A) (B1 B2 C1 C2 : Set A)\n    (h1 : closure f B1 C1) (h2 : closure f B2 C2) :\n    closure f (B1 ∪ B2) (C1 ∪ C2) := sorry\n\n\ntheorem Theorem_5_4_9 {A : Type} (f : A → A → A) (B : Set A) :\n    ∃ (C : Set A), closure2 f B C := sorry\n\n7. Suppose we define a set to be closed under a family of functions if it is closed under all of the functions in the family. Of course, the closure of a set B under a family of functions is the smallest set containing B that is closed under the family.\ndef closed_family {A : Type} (F : Set (A → A)) (C : Set A) : Prop :=\n  ∀ f ∈ F, closed f C\n\ndef closure_family {A : Type} (F : Set (A → A)) (B C : Set A) : Prop :=\n  smallestElt (sub A) C {D : Set A | B ⊆ D ∧ closed_family F D}\nProve that the closure of a set under a family of functions always exists:\ntheorem Exercise_5_4_13a {A : Type} (F : Set (A → A)) (B : Set A) :\n    ∃ (C : Set A), closure_family F B C := sorry"
  },
  {
    "objectID": "Chap5.html#images-and-inverse-images-a-research-project",
    "href": "Chap5.html#images-and-inverse-images-a-research-project",
    "title": "5  Functions",
    "section": "5.5. Images and Inverse Images: A Research Project",
    "text": "5.5. Images and Inverse Images: A Research Project\nSection 5.5 of HTPI introduces two new definitions (HTPI p. 268). Suppose \\(f : A \\to B\\). If \\(X \\subseteq A\\), then the image of \\(X\\) under \\(f\\) is the set \\(f(X)\\) defined as follows: \\[\nf(X) = \\{f(x) \\mid x \\in X\\} = \\{b \\in B \\mid \\exists x \\in X(f(x) = b)\\}.\n\\] If \\(Y \\subseteq B\\), then the inverse image of \\(Y\\) under \\(f\\) is the set \\(f^{-1}(Y)\\) defined as follows: \\[\nf^{-1}(Y) = \\{a \\in A \\mid f(a) \\in Y\\}.\n\\]\nHere are definitions of these concepts in Lean:\ndef image {A B : Type} (f : A → B) (X : Set A) : Set B :=\n  {f x | x ∈ X}\n\ndef inverse_image {A B : Type} (f : A → B) (Y : Set B) : Set A :=\n  {a : A | f a ∈ Y}\n\n--The following theorems illustrate the meaning of these definitions:\ntheorem image_def {A B : Type} (f : A → B) (X : Set A) (b : B) :\n    b ∈ image f X ↔ ∃ x ∈ X, f x = b := by rfl\n\ntheorem inverse_image_def {A B : Type} (f : A → B) (Y : Set B) (a : A) :\n    a ∈ inverse_image f Y ↔ f a ∈ Y := by rfl\nIt is natural to wonder how these concepts interact with familiar operations on sets. HTPI gives an example of such an interaction in Theorem 5.5.2. The theorem makes two assertions. Here are proofs of the two parts of the theorem in Lean.\ntheorem Theorem_5_5_2_1 {A B : Type} (f : A → B) (W X : Set A) :\n    image f (W ∩ X) ⊆ image f W ∩ image f X := by\n  fix y : B\n  assume h1 : y ∈ image f (W ∩ X)  --Goal : y ∈ image f W ∩ image f X\n  define at h1                     --h1 : ∃ x ∈ W ∩ X, f x = y\n  obtain (x : A) (h2 : x ∈ W ∩ X ∧ f x = y) from h1\n  define : x ∈ W ∩ X at h2         --h2 : (x ∈ W ∧ x ∈ X) ∧ f x = y\n  apply And.intro\n  · -- Proof that y ∈ image f W\n    define                         --Goal : ∃ x ∈ W, f x = y\n    show ∃ (x : A), x ∈ W ∧ f x = y from\n      Exists.intro x (And.intro h2.left.left h2.right)\n    done\n  · -- Proof that y ∈ image f X\n    show y ∈ image f X from\n      Exists.intro x (And.intro h2.left.right h2.right)\n    done\n  done\n\ntheorem Theorem_5_5_2_2 {A B : Type} (f : A → B) (W X : Set A)\n    (h1 : one_to_one f) : image f (W ∩ X) = image f W ∩ image f X := by\n  apply Set.ext\n  fix y : B      --Goal : y ∈ image f (W ∩ X) ↔ y ∈ image f W ∩ image f X\n  apply Iff.intro\n  · -- (→)\n    assume h2 : y ∈ image f (W ∩ X)\n    show y ∈ image f W ∩ image f X from Theorem_5_5_2_1 f W X h2\n    done\n  · -- (←)\n    assume h2 : y ∈ image f W ∩ image f X  --Goal : y ∈ image f (W ∩ X)\n    define at h2                  --h2 : y ∈ image f W ∧ y ∈ image f X\n    rewrite [image_def, image_def] at h2\n          --h2 : (∃ x ∈ W, f x = y) ∧ ∃ x ∈ X, f x = y\n    obtain (x1 : A) (h3 : x1 ∈ W ∧ f x1 = y) from h2.left\n    obtain (x2 : A) (h4 : x2 ∈ X ∧ f x2 = y) from h2.right\n    have h5 : f x2 = y := h4.right\n    rewrite [←h3.right] at h5  --h5 : f x2 = f x1\n    define at h1               --h1 : ∀ (x1 x2 : A), f x1 = f x2 → x1 = x2\n    have h6 : x2 = x1 := h1 x2 x1 h5\n    rewrite [h6] at h4           --h4 : x1 ∈ X ∧ f x1 = y\n    show y ∈ image f (W ∩ X) from\n      Exists.intro x1 (And.intro (And.intro h3.left h4.left) h3.right)\n    done\n  done\nThe rest of Section 5.5 of HTPI consists of statements for you to try to prove. Here are the statements, written as examples in Lean. Some are correct and some are not; some can be made correct by adding additional hypotheses or weakening the conclusion. Prove as much as you can.\n--Warning!  Not all of these examples are correct!\n\nexample {A B : Type} (f : A → B) (W X : Set A) :\n    image f (W ∪ X) = image f W ∪ image f X := sorry\n\nexample {A B : Type} (f : A → B) (W X : Set A) :\n    image f (W \\ X) = image f W \\ image f X := sorry\n\nexample {A B : Type} (f : A → B) (W X : Set A) :\n    W ⊆ X ↔ image f W ⊆ image f X := sorry\n\nexample {A B : Type} (f : A → B) (Y Z : Set B) :\n    inverse_image f  (Y ∩ Z) =\n        inverse_image f Y ∩ inverse_image f Z := sorry\n\nexample {A B : Type} (f : A → B) (Y Z : Set B) :\n    inverse_image f  (Y ∪ Z) =\n        inverse_image f Y ∪ inverse_image f Z := sorry\n\nexample {A B : Type} (f : A → B) (Y Z : Set B) :\n    inverse_image f  (Y \\ Z) =\n        inverse_image f Y \\ inverse_image f Z := sorry\n\nexample {A B : Type} (f : A → B) (Y Z : Set B) :\n    Y ⊆ Z ↔ inverse_image f Y ⊆ inverse_image f Z := sorry\n\nexample {A B : Type} (f : A → B) (X : Set A) :\n    inverse_image f (image f X) = X := sorry\n\nexample {A B : Type} (f : A → B) (Y : Set B) :\n    image f (inverse_image f Y) = Y := sorry\n\nexample {A : Type} (f : A → A) (C : Set A) :\n    closed f C → image f C ⊆ C := sorry\n\nexample {A : Type} (f : A → A) (C : Set A) :\n    image f C ⊆ C → C ⊆ inverse_image f C := sorry\n\nexample {A : Type} (f : A → A) (C : Set A) :\n    C ⊆ inverse_image f C → closed f C := sorry\n\nexample {A B : Type} (f : A → B) (g : B → A) (Y : Set B)\n    (h1 : f ∘ g = id) (h2 : g ∘ f = id) :\n    inverse_image f Y = image g Y := sorry"
  },
  {
    "objectID": "Chap6.html",
    "href": "Chap6.html",
    "title": "6  Mathematical Induction",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\mathbin{∆}}\n$$"
  },
  {
    "objectID": "Chap6.html#proof-by-mathematical-induction",
    "href": "Chap6.html#proof-by-mathematical-induction",
    "title": "6  Mathematical Induction",
    "section": "6.1. Proof by Mathematical Induction",
    "text": "6.1. Proof by Mathematical Induction\nSection 6.1 of HTPI introduces a new proof technique called mathematical induction. It is used for proving statements of the form ∀ (n : Nat), P n. Here is how it works (HTPI p. 273):\n\nTo prove a goal of the form ∀ (n : Nat), P n:\n\nFirst prove P 0, and then prove ∀ (n : Nat), P n → P (n + 1). The first of these proofs is sometimes called the base case and the second the induction step.\n\nFor an explanation of why this strategy works to establish the truth of ∀ (n : Nat), P n, see HTPI. Here we focus on using mathematical induction in Lean.\nTo use mathematical induction in a Lean proof, we will use the tactic by_induc. If the goal has the form ∀ (n : Nat), P n, then the by_induc tactic leaves the list of givens unchanged, but it replaces the goal with the goals for the base case and induction step. Thus, the effect of the tactic can be summarized as follows:\n\n\n>> ⋮\n⊢ ∀ (n : Nat), P n\n\n\ncase Base_Case\n>> ⋮\n⊢ P 0\ncase Induction_Step\n>> ⋮\n⊢ ∀ (n : Nat), P n → P (n + 1)\n\n\nTo illustrate proof by mathematical induction in Lean, we turn first to Example 6.1.2 in HTPI, which gives a proof of the statement \\(\\forall n \\in \\mathbb{N} (3 \\mid (n^3 - n))\\). For reasons that we will explain a little later, we will prove a slightly different theorem: \\(\\forall n \\in \\mathbb{N} (3 \\mid (n^3 + 2n))\\). Here is a proof of the theorem, modeled on the proof in Example 6.1.2 of HTPI (HTPI pp. 276–277).\n\nFor every natural number \\(n\\), \\(3 \\mid (n^3 + 2n)\\).\n\n\nProof. We use mathematical induction.\nBase Case: If \\(n = 0\\), then \\(n^3 + 2n = 0 = 3 \\cdot 0\\), so \\(3 \\mid (n^3 + 2n)\\).\nInduction Step: Let \\(n\\) be an arbitrary natural number and suppose \\(3 \\mid (n^3 + 2n)\\). Then we can choose an integer \\(k\\) such that \\(3k = n^3 + 2n\\). Thus, \\[\\begin{align*}\n(n+1)^3 + 2(n+1) &= n^3 + 3n^2 + 3n + 1 + 2n + 2\\\\\n&= (n^3 + 2n) + 3n^2 + 3n + 3\\\\\n&= 3k + 3n^2 + 3n + 3\\\\\n&= 3(k + n^2 + n + 1).\n\\end{align*}\\] Therefore \\(3 \\mid ((n+1)^3 + 2(n+1))\\), as required.  □\n\nNow let’s try writing the same proof in Lean. We start, of course, with the by_induc tactic.\n\n\ntheorem Like_Example_6_1_2 :\n    ∀ (n : Nat), 3 ∣ n ^ 3 + 2 * n := by\n  by_induc\n  **done::\n\n\ncase Base_Case\n⊢ 3 ∣ 0 ^ 3 + 2 * 0\ncase Induction_Step\n⊢ ∀ (n : ℕ),\n>>  3 ∣ n ^ 3 + 2 * n →\n>>  3 ∣ (n + 1) ^ 3 +\n>>      2 * (n + 1)\n\n\nThe base case is easy: The define tactic tells us that the goal means ∃ (c : Nat), 0 ^ 3 + 2 * 0 = 3 * c, and then apply Exists.intro 0 changes the goal to 0 ^ 3 + 2 * 0 = 3 * 0. Both sides are definitionally equal to 0, so rfl finishes off the base case. For the induction step, we begin, as in the HTPI proof, by introducing an arbitrary natural number n and assuming 3 ∣ n ^ 3 + 2 * n. This assumption is called the inductive hypothesis, so in the Lean proof we give it the identifier ih. Our goal now is to prove 3 ∣ (n + 1) ^ 3 + 2 * (n + 1).\n\n\ntheorem Like_Example_6_1_2 :\n    ∀ (n : Nat), 3 ∣ n ^ 3 + 2 * n := by\n  by_induc\n  · -- Base Case\n    define\n    apply Exists.intro 0\n    rfl\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : 3 ∣ n ^ 3 + 2 * n\n    **done::\n  done\n\n\ncase Induction_Step\nn : ℕ\nih : 3 ∣ n ^ 3 + 2 * n\n⊢ 3 ∣ (n + 1) ^ 3 +\n>>      2 * (n + 1)\n\n\nThe rest of the Lean proof follows the model of the HTPI proof: we use the inductive hypothesis to introduce a k such that n ^ 3 + 2 * n = 3 * k, and then we use a calculational proof to show that (n + 1) ^ 3 + 2 * (n + 1) = 3 * (k + n ^ 2 + n + 1).\ntheorem Like_Example_6_1_2 :\n    ∀ (n : Nat), 3 ∣ n ^ 3 + 2 * n := by\n  by_induc\n  · -- Base Case\n    define         --Goal : ∃ (c : Nat), 0 ^ 3 + 2 * 0 = 3 * c\n    apply Exists.intro 0\n    rfl\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : 3 ∣ n ^ 3 + 2 * n\n    define at ih   --ih : ∃ (c : Nat), n ^ 3 + 2 * n = 3 * c\n    obtain (k : Nat) (h1 : n ^ 3 + 2 * n = 3 * k) from ih\n    define         --Goal : ∃ (c : Nat), (n + 1) ^ 3 + 2 * (n + 1) = 3 * c\n    apply Exists.intro (k + n ^ 2 + n + 1)\n    show (n + 1) ^ 3 + 2 * (n + 1) = 3 * (k + n ^ 2 + n + 1) from\n      calc (n + 1) ^ 3 + 2 * (n + 1)\n        _ = n ^ 3 + 2 * n + 3 * n ^ 2 + 3 * n + 3 := by ring\n        _ = 3 * k + 3 * n ^ 2 + 3 * n + 3 := by rw [h1]\n        _ = 3 * (k + n ^ 2 + n + 1) := by ring\n    done\n  done\nNext we’ll look at Example 6.1.1 in HTPI, which proves that for every natural number \\(n\\), \\(2^0 + 2^1 + \\cdots + 2^n = 2^{n+1} - 1\\). Once again, we will change the theorem slightly before proving it in Lean. What we will prove is that for every \\(n\\), \\((2^0 + 2^1 + \\cdots + 2^n) + 1 = 2^{n+1}\\). To understand this theorem you must be able to recognize what the “\\(\\cdots\\)” stands for. A human reader will probably realize that the numbers being added up here are the numbers of the form \\(2^i\\), where \\(i\\) runs through all of the natural numbers from 0 to \\(n\\). But Lean can’t be expected to figure out this pattern, so we must be more explicit.\nSection 6.3 of HTPI introduces the explicit notation that mathematicians usually use for such sums. If \\(f\\) is a function whose domain is the natural numbers, then \\[\n\\sum_{i=0}^n f(i) = f(0) + f(1) + \\cdots + f(n).\n\\] More generally, if \\(k \\le n\\) then \\[\n\\sum_{i=k}^n f(i) = f(k) + f(k+1) + \\cdots + f(n).\n\\]\nThe notation we will use in Lean for this sum is Sum i from k to n, f i. Thus, a mathematician would state our theorem like this:\n\nFor every natural number \\(n\\), \\[\n\\left(\\sum_{i=0}^n 2^i\\right) + 1 = 2^{n+1}.\n\\]\n\nAnd to state the same theorem in Lean, we will write:\ntheorem Like_Example_6_1_1 :\n    ∀ (n : Nat), (Sum i from 0 to n, 2 ^ i) + 1 = 2 ^ (n + 1)\nWe will have more to say later about how the notation Sum i from k to n, f i is defined. But to use the notation in a proof, we will just need to know a few theorems. The #check command will tell us the meanings of the theorems sum_base, sum_step, and sum_from_zero_step:\n\n@sum_base : ∀ {A : Type} [inst : AddZeroClass A] {k : ℕ} {f : ℕ → A},\n            Sum i from k to k, f i = f k\n\n@sum_step : ∀ {A : Type} [inst : AddZeroClass A] {k n : ℕ} {f : ℕ → A},\n            k ≤ n → Sum i from k to n + 1, f i =\n              (Sum i from k to n, f i) + f (n + 1)\n\n@sum_from_zero_step :\n            ∀ {A : Type} [inst : AddZeroClass A] {n : ℕ} {f : ℕ → A},\n            Sum i from 0 to n + 1, f i =\n              (Sum i from 0 to n, f i) + f (n + 1)\n\nAs usual, we don’t need to pay too much attention to the implicit arguments in the first line of each statement. What is important is that sum_base can be used to prove any statement of the form\n\nSum i from k to k, f i = f k\n\nand sum_step proves any statement of the form\n\nk ≤ n → Sum i from k to n + 1, f i = (Sum i from k to n, f i) + f (n + 1).\n\nIn the case k = 0, we have the simpler theorem sum_from_zero_step, which proves\n\nSum i from 0 to n + 1, f i = (Sum i from 0 to n, f i) + f (n + 1).\n\nWith that preparation, we can start on the proof. Once again we begin with the by_induc tactic. Our goal for the base case is (Sum i from 0 to 0, 2 ^ i) + 1 = 2 ^ (0 + 1). To deal with the term Sum i from 0 to 0, 2 ^ i, we use that fact that sum_base proves Sum i from 0 to 0, 2 ^ i = 2 ^ 0. It follows that the tactic rewrite [sum_base] will change the goal to 2 ^ 0 + 1 = 2 ^ (0 + 1). Of course, this means 2 = 2, so rfl finishes the base case. For the induction step, we start by introducing an arbitrary natural number n and assuming the inductive hypothesis.\ntheorem Like_Example_6_1_1 :\n    ∀ (n : Nat), (Sum i from 0 to n, 2 ^ i) + 1 = 2 ^ (n + 1) := by\n  by_induc\n  · -- Base Case\n    rewrite [sum_base]\n    rfl\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : (Sum i from 0 to n, 2 ^ i) + 1 = 2 ^ (n + 1)\n    **done::\n  done\nOur goal is now (Sum i from 0 to n + 1, 2 ^ i) + 1 = 2 ^ (n + 1 + 1), and we use a calculational proof to prove this. Often the key to the proof of the induction step is to find a relationship between the inductive hypothesis and the goal. In this case, that means finding a relationship between Sum i from 0 to n, 2 ^ i and Sum i from 0 to n + 1, 2 ^ i. The relationship we need is given by the theorem sum_from_zero_step. The tactic rewrite [sum_from_zero_step] will replace Sum i from 0 to n + 1, 2 ^ i with (Sum i from 0 to n, 2 ^ i) + 2 ^ (n + 1). The rest of the calculation proof involves straightforward algebra, handled by the ring tactic, together with an application of the inductive hypothesis.\ntheorem Like_Example_6_1_1 :\n    ∀ (n : Nat), (Sum i from 0 to n, 2 ^ i) + 1 = 2 ^ (n + 1) := by\n  by_induc\n  · -- Base Case\n    rewrite [sum_base]\n    rfl\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : (Sum i from 0 to n, 2 ^ i) + 1 = 2 ^ (n + 1)\n    show (Sum i from 0 to n + 1, 2 ^ i) + 1 = 2 ^ (n + 1 + 1) from\n      calc (Sum i from 0 to n + 1, 2 ^ i) + 1\n        _ = (Sum i from 0 to n, 2 ^ i) + 2 ^ (n + 1) + 1 := by\n              rw [sum_from_zero_step]\n        _ = (Sum i from 0 to n, 2 ^ i) + 1 + 2 ^ (n + 1) := by ring\n        _ = 2 ^ (n + 1) + 2 ^ (n + 1) := by rw [ih]\n        _ = 2 ^ (n + 1 + 1) := by ring\n    done\n  done\nThe last example in Section 6.1 of HTPI gives a proof of the statement \\(\\forall n \\ge 5 (2^n > n^2)\\). The proof is by mathematical induction, but since we are only interested in natural numbers greater than or equal to 5, it uses 5 in the base case instead of 0. Here are the theorem and proof from HTPI (HTPI p. 278):\n\nFor every natural number \\(n \\ge 5\\), \\(2^n > n^2\\).\n\n\nProof. By mathematical induction.\nBase case: When \\(n = 5\\) we have \\(2^n = 32 > 25 = n^2\\).\nInduction step: Let \\(n \\ge 5\\) be arbitrary, and suppose that \\(2^n > n^2\\). Then \\[\\begin{align*}\n2^{n+1} &= 2 \\cdot 2^n\\\\\n&> 2n^2 &&\\text{(inductive hypothesis)}\\\\\n&= n^2 + n^2\\\\\n&\\ge n^2 + 5n &&\\text{(since $n \\ge 5$)}\\\\\n&= n^2 + 2n + 3n\\\\\n&> n^2 + 2n + 1 = (n+1)^2. &&\\Box\n\\end{align*}\\] \n\nNotice that the sequence of calculations at the end of the proof mixes \\(=\\), \\(>\\), and \\(\\ge\\) in a way that establishes the final conclusion \\(2^{n+1} > (n+1)^2\\). As we’ll see, such a mixture is allowed in calculational proofs in Lean as well.\nTo write this proof in Lean, there is no need to specify that the base case should be n = 5; the by_induc tactic is smart enough to figure that out on its own, as you can see in the tactic state below. (Notice that, as in HTPI, in Lean we can write ∀ n ≥ k, P n as an abbreviation for ∀ (n : Nat), n ≥ k → P n.)\n\n\ntheorem Example_6_1_3 :\n    ∀ n ≥ 5, 2 ^ n > n ^ 2 := by\n  by_induc\n  **done::\n\n\ncase Base_Case\n⊢ 2 ^ 5 > 5 ^ 2\ncase Induction_Step\n⊢ ∀ n ≥ 5,\n>>  2 ^ n > n ^ 2 →\n>>  2 ^ (n + 1) >\n>>      (n + 1) ^ 2\n\n\nTo complete this proof we’ll use two tactics we haven’t used before: decide and linarith. The truth or falsity of the inequality in the base case can be decided by simply doing the necessary arithmetic. The tactic decide can do such calculations, and it proves the base case.\nFor the induction step, we introduce an arbitrary natural number n, assume n ≥ 5, and assume the inductive hypothesis, 2 ^ n > n ^ 2. Then we use a calculational proof to imitate the reasoning at the end of the HTPI proof. The tactic linarith makes inferences that involve combining linear equations and inequalities. It is able to prove almost all of the inequalities in the calculational proof. The exception is n * n ≥ 5 * n (which is not linear because of the term n * n). So we prove that inequality separately, using a theorem from Lean’s library, Nat.mul_le_mul_right. The command #check @Nat.mul_le_mul_right tells us the meaning of that theorem:\n\n@Nat.mul_le_mul_right : ∀ {n m : ℕ} (k : ℕ), n ≤ m → n * k ≤ m * k\n\nThus, Nat.mul_le_mul_right n can be used to prove the statement 5 ≤ n → 5 * n ≤ n * n. Lean recognizes x ≥ y as meaning the same thing as y ≤ x, so we can apply this statement to our assumption n ≥ 5 to prove that n * n ≥ 5 * n. Once we have proven that inequality, the linarith tactic can use it to complete the required inequality reasoning.\ntheorem Example_6_1_3 : ∀ n ≥ 5, 2 ^ n > n ^ 2 := by\n  by_induc\n  · -- Base Case\n    decide\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume h1 : n ≥ 5\n    assume ih : 2 ^ n > n ^ 2\n    have h2 : n * n ≥ 5 * n := Nat.mul_le_mul_right n h1\n    show 2 ^ (n + 1) > (n + 1) ^ 2 from\n      calc 2 ^ (n + 1)\n        _ = 2 * 2 ^ n := by ring\n        _ > 2 * n ^ 2 := by linarith\n        _ ≥ n ^ 2 + 5 * n := by linarith\n        _ > n ^ 2 + 2 * n + 1 := by linarith\n        _ = (n + 1) ^ 2 := by ring\n    done\n  done\nFinally, we turn to the question of why we made small changes in two of the examples from HTPI. Perhaps you have guessed by now that we were trying to avoid the use of subtraction. All of the numbers in the examples in this section were natural numbers, and subtraction of natural numbers is problematic. In the natural numbers, 3 - 2 is equal to 1, but what is 2 - 3? Lean’s answer is 0.\n++#eval:: 2 - 3     --Answer: 0\nIn Lean, if a and b are natural numbers and a < b, then a - b is defined to be 0. As a result, the algebraic laws of natural number subtraction are complicated. For example, 2 - 3 + 1 = 0 + 1 = 1, but 2 + 1 - 3 = 3 - 3 = 0, so it is not true that for all natural numbers a, b, and c, a - b + c = a + c - b.\nIf you thought that the answer to the subtraction problem 2 - 3 was -1, then you automatically switched from the natural numbers to the integers. (Recall that the natural numbers are the numbers 0, 1, 2, …, while the integers are the numbers …, –3, –2, –1, 0, 1, 2, 3, ….) To a human mathematician, this is a perfectly natural thing to do: the natural numbers are a subset of the integers, so 2 and 3 are not only natural numbers but also integers, and we can compute 2 - 3 in the integers.\nHowever, that’s not how things work in Lean. In Lean, different types are completely separate. In particular, Nat and Int are separate types, and therefore the natural numbers are not a subset of the integers. Of course, there is an integer 2, but it is different from the natural number 2. By default, Lean assumes that 2 denotes the natural number 2, but you can specify that you want the integer 2 by writing (2 : Int). Subtraction of integers in Lean is the subtraction you are familiar with, and it has all the algebraic properties you would expect. If we want to use subtraction in the theorems in this section, we are better off using familiar integer subtraction rather than funky natural number subtraction.\nTo prove the theorem in Example 6.1.1 as it appears in HTPI, we could state the theorem like this:\ntheorem Example_6_1_1 :\n    ∀ (n : Nat), Sum i from 0 to n, (2 : Int) ^ i =\n    (2 : Int) ^ (n + 1) - (1 : Int)\nThe expression Sum i from 0 to n, (2 : Int) ^ i denotes a sum of integers, so it is an integer. Similarly, the right side of the equation is an integer, and the equation asserts the equality of two integers. The subtraction on the right side of the equation is integer subtraction, so we can use the usual algebraic laws to reason about it. In fact, the proof of the theorem in this form is not hard:\ntheorem Example_6_1_1 :\n    ∀ (n : Nat), Sum i from 0 to n, (2 : Int) ^ i =\n    (2 : Int) ^ (n + 1) - (1 : Int) := by\n  by_induc\n  · -- Base Case\n    rewrite [sum_base]\n    rfl\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : Sum i from 0 to n, (2 : Int) ^ i =\n        (2 : Int) ^ (n + 1) - (1 : Int)\n    show Sum i from 0 to n + 1, (2 : Int) ^ i =\n        (2 : Int) ^ (n + 1 + 1) - (1 : Int) from\n      calc Sum i from 0 to n + 1, (2 : Int) ^ i\n        _ = (Sum i from 0 to n, (2 : Int) ^ i)\n            + (2 : Int) ^ (n + 1) := by rw [sum_from_zero_step]\n        _ = (2 : Int) ^ (n + 1) - (1 : Int)\n            + (2 : Int) ^ (n + 1) := by rw [ih]\n        _ = (2 : Int) ^ (n + 1 + 1) - (1 : Int) := by ring\n    done\n  done\nIf you change (2 : Int) and (1 : Int) to 2 and 1, then the right side of the equation will be a difference of two natural numbers, and Lean will interpret the subtraction as natural number subtraction. The proof won’t work because the ring tactic is not able to deal with the peculiar algebraic properties of natural number subtraction. (The theorem is still true, but the proof is harder.)\n\n\nExercises\n\ntheorem Like_Exercise_6_1_1 :\n    ∀ (n : Nat), 2 * Sum i from 0 to n, i = n * (n + 1) := sorry\n\n\ntheorem Like_Exercise_6_1_4 :\n    ∀ (n : Nat), Sum i from 0 to n, 2 * i + 1 = (n + 1) ^ 2 := sorry\n\n\ntheorem Exercise_6_1_9a : ∀ (n : Nat), 2 ∣ n ^ 2 + n := sorry\n\n\ntheorem Exercise_6_1_13 :\n    ∀ (a b : Int) (n : Nat), (a - b) ∣ (a ^ n - b ^ n) := sorry\n\n\ntheorem Exercise_6_1_15 : ∀ n ≥ 10, 2 ^ n > n ^ 3 := sorry\n\n\nlemma nonzero_is successor :\n    ∀ (n : Nat), n ≠ 0 → ∃ (m : Nat), n = m + 1 := sorry\n\nFor the next two exercises you will need the following definitions:\ndef nat_even (n : Nat) : Prop := ∃ (k : Nat), n = 2 * k\n\ndef nat_odd (n : Nat) : Prop := ∃ (k : Nat), n = 2 * k + 1\n\ntheorem Exercise_6_1_16a1 :\n    ∀ (n : Nat), nat_even n ∨ nat_odd n := sorry\n\n\n--Hint:  You may find the lemma nonzero_is_successor\n--from a previous exercise useful, as well as Nat.add_right_cancel.\ntheorem Exercise_6_1_16a2 :\n    ∀ (n : Nat), ¬(nat_even n ∧ nat_odd n) := sorry"
  },
  {
    "objectID": "Chap6.html#more-examples",
    "href": "Chap6.html#more-examples",
    "title": "6  Mathematical Induction",
    "section": "6.2. More Examples",
    "text": "6.2. More Examples\nWe saw in the last section that mathematical induction can be used to prove theorems about calculations involving natural numbers. But mathematical induction has a much wider range of uses. Section 6.2 of HTPI illustrates this by proving two theorems about finite sets.\nHow can mathematical induction be used to prove a statement about finite sets? To say that a set is finite means that it has \\(n\\) elements, for some natural number \\(n\\). Thus, to say that all finite sets have some property, we can say that for every natural number \\(n\\), every set with \\(n\\) elements has the property. Since this statement starts with “for every natural number \\(n\\),” we can use mathematical induction to try to prove it.\nWhat does it mean to say that a set “has \\(n\\) elements”? Section 6.2 of HTPI says that for the proofs in that section, “an intuitive understanding of this concept will suffice.” Unfortunately, intuition is not Lean’s strong suit! So we’ll need to be more explicit about how to talk about finite sets in Lean.\nIn Chapter 8, we’ll define numElts A n to be a proposition saying that the set A has n elements, and we’ll prove several theorems involving that proposition. Those theorems make precise and explicit the intuitive ideas that we’ll need in this section. We’ll state those theorems here and use them in our proofs, but you’ll have to wait until Section 8.1½ to see how they are proven. Here are the theorems we’ll need:\ntheorem zero_elts_iff_empty {U : Type} (A : Set U) :\n    numElts A 0 ↔ empty A\n\ntheorem one_elt_iff_singleton {U : Type} (A : Set U) :\n    numElts A 1 ↔ ∃ (x : U), A = {x}\n\ntheorem nonempty_of_pos_numElts {U : Type} {A : Set U} {n : Nat}\n    (h1 : numElts A n) (h2 : n > 0) : ∃ (x : U), x ∈ A\n\ntheorem remove_one_numElts {U : Type} {A : Set U} {n : Nat} {a : U}\n    (h1 : numElts A (n + 1)) (h2 : a ∈ A) : numElts (A \\ {a}) n\nThese theorems should make intuitive sense. The first says that a set has zero elements if and only if it is empty, and the second says that a set has one element if and only if it is a singleton set. The third theorem says that if a set has a positive number of elements, then there is something in the set. And the fourth says that if a set has \\(n + 1\\) elements and you remove one element, then the resulting set has \\(n\\) elements. You can probably guess that we’ll be using the last theorem in the induction steps of our proofs.\nOur first theorem about finite sets says that if \\(R\\) is a partial order on \\(A\\), then every finite, nonempty subset of \\(A\\) has an \\(R\\)-minimal element. (This is not true in general for infinite subsets of \\(A\\). Can you think of an example of an infinite subset of a partially ordered set that has no minimal element?) To say that a set is finite and nonempty we can say that it has \\(n\\) elements for some \\(n \\ge 1\\). So here’s how we state our theorem in Lean:\ntheorem Example_6_2_1 {A : Type} (R : BinRel A) (h : partial_order R) :\n    ∀ n ≥ 1, ∀ (B : Set A), numElts B n →\n      ∃ (x : A), minimalElt R x B\nWhen we use mathematical induction to prove this theorem, the base case will be n = 1. To write the proof for the base case, we start by assuming B is a set with one element. We can then use the theorem one_elt_iff_singleton to conclude that B = {b}, for some b of type A. We need to prove that B has a minimal element, and the only possibility for the minimal element is b. Verifying that minimalElt R b B is straightforward. Here is the proof of the base case:\ntheorem Example_6_2_1 {A : Type} (R : BinRel A) (h : partial_order R) :\n    ∀ n ≥ 1, ∀ (B : Set A), numElts B n →\n      ∃ (x : A), minimalElt R x B := by\n  by_induc\n  · -- Base Case\n    fix B : Set A\n    assume h2 : numElts B 1\n    rewrite [one_elt_iff_singleton] at h2\n    obtain (b : A) (h3 : B = {b}) from h2\n    apply Exists.intro b\n    define         --Goal : b ∈ B ∧ ¬∃ x ∈ B, R x b ∧ x ≠ b\n    apply And.intro\n    · -- Proof that b ∈ B\n      rewrite [h3]    --Goal : b ∈ {b}\n      define          --Goal : b = b\n      rfl\n      done\n    · -- Proof that nothing in B is smaller than b\n      by_contra h4\n      obtain (x : A) (h5 : x ∈ B ∧ R x b ∧ x ≠ b) from h4\n      have h6 : x ∈ B := h5.left\n      rewrite [h3] at h6   --h6 : x ∈ {b}\n      define at h6         --h6 : x = b\n      show False from h5.right.right h6\n      done\n    done\n  · -- Induction Step\n\n    **done::\n  done\nNotice that since the definition of minimalElt R b B involves a negative statement, we found it convenient to use proof by contradiction to prove it.\nFor the induction step, we assume that n ≥ 1 and that every set with n elements has an R-minimal element. We must prove that every set with n + 1 elements has a minimal element, so we let B be an arbitrary set with n + 1 elements. To apply the inductive hypothesis, we need a set with n elements. So we pick some b ∈ B (using the theorem nonempty_of_pos_numElts) and then remove it from B to get the set B' = B \\ {b}. The theorem remove_one_numElts tells us that B' has n elements, so by the inductive hypothesis, we can then let c be a minimal element of B'. We now know about two elements of B: b and c. Which will be a minimal element of B? As explained in HTPI, it depends on whether or not R b c. We’ll prove that if R b c, then b is a minimal element of B, and if not, then c is a minimal element. It will be convenient to prove these last two facts separately as lemmas. The first lemma says that in the situation at this point in the proof, if R b c, then b is an R-minimal element of B. Here is the proof.\nlemma Lemma_6_2_1_1 {A : Type} {R : BinRel A} {B : Set A} {b c : A}\n    (h1 : partial_order R) (h2 : b ∈ B) (h3 : minimalElt R c (B \\ {b}))\n    (h4 : R b c) : minimalElt R b B := by\n  define at h3\n    --h3 : c ∈ B \\ {b} ∧ ¬∃ x ∈ B \\ {b}, R x c ∧ x ≠ c\n  define  --Goal : b ∈ B ∧ ¬∃ x ∈ B, R x b ∧ x ≠ b\n  apply And.intro h2    --Goal : ¬∃ x ∈ B, R x b ∧ x ≠ b\n  contradict h3.right with h5\n  obtain (x : A) (h6 : x ∈ B ∧ R x b ∧ x ≠ b) from h5\n  apply Exists.intro x  --Goal : x ∈ B \\ {b} ∧ R x c ∧ x ≠ c\n  apply And.intro\n  · -- Proof that x ∈ B \\ {b}\n    show x ∈ B \\ {b} from And.intro h6.left h6.right.right\n    done\n  · -- Proof that R x c ∧ x ≠ c\n    have Rtrans : transitive R := h1.right.left\n    have h7 : R x c := Rtrans x b c h6.right.left h4\n    apply And.intro h7\n    by_contra h8\n    rewrite [h8] at h6  --h6 : c ∈ B ∧ R c b ∧ c ≠ b\n    have Rantisymm : antisymmetric R := h1.right.right\n    have h9 : c = b := Rantisymm c b h6.right.left h4\n    show False from h6.right.right h9\n    done\n  done\nThe second lemma says that if ¬R b c, then c is an R-minimal element of B. We’ll leave the proof as an exercise for you:\nlemma Lemma_6_2_1_2 {A : Type} {R : BinRel A} {B : Set A} {b c : A}\n    (h1 : partial_order R) (h2 : b ∈ B) (h3 : minimalElt R c (B \\ {b}))\n    (h4 : ¬R b c) : minimalElt R c B := sorry\nWith this preparation, we are finally ready to give the proof of the induction step of Example_6_2_1:\ntheorem Example_6_2_1 {A : Type} (R : BinRel A) (h : partial_order R) :\n    ∀ n ≥ 1, ∀ (B : Set A), numElts B n →\n      ∃ (x : A), minimalElt R x B := by\n  by_induc\n  · -- Base Case\n    ...\n  · -- Induction Step\n    fix n : Nat\n    assume h2 : n ≥ 1\n    assume ih : ∀ (B : Set A), numElts B n → ∃ (x : A), minimalElt R x B\n    fix B : Set A\n    assume h3 : numElts B (n + 1)\n    have h4 : n + 1 > 0 := by linarith\n    obtain (b : A) (h5 : b ∈ B) from nonempty_of_pos_numElts h3 h4\n    set B' : Set A := B \\ {b}\n    have h6 : numElts B' n := remove_one_numElts h3 h5\n    obtain (c : A) (h7 : minimalElt R c B') from ih B' h6\n    by_cases h8 : R b c\n    · -- Case 1. h8 : R b c\n      have h9 : minimalElt R b B := Lemma_6_2_1_1 h h5 h7 h8\n      show ∃ (x : A), minimalElt R x B from Exists.intro b h9\n      done\n    · -- Case 2. h8 : ¬R b c\n      have h9 : minimalElt R c B := Lemma_6_2_1_2 h h5 h7 h8\n      show ∃ (x : A), minimalElt R x B from Exists.intro c h9\n      done\n    done\n  done\nWe’ll consider one more theorem from Section 6.2 of HTPI. Example 6.2.2 proves that a partial order on a finite set can always be extended to a total order. Rather than give that proof, we are going to prove the more general theorem that is stated in Exercise 2 in Section 6.2 of HTPI. To explain the theorem in that exercise, it will be helpful to introduce a bit of terminology. Suppose R is a partial order on A and b has type A. We will say that b is R-comparable to everything if ∀ (x : A), R b x ∨ R x b. If B is a set of objects of type A, we say that B is R-comparable to everything if every element of B is R-comparable to everything; that is, if ∀ b ∈ B, ∀ (x : A), R b x ∨ R x b. Finally, we say that another binary relation T extends R if ∀ (x y : A), R x y → T x y. We are going to prove that if R is a partial order on A and B is a finite set of objects of type A, then there is a partial order T that extends R such that B is T-comparable to everything. In other words, we are going to prove the following theorem:\ntheorem Exercise_6_2_2 {A : Type} (R : BinRel A) (h : partial_order R) :\n    ∀ (n : Nat) (B : Set A), numElts B n → ∃ (T : BinRel A),\n    partial_order T ∧ (∀ (x y : A), R x y → T x y) ∧\n    ∀ x ∈ B, ∀ (y : A), T x y ∨ T y x\nIn the exercises, we will ask you to show that this implies the theorem in Example 6.2.2.\nIt will be helpful to begin with a warm-up exercise. We’ll show that a partial order can always be extended to make a single object comparable to everything. In other words, we’ll show that if R is a partial order on A and b has type A, then we can define a partial order T extending R such that b is T-comparable to everything. To define T, we will need to make sure that for every x of type A, either T b x or T x b. If R x b, then since T must extend R, we must have T x b. If ¬R x b, then we will define T so that T b x. But notice that if we follow this plan, then for any x and y, if we have R x b and ¬R y b, then we will have T x b and T b y, and since T must be transitive, we must then have T x y. Summing up, if we have R x y then we must have T x y, and if we have R x b and ¬R y b then we will also need to have T x y. So let’s try defining T x y to mean R x y ∨ (R x b ∧ ¬R y b).\nIt will be useful to have a name for this relation T. Since it is an extension of R determined by the element b, we will give it the name extendPO R b. Here is the definition of this relation:\ndef extendPO {A : Type} (R : BinRel A) (b : A) (x y : A) : Prop :=\n  R x y ∨ (R x b ∧ ¬R y b)\nWe need to prove a number of things about extendPO R b. First of all, we need to prove that it is a partial order. We’ll leave most of the details as exercises for you:\nlemma extendPO_is_ref {A : Type} (R : BinRel A) (b : A)\n    (h : partial_order R) : reflexive (extendPO R b) := sorry\n\nlemma extendPO_is_trans {A : Type} (R : BinRel A) (b : A)\n    (h : partial_order R) : transitive (extendPO R b) := sorry\n\nlemma extendPO_is_antisymm {A : Type} (R : BinRel A) (b : A)\n    (h : partial_order R) : antisymmetric (extendPO R b) := sorry\n\nlemma extendPO_is_po {A : Type} (R : BinRel A) (b : A)\n    (h : partial_order R) : partial_order (extendPO R b) := \n  And.intro (extendPO_is_ref R b h)\n    (And.intro (extendPO_is_trans R b h) (extendPO_is_antisymm R b h))\nIt is easy to prove that extendPO R b extends R:\nlemma extendPO_extends {A : Type} (R : BinRel A) (b : A) (x y : A) :\n    R x y → extendPO R b x y := by\n  assume h1 : R x y\n  define\n  show R x y ∨ R x b ∧ ¬R y b from Or.inl h1\n  done\nFinally, we verify that extendPO R b does what it was supposed to do: it makes b comparable with everything:\nlemma extendPO_all_comp {A : Type} (R : BinRel A) (b : A)\n    (h : partial_order R) :\n    ∀ (x : A), extendPO R b b x ∨ extendPO R b x b := by\n  have Rref : reflexive R := h.left\n  fix x : A\n  or_left with h1\n  define at h1     --h1 : ¬(R x b ∨ R x b ∧ ¬R b b)\n  demorgan at h1   --h1 : ¬R x b ∧ ¬(R x b ∧ ¬R b b)\n  define           --Goal : R b x ∨ R b b ∧ ¬R x b\n  apply Or.inr\n  show R b b ∧ ¬R x b from And.intro (Rref b) h1.left\n  done\nWith this preparation, we can finally return to our theorem Exercise_6_2_2. We will prove it by mathematical induction. In the base case we must show that if B has 0 elements then we can extend R to make everything in B comparable to everything. Of course, no extension is necessary, since it is vacuously true that all elements of B are R-comparable to everything. For the induction step, after assuming the inductive hypothesis, we must prove that if B has n + 1 elements then we can extend R to make all elements of B comparable to everything. As before, we choose b ∈ B and let B' = B \\ {b}. By inductive hypothesis, we can find an extension T' of R that makes all elements of B' comparable to everything, so we just have to extend T' further to make b comparable to everything. But as we have just seen, we can do this with extendPO T' b.\ntheorem Exercise_6_2_2 {A : Type} (R : BinRel A) (h : partial_order R) :\n    ∀ (n : Nat) (B : Set A), numElts B n → ∃ (T : BinRel A),\n    partial_order T ∧ (∀ (x y : A), R x y → T x y) ∧\n    ∀ x ∈ B, ∀ (y : A), T x y ∨ T y x := by\n  by_induc\n  · -- Base Case\n    fix B : Set A\n    assume h2 : numElts B 0\n    rewrite [zero_elts_iff_empty] at h2\n    define at h2     --h2 : ¬∃ (x : A), x ∈ B\n    apply Exists.intro R\n    apply And.intro h\n    apply And.intro\n    · -- Proof that R extends R\n      fix x : A; fix y : A\n      assume h3 : R x y\n      show R x y from h3\n      done\n    · -- Proof that everything in B comparable to everything under R\n      fix x : A\n      assume h3 : x ∈ B\n      contradict h2\n      show ∃ (x : A), x ∈ B from Exists.intro x h3\n      done\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : ∀ (B : Set A), numElts B n → ∃ (T : BinRel A),\n      partial_order T ∧ (∀ (x y : A), R x y → T x y) ∧\n      ∀ (x : A), x ∈ B → ∀ (y : A), T x y ∨ T y x\n    fix B : Set A\n    assume h2 : numElts B (n + 1)\n    have h3 : n + 1 > 0 := by linarith\n    obtain (b : A) (h4 : b ∈ B) from nonempty_of_pos_numElts h2 h3\n    set B' : Set A := B \\ {b}\n    have h5 : numElts B' n := remove_one_numElts h2 h4\n    have h6 : ∃ (T : BinRel A), partial_order T ∧\n      (∀ (x y : A), R x y → T x y) ∧\n      ∀ (x : A), x ∈ B' → ∀ (y : A), T x y ∨ T y x := ih B' h5\n    obtain (T' : BinRel A)\n      (h7 : partial_order T' ∧ (∀ (x y : A), R x y → T' x y) ∧\n      ∀ (x : A), x ∈ B' → ∀ (y : A), T' x y ∨ T' y x) from h6\n    have T'po : partial_order T' := h7.left\n    have T'extR : ∀ (x y : A), R x y → T' x y := h7.right.left\n    have T'compB' : ∀ (x : A), x ∈ B' →\n      ∀ (y : A), T' x y ∨ T' y x := h7.right.right\n    set T : BinRel A := extendPO T' b\n    apply Exists.intro T\n    apply And.intro (extendPO_is_po T' b T'po)\n    apply And.intro\n    · -- Proof that T extends R\n      fix x : A; fix y : A\n      assume h8 : R x y\n      have h9 : T' x y := T'extR x y h8\n      show T x y from (extendPO_extends T' b x y h9)\n      done\n    · -- Proof that everything in B comparable to everything under T\n      fix x : A\n      assume h8 : x ∈ B\n      by_cases h9 : x = b\n      · -- Case 1. h9 : x = b\n        rewrite [h9]\n        show ∀ (y : A), T b y ∨ T y b from extendPO_all_comp T' b T'po\n        done\n      · -- Case 2. h9 : x ≠ b\n        have h10 : x ∈ B' := And.intro h8 h9\n        fix y : A\n        have h11 : T' x y ∨ T' y x := T'compB' x h10 y\n        by_cases on h11\n        · -- Case 2.1. h11 : T' x y\n          show T x y ∨ T y x from\n            Or.inl (extendPO_extends T' b x y h11)\n          done\n        · -- Case 2.2. h11 : T' y x\n          show T x y ∨ T y x from\n            Or.inr (extendPO_extends T' b y x h11)\n          done\n        done\n      done\n    done\n  done\n\nExercises\n\nlemma Lemma_6_2_1_2 {A : Type} {R : BinRel A} {B : Set A} {b c : A}\n    (h1 : partial_order R) (h2 : b ∈ B) (h3 : minimalElt R c (B \\ {b}))\n    (h4 : ¬R b c) : minimalElt R c B := sorry\n\n\nlemma extendPO_is_ref {A : Type} (R : BinRel A) (b : A)\n    (h : partial_order R) : reflexive (extendPO R b) := sorry\n\n\nlemma extendPO_is_trans {A : Type} (R : BinRel A) (b : A)\n    (h : partial_order R) : transitive (extendPO R b) := sorry\n\n\nlemma extendPO_is_antisymm {A : Type} (R : BinRel A) (b : A)\n    (h : partial_order R) : antisymmetric (extendPO R b) := sorry\n\n\ntheorem Exercise_6_2_3 {A : Type} (R : BinRel A)\n    (h : total_order R) : ∀ n ≥ 1, ∀ (B : Set A),\n    numElts B n → ∃ (b : A), smallestElt R b B := sorry\n\n\n--Hint:  First prove that R is reflexive.\ntheorem Exercise_6_2_4a {A : Type} (R : BinRel A)\n    (h : ∀ (x y : A), R x y ∨ R y x) : ∀ n ≥ 1, ∀ (B : Set A),\n    numElts B n → ∃ x ∈ B, ∀ y ∈ B, ∃ (z : A), R x z ∧ R z y := sorry\n\n\ntheorem Like_Exercise_6_2_16 {A : Type} (f : A → A)\n    (h : one_to_one f) : ∀ (n : Nat) (B : Set A), numElts B n →\n    closed f B → ∀ y ∈ B, ∃ x ∈ B, f x = y := sorry\n\n\n--Hint:  Use Exercise_6_2_2.\ntheorem Example_6_2_2 {A : Type} (R : BinRel A)\n    (h1 : ∃ (n : Nat), numElts {x : A | x = x} n)\n    (h2 : partial_order R) : ∃ (T : BinRel A),\n      total_order T ∧ ∀ (x y : A), R x y → T x y := sorry"
  },
  {
    "objectID": "Chap6.html#recursion",
    "href": "Chap6.html#recursion",
    "title": "6  Mathematical Induction",
    "section": "6.3. Recursion",
    "text": "6.3. Recursion\nIn the last two sections, we saw that we can prove that all natural numbers have some property by proving that 0 has the property, and also that for every natural number \\(n\\), if \\(n\\) has the property then so does \\(n + 1\\). In this section we will see that a similar idea can be used to define a function whose domain is the natural numbers. We can define a function \\(f\\) with domain \\(\\mathbb{N}\\) by specifying the value of \\(f(0)\\), and also saying how to compute \\(f(n+1)\\) if you already know the value of \\(f(n)\\).\nFor example, we can define a function \\(f : \\mathbb{N} \\to \\mathbb{N}\\) as follows:\n\n\\(f(0) = 1\\); for every \\(n \\in \\mathbb{N}\\), \\(f(n+1) = (n+1) \\cdot f(n)\\).\n\nHere is the same definition written in Lean. (For reasons that will become clear shortly, we have given the function the name fact.)\ndef fact (k : Nat) : Nat :=\n  match k with\n    | 0 => 1\n    | n + 1 => (n + 1) * fact n\nLean can use this definition to compute fact k for any natural number k. The match statement tells Lean to try to match the input k with one of the two patterns 0 and n + 1, and then to use the corresponding formula after => to compute fact k. For example, if we ask Lean for fact 4, it first checks if 4 matches 0. Since it doesn’t, it goes on to the next line and determines that 4 matches the pattern n + 1, with n = 3, so it uses the formula fact 4 = 4 * fact 3. Of course, now it must compute fact 3, which it does in the same way: 3 matches n + 1 with n = 2, so fact 3 = 3 * fact 2. Continuing in this way, Lean determines that\n\nfact 4 = 4 * fact 3 = 4 * (3 * fact 2) = 4 * (3 * (2 * fact 1))\n       = 4 * (3 * (2 * (1 * fact 0))) = 4 * (3 * (2 * (1 * 1))) = 24.\n\nYou can confirm this with the #eval command:\n#eval fact 4   --Answer: 24\nOf course, by now you have probably guessed why we used the name fact for his function: fact k is k factorial—the product of all the numbers from 1 to k.\nThis style of definition is called a recursive definition. If a function is defined by a recursive definition, then theorems about that function are often most easily proven by induction. For example, here is a theorem about the factorial function. It is Example 6.3.1 in HTPI, and we begin the Lean proof by imitating the proof in HTPI.\ntheorem ??Example_6_3_1:: : ∀ n ≥ 4, fact n > 2 ^ n := by\n  by_induc\n  · -- Base Case\n    decide\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume h1 : n ≥ 4\n    assume ih : fact n > 2 ^ n\n    show fact (n + 1) > 2 ^ (n + 1) from\n      calc fact (n + 1)\n        _ = (n + 1) * fact n := by rfl\n        _ > (n + 1) * 2 ^ n := sorry\n        _ > 2 * 2 ^ n := sorry\n        _ = 2 ^ (n + 1) := by ring\n    done\n  done\nThere are two steps in the calculational proof at the end that require justification. The first says that (n + 1) * fact n > (n + 1) * 2 ^ n, which should follow from the inductive hypothesis ih : fact n > 2 ^ n by multiplying both sides by n + 1. Is there a theorem that would justify this inference?\nThis may remind you of a step in Example_6_1_3 where we used the theorem Nat.mul_le_mul_right, which says ∀ {n m : ℕ} (k : ℕ), n ≤ m → n * k ≤ m * k. Our situation in this example is similar, but it involves a strict inequality (> rather than ≥) and it involves multiplying on the left rather than the right. Many theorems about inequalities in Lean’s library contain either le (for “less than or equal to”) or lt (for “less than”) in their names, but they can also be used to prove statements involving ≥ or >. Perhaps the theorem we need is named something like Nat.mul_lt_mul_left. If you type #check @Nat.mul_lt_mul_ into VS Code, a pop-up window will appear listing several theorems that begin with Nat.mul_lt_mul_. There is no Nat.mul_lt_mul_left, but there is a theorem called Nat.mul_lt_mul_of_pos_left, and its meaning is\n\n@Nat.mul_lt_mul_of_pos_left : ∀ {n m k : ℕ},\n                                n < m → k > 0 → k * n < k * m\n\nLean has correctly reminded us that, to multiply both sides of a strict inequality by a number k, we need to know that k > 0. So in our case, we’ll need to prove that n + 1 > 0. Once we have that, we can use the theorem Nat.mul_lt_mul_of_pos_left to eliminate the first sorry.\nThe second sorry is similar: (n + 1) * 2 ^ n > 2 * 2 ^ n should follow from n + 1 > 2 and 2 ^ n > 0, and you can verify that the theorem that will justify this inference is Nat.mul_lt_mul_of_pos_right.\nSo we have three inequalities that we need to prove before we can justify the steps of the calculational proof: n + 1 > 0, n + 1 > 2, and 2 ^ n > 0. We’ll insert have steps before the calculational proof to assert these three inequalities. If you try it, you’ll find that linarith can prove the first two, but not the third.\nHow can we prove 2 ^ n > 0? It is often helpful to think about whether there is a general principle that is behind a statement we are trying to prove. In our case, the inequality 2 ^ n > 0 is an instance of the general fact that if m and n are any natural numbers with m > 0, then m ^ n > 0. Maybe that fact is in Lean’s library:\nexample (m n : Nat) (h : m > 0) : m ^ n > 0 := by ++apply?::\nThe apply? tactic comes up with exact Nat.pos_pow_of_pos n h, and #check @Nat.pos_pow_of_pos gives the result\n\n@Nat.pos_pow_of_pos : ∀ {n : ℕ} (m : ℕ), 0 < n → 0 < n ^ m\n\nThat means that we can use Nat.pos_pow_of_pos to prove 2 ^ n > 0, but first we’ll need to prove that 2 > 0. We now have all the pieces we need; putting them together leads to this proof:\ntheorem Example_6_3_1 : ∀ n ≥ 4, fact n > 2 ^ n := by\n  by_induc\n  · -- Base Case\n    decide\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume h1 : n ≥ 4\n    assume ih : fact n > 2 ^ n\n    have h2 : n + 1 > 0 := by linarith\n    have h3 : n + 1 > 2 := by linarith\n    have h4 : 2 > 0 := by linarith\n    have h5 : 2 ^ n > 0 := Nat.pos_pow_of_pos n h4\n    show fact (n + 1) > 2 ^ (n + 1) from\n      calc fact (n + 1)\n        _ = (n + 1) * fact n := by rfl\n        _ > (n + 1) * 2 ^ n := Nat.mul_lt_mul_of_pos_left ih h2\n        _ > 2 * 2 ^ n := Nat.mul_lt_mul_of_pos_right h3 h5\n        _ = 2 ^ (n + 1) := by ring\n    done\n  done\nBut there is an easier way. Look at the two “>” steps in the calculational proof at the end of Example_6_3_1. In both cases, we took a known relationship between two quantities and did something to both sides that preserved the relationship. In the first case, the known relationship was ih : fact n > 2 ^ n, and we multiplied both sides by n + 1 on the left; in the second, the known relationship was h3 : n + 1 > 2, and we multiplied both sides by 2 ^ n on the right. To justify these steps, we had to find the right theorems in Lean’s library, and we ended up needing auxiliary positivity facts: h2 : n + 1 > 0 in the first case and h5 : 2 ^ n > 0 in the second. There is a tactic that can simplify these steps: if h is a proof of a statement asserting a relationship between two quantities, then the tactic rel [h] will attempt to prove any statement obtained from that relationship by applying the same operation to both sides. The tactic will try to find a theorem in Lean’s library that says that the operation preserves the relationship, and if the theorem requires auxiliary positivity facts, it will try to prove those facts as well. The rel tactic doesn’t always succeed, but when it does, it saves you the trouble of searching through the library for the necessary theorems. In this case, the tactic allows us to give a much simpler proof of Example_6_3_1:\ntheorem Example_6_3_1 : ∀ n ≥ 4, fact n > 2 ^ n := by\n  by_induc\n  · -- Base Case\n    decide\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume h1 : n ≥ 4\n    assume ih : fact n > 2 ^ n\n    have h2 : n + 1 > 2 := by linarith\n    show fact (n + 1) > 2 ^ (n + 1) from\n      calc fact (n + 1)\n        _ = (n + 1) * fact n := by rfl\n        _ > (n + 1) * 2 ^ n := by rel [ih]\n        _ > 2 * 2 ^ n := by rel [h2]\n        _ = 2 ^ (n + 1) := by ring\n    done\n  done\nThe next example in HTPI is a proof of one of the laws of exponents: a ^ (m + n) = a ^ m * a ^ n. Lean’s definition of exponentiation with natural number exponents is recursive. The definitions Lean uses are essentially as follows:\n--For natural numbers b and k, b ^ k = nat_pow b k:\ndef nat_pow (b k : Nat) : Nat :=\n  match k with\n    | 0 => 1\n    | n + 1 => (nat_pow b n) * b\n\n--For a real number b and a natural number k, b ^ k = real_pow b k:\ndef real_pow (b : Real) (k : Nat) : Real :=\n  match k with\n    | 0 => 1\n    | n + 1 => (real_pow b n) * b\nLet’s prove the addition law for exponents:\ntheorem Example_6_3_2_cheating : ∀ (a : Real) (m n : Nat),\n    a ^ (m + n) = a ^ m * a ^ n := by\n  fix a : Real; fix m : Nat; fix n : Nat\n  ring\n  done\nWell, that wasn’t really fair. The ring tactic knows the laws of exponents, so it has no trouble proving this theorem. But we want to know why the law holds, so let’s see if we can prove it without using ring. The following proof is essentially the same as the proof in HTPI:\ntheorem Example_6_3_2 : ∀ (a : Real) (m n : Nat),\n    a ^ (m + n) = a ^ m * a ^ n := by\n  fix a : Real; fix m : Nat\n    --Goal : ∀ (n : Nat), a ^ (m + n) = a ^ m * a ^ n\n  by_induc\n  · -- Base Case\n    show a ^ (m + 0) = a ^ m * a ^ 0 from\n      calc a ^ (m + 0)\n        _ = a ^ m := by rfl\n        _ = a ^ m * 1 := (mul_one (a ^ m)).symm\n        _ = a ^ m * a ^ 0 := by rfl\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : a ^ (m + n) = a ^ m * a ^ n\n    show a ^ (m + (n + 1)) = a ^ m * a ^ (n + 1) from\n      calc a ^ (m + (n + 1))\n        _ = a ^ ((m + n) + 1) := by rw [add_assoc]\n        _ = a ^ (m + n) * a := by rfl\n        _ = (a ^ m * a ^ n) * a := by rw [ih]\n        _ = a ^ m * (a ^ n * a) := by rw [mul_assoc]\n        _ = a ^ m * (a ^ (n + 1)) := by rfl\n    done\n  done\nFinally, we’ll prove the theorem in Example 6.3.4 of HTPI, which again involves exponentiation with natural number exponents. Here’s the beginning of the proof:\n\n\ntheorem Example_6_3_4 : ∀ (x : Real), x > -1 →\n    ∀ (n : Nat), (1 + x) ^ n ≥ 1 + n * x := by\n  fix x : Real\n  assume h1 : x > -1\n  **done::\n\n\nx : ℝ\nh1 : x > -1\n⊢ ∀ (n : ℕ),\n>>  (1 + x) ^ n ≥\n>>    1 + ↑n * x\n\n\nLook carefully at the goal in the tactic state. Why is there a ↑ before the last n? The reason has to do with types. The variable x has type Real and n has type Nat, so how can Lean multiply n by x? Remember, in Lean, the natural numbers are not a subset of the real numbers. The two types are completely separate, but for each natural number, there is a corresponding real number. To multiply n by x, Lean had to convert n to the corresponding real number, through a process called coercion. The notation ↑n denotes the result of coercing (or casting) n to another type—in this case, Real. Since ↑n and x are both real numbers, Lean can use the multiplication operation on the real numbers to multiply them. (To type ↑ in VSCode, type \\uparrow, or just \\u.)\nAs we will see, the need for coercion in this example will make the proof a bit more complicated, because we’ll need to use some theorems about coercions. Theorems about coercion of natural numbers to some other type often have names that start Nat.cast.\nContinuing with the proof, since exponentiation is defined recursively, let’s try mathematical induction:\n\n\ntheorem Example_6_3_4 : ∀ (x : Real), x > -1 →\n    ∀ (n : Nat), (1 + x) ^ n ≥ 1 + n * x := by\n  fix x : Real\n  assume h1 : x > -1\n  by_induc\n  · -- Base Case\n\n    **done::\n  · -- Induction Step\n\n    **done::  \n  done\n\n\ncase Base_Case\nx : ℝ\nh1 : x > -1\n⊢ (1 + x) ^ 0 ≥\n>>  1 + ↑0 * x\n\n\nYou might think that linarith could prove the goal for the base case, but it can’t. The problem is the ↑0, which denotes the result of coercing the natural number 0 to a real number. Of course, that should be the real number 0, but is it? Yes, but the linarith tactic doesn’t know that. The theorem Nat.cast_zero says that ↑0 = 0 (where the 0 on the right side of the equation is the real number 0), so the tactic rewrite [Nat.cast_zero] will convert ↑0 to 0. After that step, linarith can complete the proof of the base case, and we can start on the induction step.\n\n\ntheorem Example_6_3_4 : ∀ (x : Real), x > -1 →\n    ∀ (n : Nat), (1 + x) ^ n ≥ 1 + n * x := by\n  fix x : Real\n  assume h1 : x > -1\n  by_induc\n  · -- Base Case\n    rewrite [Nat.cast_zero]\n    linarith\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : (1 + x) ^ n ≥ 1 + n * x\n    **done::\n  done\n\n\ncase Induction_Step\nx : ℝ\nh1 : x > -1\nn : ℕ\nih : (1 + x) ^ n ≥\n>>    1 + ↑n * x\n⊢ (1 + x) ^ (n + 1) ≥\n>>  1 + ↑(n + 1) * x\n\n\nOnce again, there’s a complication caused by coercion. The inductive hypothesis talks about ↑n, but the goal involves ↑(n + 1). What is the relationship between these? Surely it should be the case that ↑(n + 1) = ↑n + 1; that is, the result of coercing the natural number n + 1 to a real number should be one larger than the result of coercing n to a real number. The theorem Nat.cast_succ says exactly that, so rewrite [Nat.cast_succ] will change the ↑(n + 1) in the goal to ↑n + 1. (The number n + 1 is sometimes called the successor of n, and succ is short for “successor.”) With that change, we can continue with the proof. The following proof is modeled on the proof in HTPI.\ntheorem ??Example_6_3_4:: : ∀ (x : Real), x > -1 →\n    ∀ (n : Nat), (1 + x) ^ n ≥ 1 + n * x := by\n  fix x : Real\n  assume h1 : x > -1\n  by_induc\n  · -- Base Case\n    rewrite [Nat.cast_zero]\n    linarith\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : (1 + x) ^ n ≥ 1 + n * x\n    rewrite [Nat.cast_succ]\n    show (1 + x) ^ (n + 1) ≥ 1 + (n + 1) * x from\n      calc (1 + x) ^ (n + 1)\n        _ = (1 + x) ^ n * (1 + x) := by rfl\n        _ ≥ (1 + n * x) * (1 + x) := sorry\n        _ = 1 + n * x + x + n * x ^ 2 := by ring\n        _ ≥ 1 + n * x + x + 0 := sorry\n        _ = 1 + (n + 1) * x := by ring\n    done\n  done\nNote that in the calculational proof, each n or n + 1 that is multiplied by x is really ↑n or ↑n + 1, but we don’t need to say so explicitly; Lean fills in coercions automatically when they are required.\nAll that’s left is to replace the two occurrences of sorry with justifications. The first sorry step should follow from the inductive hypothesis by multiplying both sides by 1 + x, so a natural attempt to justify it would be by rel [ih]. Unfortunately, we get an error message saying that rel failed. The error message tells us that rel needed to know that 0 ≤ 1 + x, and it was unable to prove it, so we’ll have to provide a proof of that statement ourselves. Fortunately, linarith can handle it (deducing it from h1 : x > -1), and once we fill in that additional step, the rel tactic succeeds.\ntheorem ??Example_6_3_4:: : ∀ (x : Real), x > -1 →\n    ∀ (n : Nat), (1 + x) ^ n ≥ 1 + n * x := by\n  fix x : Real\n  assume h1 : x > -1\n  by_induc\n  · -- Base Case\n    rewrite [Nat.cast_zero]\n    linarith\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : (1 + x) ^ n ≥ 1 + n * x\n    rewrite [Nat.cast_succ]\n    have h2 : 0 ≤ 1 + x := by linarith\n    show (1 + x) ^ (n + 1) ≥ 1 + (n + 1) * x from\n      calc (1 + x) ^ (n + 1)\n        _ = (1 + x) ^ n * (1 + x) := by rfl\n        _ ≥ (1 + n * x) * (1 + x) := by rel [ih]\n        _ = 1 + n * x + x + n * x ^ 2 := by ring\n        _ ≥ 1 + n * x + x + 0 := sorry\n        _ = 1 + (n + 1) * x := by ring\n    done\n  done\nFor the second sorry step, we’ll need to know that n * x ^ 2 ≥ 0. To prove it, we start with the fact that the square of any real number is nonnegative:\n\n@sq_nonneg : ∀ {α : Type u_1} [inst : LinearOrderedSemiring α]\n              [inst_1 : ExistsAddOfLE α]\n              (a : α), 0 ≤ a ^ 2\n\nAs usual, we don’t need to pay much attention to the implicit arguments; what is important is the last line, which tells us that sq_nonneg x is a proof of x ^ 2 ≥ 0. To get n * x ^ 2 ≥ 0 we just have to multiply both sides by n, which we can justify with the rel tactic, and then one more application of rel will handle the remaining sorry. Here is the complete proof:\ntheorem Example_6_3_4 : ∀ (x : Real), x > -1 →\n    ∀ (n : Nat), (1 + x) ^ n ≥ 1 + n * x := by\n  fix x : Real\n  assume h1 : x > -1\n  by_induc\n  · -- Base Case\n    rewrite [Nat.cast_zero]\n    linarith\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : (1 + x) ^ n ≥ 1 + n * x\n    rewrite [Nat.cast_succ]\n    have h2 : 0 ≤ 1 + x := by linarith\n    have h3 : x ^ 2 ≥ 0 := sq_nonneg x\n    have h4 : n * x ^ 2 ≥ 0 :=\n      calc n * x ^ 2\n        _ ≥ n * 0 := by rel [h3]\n        _ = 0 := by ring\n    show (1 + x) ^ (n + 1) ≥ 1 + (n + 1) * x from\n      calc (1 + x) ^ (n + 1)\n        _ = (1 + x) ^ n * (1 + x) := by rfl\n        _ ≥ (1 + n * x) * (1 + x) := by rel [ih]\n        _ = 1 + n * x + x + n * x ^ 2 := by ring\n        _ ≥ 1 + n * x + x + 0 := by rel [h4]\n        _ = 1 + (n + 1) * x := by ring\n    done\n  done\nBefore ending this section, we’ll return to a topic left unexplained before. We can now describe how Sum i from k to n, f i is defined. The key is a function sum_seq, which is defined by recursion:\ndef sum_seq {A : Type} [AddZeroClass A]\n    (m k : Nat) (f : Nat → A) : A :=\n  match m with\n    | 0 => 0\n    | n + 1 => sum_seq n k f + f (k + n)\nTo get an idea of what this definition means, let’s try evaluating sum_seq 3 k f:\n\nsum_seq 3 k f = sum_seq 2 k f + f (k + 2)\n              = sum_seq 1 k f + f (k + 1) + f (k + 2)\n              = sum_seq 0 k f + f (k + 0) + f (k + 1) + f (k + 2)\n              = 0 + f (k + 0) + f (k + 1) + f (k + 2)\n              = f k + f (k + 1) + f (k + 2).\n\nSo sum_seq 3 k f adds up three consecutive values of f, starting with f k. More generally, sum_seq n k f adds up a sequence of n consecutive values of f, starting with f k. (The implicit arguments say that the type of the values of f can be any type for which + and 0 make sense.) The notation Sum i from k to n, f i is now defined to be a shorthand for sum_seq (n + 1 - k) k f. We’ll leave it to you to puzzle out why that gives the desired result.\n\nExercises\n\ntheorem Exercise_6_3_4 : ∀ (n : Nat),\n    3 * (Sum i from 0 to n, (2 * i + 1) ^ 2) =\n    (n + 1) * (2 * n + 1) * (2 * n + 3) := sorry\n\n\ntheorem Exercise_6_3_7b (f : Nat → Real) (c : Real) : ∀ (n : Nat),\n    Sum i from 0 to n, c * f i = c * Sum i from 0 to n, f i := sorry\n\n\ntheorem fact_pos : ∀ (n : Nat), fact n ≥ 1 := sorry\n\n\n--Hint:  Use the theorem fact_pos from the previous exercise.\ntheorem Exercise_6_3_13a (k : Nat) : ∀ (n : Nat),\n    fact (k ^ 2 + n) ≥ k ^ (2 * n) := sorry\n\n\n--Hint:  Use the theorem in the previous exercise.\n--You may find it useful to first prove a lemma:\n--∀ (k : Nat), 2 * k ^ 2 + 1 ≥ k\ntheorem Exercise_6_3_13b (k : Nat) : ∀ n ≥ 2 * k ^ 2,\n    fact n ≥ k ^ n := sorry\n\n6. A sequence is defined recursively as follows:\ndef seq_6_3_15 (k : Nat) : Int :=\n  match k with\n    | 0 => 0\n    | n + 1 => 2 * seq_6_3_15 n + n\nProve the following theorem about this sequence:\ntheorem Exercise_6_3_15 : ∀ (n : Nat),\n    seq_6_3_15 n = 2 ^ n - n - 1 := sorry\n7. A sequence is defined recursively as follows:\ndef seq_6_3_16 (k : Nat) : Nat :=\n  match k with\n    | 0 => 2\n    | n + 1 => (seq_6_3_16 n) ^ 2\nFind a formula for seq_6_3_16 n. Fill in the blank in the theorem below with your formula and then prove the theorem.\ntheorem Exercise_6_3_16 : ∀ (n : Nat),\n    seq_6_3_16 n = ___ := sorry"
  },
  {
    "objectID": "Chap6.html#strong-induction",
    "href": "Chap6.html#strong-induction",
    "title": "6  Mathematical Induction",
    "section": "6.4. Strong Induction",
    "text": "6.4. Strong Induction\nIn the induction step of a proof by mathematical induction, we prove that a natural number has some property from the assumption that the previous number has the property. Section 6.4 of HTPI introduces a version of mathematical induction in which we get to assume that all smaller numbers have the property. Since this is a stronger assumption, this version of induction is called strong induction. Here is how strong induction works (HTPI p. 304):\n\nTo prove a goal of the form ∀ (n : Nat), P n:\n\nProve that ∀ (n : Nat), (∀ n_1 < n, P n_1) → P n.\n\nTo write a proof by strong induction in Lean, we use the tactic by_strong_induc, whose effect on the tactic state can be illustrated as follows.\n\n\n>> ⋮\n⊢ ∀ (n : Nat), P n\n\n\n>> ⋮\n⊢  ∀ (n : Nat),\n>>  (∀ n_1 < n, P n_1) → P n\n\n\nTo illustrate this, we begin with Example 6.4.1 of HTPI.\ntheorem Example_6_4_1 : ∀ m > 0, ∀ (n : Nat),\n    ∃ (q r : Nat), n = m * q + r ∧ r < m\nImitating the strategy of the proof in HTPI, we let m be an arbitrary natural number, assume m > 0, and then prove the statement ∀ (n : Nat), ∃ (q r : Nat), n = m * q + r ∧ r < m by strong induction. That means that after introducing an arbitrary natural number n, we assume the inductive hypothesis, which says ∀ n_1 < n, ∃ (q r : Nat), n_1 = m * q + r ∧ r < m.\ntheorem Example_6_4_1 : ∀ m > 0, ∀ (n : Nat),\n    ∃ (q r : Nat), n = m * q + r ∧ r < m := by\n  fix m : Nat\n  assume h1 : m > 0\n  by_strong_induc\n  fix n : Nat\n  assume ih : ∀ n_1 < n, ∃ (q r : Nat), n_1 = m * q + r ∧ r < m\n  **done::\nOur goal now is to prove that ∃ (q r : Nat), n = m * q + r ∧ r < m. Although strong induction does not require a base case, it is not uncommon for proofs by strong induction to involve reasoning by cases. The proof in HTPI uses cases based on whether or not n < m. If n < m, then the proof is easy: the numbers q = 0 and r​ = n clearly have the required properties. If ¬n < m, then we can write n as n = k + m, for some natural number k. Since m > 0, we have k < n, so we can apply the inductive hypothesis to k. Notice that if m > 1, then k is not the number immediately preceding n; that’s why this proof uses strong induction rather than ordinary induction.\nHow do we come up with the number k in the previous paragraph? We’ll use a theorem from Lean’s library. There are two slightly different versions of the theorem—notice that the first ends with m + k and the second ends with k + m:\n\n@Nat.exists_eq_add_of_le : ∀ {m n : ℕ}, m ≤ n → ∃ (k : ℕ), n = m + k\n\n@Nat.exists_eq_add_of_le' : ∀ {m n : ℕ}, m ≤ n → ∃ (k : ℕ), n = k + m\n\nWe’ll use the second version in our proof.\ntheorem Example_6_4_1 : ∀ m > 0, ∀ (n : Nat),\n    ∃ (q r : Nat), n = m * q + r ∧ r < m := by\n  fix m : Nat\n  assume h1 : m > 0\n  by_strong_induc\n  fix n : Nat\n  assume ih : ∀ n_1 < n, ∃ (q r : Nat), n_1 = m * q + r ∧ r < m\n  by_cases h2 : n < m\n  · -- Case 1. h2 : n < m\n    apply Exists.intro 0\n    apply Exists.intro n     --Goal : n = m * 0 + n ∧ n < m\n    apply And.intro _ h2\n    ring\n    done\n  · -- Case 2. h2 : ¬n < m\n    have h3 : m ≤ n := by linarith\n    obtain (k : Nat) (h4 : n = k + m) from Nat.exists_eq_add_of_le' h3\n    have h5 : k < n := by linarith\n    have h6 : ∃ (q r : Nat), k = m * q + r ∧ r < m := ih k h5\n    obtain (q' : Nat)\n      (h7 : ∃ (r : Nat), k = m * q' + r ∧ r < m) from h6\n    obtain (r' : Nat) (h8 : k = m * q' + r' ∧ r' < m) from h7\n    apply Exists.intro (q' + 1)\n    apply Exists.intro r'     --Goal : n = m * (q' + 1) + r' ∧ r' < m\n    apply And.intro _ h8.right\n    show n = m * (q' + 1) + r' from\n      calc n\n        _ = k + m := h4\n        _ = m * q' + r' + m := by rw [h8.left]\n        _ = m * (q' + 1) + r' := by ring\n    done\n  done\nThe numbers q and r in Example_6_4_1 are called the quotient and remainder when n is divided by m. Lean knows how to compute these numbers: if n and m are natural numbers, then in Lean, n / m denotes the quotient when n is divided by m, and n % m denotes the remainder. (The number n % m is also sometimes called n modulo m, or n mod m.) And Lean knows theorems stating that these numbers have the properties specified in Example_6_4_1:\n\n@Nat.div_add_mod : ∀ (m n : ℕ), n * (m / n) + m % n = m\n\n@Nat.mod_lt : ∀ (x : ℕ) {y : ℕ}, y > 0 → x % y < y\n\nBy the way, although we are unlikely to want to use the notation n / 0 or n % 0, Lean uses the definitions n / 0 = 0 and n % 0 = n. As a result, the equation n * (m / n) + m % n = m is true even if n = 0. That’s why the theorem Nat.div_add_mod doesn’t include a requirement that n > 0. It is important to keep in mind that division of natural numbers is not the same as division of real numbers. For example, dividing the natural number 5 by the natural number 2 gives a quotient of 2 (with a remainder of 1), so (5 : Nat) / (2 : Nat) is 2, but (5 : Real) / (2 : Real) is 2.5.\nThere is also a strong form of recursion. As an example of this, here is a recursive definition of a sequence of numbers called the Fibonacci numbers:\ndef Fib (n : Nat) : Nat :=\n  match n with\n    | 0 => 0\n    | 1 => 1\n    | k + 2 => Fib k + Fib (k + 1)\nNotice that the formula for Fib (k + 2) involves the two previous values of Fib, not just the immediately preceding value. That is the sense in which the recursion is strong. Not surprisingly, theorems about the Fibonacci numbers are often proven by induction—either ordinary or strong. We’ll illustrate this with a proof by strong induction that ∀ (n : Nat), Fib n < 2 ^ n. This time we’ll need to treat the cases n = 0 and n = 1 separately, since these values are treated separately in the definition of Fib n. And we’ll need to know that if n doesn’t fall into either of those cases, then it falls into the third case: n = k + 2 for some natural number k. Since similar ideas will come up several times in the rest of this book, it will be useful to begin by proving lemmas that will help with this kind of reasoning.\nWe’ll need two theorems from Lean’s library, the second of which has two slightly different versions:\n\n@Nat.pos_of_ne_zero : ∀ {n : ℕ}, n ≠ 0 → 0 < n\n\n@lt_of_le_of_ne : ∀ {α : Type u_1} [inst : PartialOrder α] {a b : α},\n                    a ≤ b → a ≠ b → a < b\n\n@lt_of_le_of_ne' : ∀ {α : Type u_1} [inst : PartialOrder α] {a b : α},\n                    a ≤ b → b ≠ a → a < b\n\nIf we have h1 : n ≠ 0, then Nat.pos_of_ne_zero h1 is a proof of 0 < n. But for natural numbers a and b, Lean treats a < b as meaning the same thing as a + 1 ≤ b, so this is also a proof of 1 ≤ n. If we also have h2 : n ≠ 1, then we can use lt_of_le_of_ne' to conclude 1 < n, which is definitionally equal to 2 ≤ n. Combining this reasoning with the theorem Nat.exists_eq_add_of_le', which we used in the last example, we can prove two lemmas that will be helpful for reasoning in which the first one or two natural numbers have to be treated separately.\nlemma exists_eq_add_one_of_ne_zero {n : Nat}\n    (h1 : n ≠ 0) : ∃ (k : Nat), n = k + 1 := by\n  have h2 : 1 ≤ n := Nat.pos_of_ne_zero h1\n  show ∃ (k : Nat), n = k + 1 from Nat.exists_eq_add_of_le' h2\n  done\n\ntheorem exists_eq_add_two_of_ne_zero_one {n : Nat}\n    (h1 : n ≠ 0) (h2 : n ≠ 1) : ∃ (k : Nat), n = k + 2 := by\n  have h3 : 1 ≤ n := Nat.pos_of_ne_zero h1\n  have h4 : 2 ≤ n := lt_of_le_of_ne' h3 h2\n  show ∃ (k : Nat), n = k + 2 from Nat.exists_eq_add_of_le' h4\n  done\nWith this preparation, we can present the proof:\nexample : ∀ (n : Nat), Fib n < 2 ^ n := by\n  by_strong_induc\n  fix n : Nat\n  assume ih : ∀ n_1 < n, Fib n_1 < 2 ^ n_1\n  by_cases h1 : n = 0\n  · -- Case 1. h1 : n = 0\n    rewrite [h1]   --Goal : Fib 0 < 2 ^ 0\n    decide\n    done\n  · -- Case 2. h1 : ¬n = 0\n    by_cases h2 : n = 1\n    · -- Case 2.1. h2 : n = 1\n      rewrite [h2]\n      decide\n      done\n    · -- Case 2.2. h2 : ¬n = 1\n      obtain (k : Nat) (h3 : n = k + 2) from\n        exists_eq_add_two_of_ne_zero_one h1 h2\n      have h4 : k < n := by linarith\n      have h5 : Fib k < 2 ^ k := ih k h4\n      have h6 : k + 1 < n := by linarith\n      have h7 : Fib (k + 1) < 2 ^ (k + 1) := ih (k + 1) h6\n      rewrite [h3]            --Goal : Fib (k + 2) < 2 ^ (k + 2)\n      show Fib (k + 2) < 2 ^ (k + 2) from\n        calc Fib (k + 2)\n          _ = Fib k + Fib (k + 1) := by rfl\n          _ < 2 ^ k + Fib (k + 1) := by rel [h5]\n          _ < 2 ^ k + 2 ^ (k + 1) := by rel [h7]\n          _ ≤ 2 ^ k + 2 ^ (k + 1) + 2 ^ k := by linarith\n          _ = 2 ^ (k + 2) := by ring\n      done\n    done\n  done\nAs with ordinary induction, strong induction can be useful for proving statements that do not at first seem to have the form ∀ (n : Nat), .... To illustrate this, we’ll prove the well-ordering principle, which says that if a set S : Set Nat is nonempty, then it has a smallest element. We’ll prove the contrapositive: if S has no smallest element, then it is empty. To say that S is empty means ∀ (n : Nat), n ∉ S, and that’s the statement to which we will apply strong induction.\ntheorem well_ord_princ (S : Set Nat) : (∃ (n : Nat), n ∈ S) →\n    ∃ n ∈ S, ∀ m ∈ S, n ≤ m := by\n  contrapos\n  assume h1 : ¬∃ n ∈ S, ∀ m ∈ S, n ≤ m\n  quant_neg                   --Goal : ∀ (n : Nat), n ∉ S\n  by_strong_induc\n  fix n : Nat\n  assume ih : ∀ n_1 < n, n_1 ∉ S  --Goal : n ∉ S\n  contradict h1 with h2       --h2 : n ∈ S\n    --Goal : ∃ n ∈ S, ∀ m ∈ S, n ≤ m\n  apply Exists.intro n        --Goal : n ∈ S ∧ ∀ m ∈ S, n ≤ m\n  apply And.intro h2          --Goal : ∀ m ∈ S, n ≤ m\n  fix m : Nat\n  assume h3 : m ∈ S\n  have h4 : m < n → m ∉ S := ih m\n  contrapos at h4             --h4 : m ∈ S → ¬m < n\n  have h5 : ¬m < n := h4 h3\n  linarith\n  done\nSection 6.4 of HTPI ends with an example of an application of the well-ordering principle. The example gives a proof that \\(\\sqrt{2}\\) is irrational. If \\(\\sqrt{2}\\) were rational, then there would be natural numbers \\(p\\) and \\(q\\) such that \\(q \\ne 0\\) and \\(p/q = \\sqrt{2}\\), and therefore \\(p^2 = 2q^2\\). So we can prove that \\(\\sqrt{2}\\) is irrational by showing that there do not exist natural numbers \\(p\\) and \\(q\\) such that \\(q \\ne 0\\) and \\(p^2 = 2q^2\\).\nThe proof uses a definition from the exercises of Section 6.1:\ndef nat_even (n : Nat) : Prop := ∃ (k : Nat), n = 2 * k\nWe will also use the following lemma, whose proof we leave as an exercise for you:\nlemma sq_even_iff_even (n : Nat) : nat_even (n * n) ↔ nat_even n := sorry\nAnd we’ll need another theorem that we haven’t seen before:\n\n@mul_left_cancel_iff_of_pos : ∀ {α : Type u_1} {a b c : α}\n                    [inst : MulZeroClass α] [inst_1 : PartialOrder α]\n                    [inst_2 : PosMulReflectLE α],\n                    0 < a → (a * b = a * c ↔ b = c)\n\nTo show that \\(\\sqrt{2}\\) is irrational, we will prove the statement\n\n¬∃ (q p : Nat), p * p = 2 * (q * q) ∧ q ≠ 0\n\nWe proceed by contradiction. If this statement were false, then the set\n\nS = {q : Nat | ∃ (p : Nat), p * p = 2 * (q * q) ∧ q ≠ 0}\n\nwould be nonempty, and therefore, by the well-ordering principle, it would have a smallest element. We then show that this leads to a contradiction. Here is the proof.\ntheorem Theorem_6_4_5 :\n    ¬∃ (q p : Nat), p * p = 2 * (q * q) ∧ q ≠ 0 := by\n  set S : Set Nat :=\n    {q : Nat | ∃ (p : Nat), p * p = 2 * (q * q) ∧ q ≠ 0}\n  by_contra h1\n  have h2 : ∃ (q : Nat), q ∈ S := h1\n  have h3 : ∃ q ∈ S, ∀ r ∈ S, q ≤ r := well_ord_princ S h2\n  obtain (q : Nat) (h4 : q ∈ S ∧ ∀ r ∈ S, q ≤ r) from h3\n  have qinS : q ∈ S := h4.left\n  have qleast : ∀ r ∈ S, q ≤ r := h4.right\n  define at qinS     --qinS : ∃ (p : Nat), p * p = 2 * (q * q) ∧ q ≠ 0\n  obtain (p : Nat) (h5 : p * p = 2 * (q * q) ∧ q ≠ 0) from qinS\n  have pqsqrt2 : p * p = 2 * (q * q) := h5.left\n  have qne0 : q ≠ 0 := h5.right\n  have h6 : nat_even (p * p) := Exists.intro (q * q) pqsqrt2\n  rewrite [sq_even_iff_even p] at h6    --h6 : nat_even p\n  obtain (p' : Nat) (p'halfp : p = 2 * p') from h6\n  have h7 : 2 * (2 * (p' * p')) = 2 * (q * q) := by\n    rewrite [←pqsqrt2, p'halfp]\n    ring\n    done\n  have h8 : 2 > 0 := by decide\n  rewrite [mul_left_cancel_iff_of_pos h8] at h7\n    --h7 : 2 * (p' * p') = q * q\n  have h9 : nat_even (q * q) := Exists.intro (p' * p') h7.symm\n  rewrite [sq_even_iff_even q] at h9   --h9 : nat_even q\n  obtain (q' : Nat) (q'halfq : q = 2 * q') from h9\n  have h10 : 2 * (p' * p') = 2 * (2 * (q' * q')) := by\n    rewrite [h7, q'halfq]\n    ring\n    done\n  rewrite [mul_left_cancel_iff_of_pos h8] at h10\n    --h10 : p' * p' = 2 * (q' * q')\n  have q'ne0 : q' ≠ 0 := by\n    contradict qne0 with h11\n    rewrite [q'halfq, h11]\n    rfl\n    done\n  have q'inS : q' ∈ S := Exists.intro p' (And.intro h10 q'ne0)\n  have qleq' : q ≤ q' := qleast q' q'inS\n  rewrite [q'halfq] at qleq'        --qleq' : 2 * q' ≤ q'\n  contradict q'ne0\n  linarith\n  done\n\n\nExercises\n\n--Hint: Use Exercise_6_1_16a1 and Exercise_6_1_16a2.\n--from the exercises of Section 6.1.\nlemma sq_even_iff_even (n : Nat) :\n    nat_even (n * n) ↔ nat_even n := sorry\n\n\n--This theorem proves that the square root of 6 is irrational\ntheorem Exercise_6_4_4a :\n    ¬∃ (q p : Nat), p * p = 6 * (q * q) ∧ q ≠ 0 := sorry\n\n\ntheorem Exercise_6_4_5 :\n    ∀ n ≥ 12, ∃ (a b : Nat), 3 * a + 7 * b = n := sorry\n\n\ntheorem Exercise_6_4_7a : ∀ (n : Nat),\n    (Sum i from 0 to n, Fib i) + 1 = Fib (n + 2) := sorry\n\n\ntheorem Exercise_6_4_7c : ∀ (n : Nat),\n    Sum i from 0 to n, Fib (2 * i + 1) = Fib (2 * n + 2) := sorry\n\n\ntheorem Exercise_6_4_8a : ∀ (m n : Nat),\n    Fib (m + n + 1) = Fib m * Fib n + Fib (m + 1) * Fib (n + 1) := sorry\n\n\ntheorem Exercise_6_4_8d : ∀ (m k : Nat), Fib m ∣ Fib (m * k) := sorry\nHint for #7: Let m be an arbitrary natural number, and then use induction on k. For the induction step, you must prove Fib m ∣ Fib (m * (k + 1)). If m = 0 ∨ k = 0, then this is easy. If not, then use exists_eq_add_one_of_ne_zero to obtain a natural number j such that m * k = j + 1, and therefore m * (k + 1) = j + m + 1, and then apply Exercise_6_4_8a.\n\n\ndef Fib_like (n : Nat) : Nat :=\n  match n with\n    | 0 => 1\n    | 1 => 2\n    | k + 2 => 2 * (Fib_like k) + Fib_like (k + 1)\n\ntheorem Fib_like_formula : ∀ (n : Nat), Fib_like n = 2 ^ n := sorry\n\n\ndef triple_rec (n : Nat) : Nat :=\n  match n with\n    | 0 => 0\n    | 1 => 2\n    | 2 => 4\n    | k + 3 => 4 * triple_rec k +\n                6 * triple_rec (k + 1) + triple_rec (k + 2)\n\ntheorem triple_rec_formula :\n    ∀ (n : Nat), triple_rec n = 2 ^ n * Fib n := sorry\n\n10. In this exercise you will prove that the numbers q and r in Example_6_4_1 are unique. It is helpful to prove a lemma first.\nlemma quot_rem_unique_lemma {m q r q' r' : Nat}\n    (h1 : m * q + r = m * q' + r') (h2 : r' < m) : q ≤ q' := sorry\n\ntheorem quot_rem_unique (m q r q' r' : Nat)\n    (h1 : m * q + r = m * q' + r') (h2 : r < m) (h3 : r' < m) :\n    q = q' ∧ r = r' := sorry\n11. Use the theorem in the previous exercise to prove the following characterization of n / m and n % m.\ntheorem div_mod_char (m n q r : Nat)\n    (h1 : n = m * q + r) (h2 : r < m) : q = n / m ∧ r = n % m := sorry"
  },
  {
    "objectID": "Chap6.html#closures-again",
    "href": "Chap6.html#closures-again",
    "title": "6  Mathematical Induction",
    "section": "6.5. Closures Again",
    "text": "6.5. Closures Again\nSection 6.5 of HTPI gives one more application of recursion and induction: another proof of the existence of closures of sets under functions. Recall from Section 5.4 that if f : A → A and B : Set A, then the closure of B under f is the smallest set containing B that is closed under f. In Section 5.4, we constructed the closure of B under f by taking the intersection of all sets containing B that are closed under f. In this section, we construct the closure by starting with the set B and repeatedly taking the image under f. For the motivation for this strategy, see HTPI; here we focus on how to carry out this strategy in Lean.\nTo talk about repeatedly taking the image of a set under a function, we will need a recursive definition:\ndef rep_image {A : Type} (f : A → A) (n : Nat) (B : Set A) : Set A :=\n  match n with\n    | 0 => B\n    | k + 1 => image f (rep_image f k B)\nAccording to this definition, rep_image f 0 B = B, rep_image f 1 B = image f B, rep_image f 2 B = image f (image f B), and so on. In other words, rep_image f n B is the result of starting with B and then taking the image under f n times. To make it easier to work with this definition, we state two simple theorems, both of which follow immediately from the definition.\ntheorem rep_image_base {A : Type} (f : A → A) (B : Set A) :\n    rep_image f 0 B = B := by rfl\n\ntheorem rep_image_step {A : Type} (f : A → A) (n : Nat) (B : Set A) :\n    rep_image f (n + 1) B = image f (rep_image f n B) := by rfl\nWe will prove that the closure of B under f is the union of the sets rep_image f n B. We will call this the cumulative image of B under f, and we define it as follows:\ndef cumul_image {A : Type} (f : A → A) (B : Set A) : Set A :=\n  {x : A | ∃ (n : Nat), x ∈ rep_image f n B}\nTo prove that cumul_image f B is the closure of B under f, we first prove a lemma saying that if B ⊆ D and D is closed under f, then for every natural number n, rep_image f n B ⊆ D. We prove it by induction.\nlemma rep_image_sub_closed {A : Type} {f : A → A} {B D : Set A}\n    (h1 : B ⊆ D) (h2 : closed f D) :\n    ∀ (n : Nat), rep_image f n B ⊆ D := by\n  by_induc\n  · -- Base Case\n    rewrite [rep_image_base]          --Goal : B ⊆ D\n    show B ⊆ D from h1\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : rep_image f n B ⊆ D   --Goal : rep_image f (n + 1) B ⊆ D\n    fix x : A\n    assume h3 : x ∈ rep_image f (n + 1) B  --Goal : x ∈ D\n    rewrite [rep_image_step] at h3\n    define at h3    --h3 : ∃ x_1 ∈ rep_image f n B, f x_1 = x\n    obtain (b : A) (h4 : b ∈ rep_image f n B ∧ f b = x) from h3\n    rewrite [←h4.right]   --Goal : f b ∈ D    \n    have h5 : b ∈ D := ih h4.left\n    define at h2          --h2 : ∀ x ∈ D, f x ∈ D\n    show f b ∈ D from h2 b h5\n    done\n  done\nWith this preparation, we can now prove that cumul_image f B is the closure of B under f.\ntheorem Theorem_6_5_1 {A : Type} (f : A → A) (B : Set A) :\n    closure f B (cumul_image f B) := by\n  define\n  apply And.intro\n  · -- Proof that cumul_image f B ∈ {D : Set A | B ⊆ D ∧ closed f D}\n    define  --Goal : B ⊆ cumul_image f B ∧ closed f (cumul_image f B)\n    apply And.intro\n    · -- Proof that B ⊆ cumul_image f B\n      fix x : A\n      assume h1 : x ∈ B\n      define     --Goal : ∃ (n : Nat), x ∈ rep_image f n B\n      apply Exists.intro 0\n      rewrite [rep_image_base]  --Goal : x ∈ B\n      show x ∈ B from h1\n      done\n    · -- Proof that cumul_image f B closed under f\n      define\n      fix x : A\n      assume h1 : x ∈ cumul_image f B  --Goal : f x ∈ cumul_image f B\n      define at h1\n      obtain (m : Nat) (h2 : x ∈ rep_image f m B) from h1\n      define     --Goal : ∃ (n : Nat), f x ∈ rep_image f n B\n      apply Exists.intro (m + 1) --Goal : f x ∈ rep_image f (m + 1) B\n      rewrite [rep_image_step]   --Goal : f x ∈ image f (rep_image f m B)\n      define     --Goal : ∃ x_1 ∈ rep_image f m B, f x_1 = f x\n      apply Exists.intro x  --Goal : x ∈ rep_image f m B ∧ f x = f x\n      apply And.intro h2\n      rfl\n      done\n    done\n  · -- Proof that cumul_image f B is smallest\n    fix D : Set A\n    assume h1 : D ∈ {D : Set A | B ⊆ D ∧ closed f D}\n    define at h1  --h1 : B ⊆ D ∧ closed f D\n    define   --Goal : ∀ ⦃a : A⦄, a ∈ cumul_image f B → a ∈ D\n    fix x : A\n    assume h2 : x ∈ cumul_image f B  --Goal : x ∈ D\n    define at h2  --h2: ∃ (n : Nat), x ∈ rep_image f n B\n    obtain (m : Nat) (h3 : x ∈ rep_image f m B) from h2\n    have h4 : rep_image f m B ⊆ D :=\n      rep_image_sub_closed h1.left h1.right m\n    show x ∈ D from h4 h3\n    done\n  done\n\nExercises\n1. Recall the following definitions from the exercises of Section 5.4:\ndef closed_family {A : Type} (F : Set (A → A)) (C : Set A) : Prop :=\n  ∀ f ∈ F, closed f C\n\ndef closure_family {A : Type} (F : Set (A → A)) (B C : Set A) : Prop :=\n  smallestElt (sub A) C {D : Set A | B ⊆ D ∧ closed_family F D}\nThese definitions say that a set is closed under a family of functions if it is closed under all of the functions in the family, and the closure of a set B under a family of functions is the smallest set containing B that is closed under the family.\nIn this exercise we will use the following additional definitions:\ndef rep_image_family {A : Type}\n    (F : Set (A → A)) (n : Nat) (B : Set A) : Set A :=\n  match n with\n    | 0 => B\n    | k + 1 => {x : A | ∃ f ∈ F, x ∈ image f (rep_image_family F k B)}\n\ndef cumul_image_family {A : Type}\n    (F : Set (A → A)) (B : Set A) : Set A :=\n  {x : A | ∃ (n : Nat), x ∈ rep_image_family F n B}\nThe following theorems establish that if F : Set (A → A) and B : Set A, then cumul_image_family F B is the closure of B under F. The first two are proven by rfl; the other two are for you to prove.\ntheorem rep_image_family_base {A : Type}\n    (F : Set (A → A)) (B : Set A) : rep_image_family F 0 B = B := by rfl\n\ntheorem rep_image_family_step {A : Type}\n    (F : Set (A → A)) (n : Nat) (B : Set A) :\n    rep_image_family F (n + 1) B =\n    {x : A | ∃ f ∈ F, x ∈ image f (rep_image_family F n B)} := by rfl\n\nlemma rep_image_family_sub_closed {A : Type}\n    (F : Set (A → A)) (B D : Set A)\n    (h1 : B ⊆ D) (h2 : closed_family F D) :\n    ∀ (n : Nat), rep_image_family F n B ⊆ D := sorry\n\ntheorem Exercise_6_5_3 {A : Type} (F : Set (A → A)) (B : Set A) :\n    closure_family F B (cumul_image_family F B) := sorry\n\n\n\nThe next two exercises concern the following two definitions from Section 5.4:\ndef closed2 {A : Type} (f : A → A → A) (C : Set A) : Prop :=\n  ∀ x ∈ C, ∀ y ∈ C, f x y ∈ C\n\ndef closure2 {A : Type} (f : A → A → A) (B C : Set A) : Prop := \n  smallestElt (sub A) C {D : Set A | B ⊆ D ∧ closed2 f D}\nThey also use the following definition, which extends the idea of the image of a set under a function to functions of two variables:\ndef image2 {A : Type} (f : A → A → A) (B : Set A) : Set A :=\n  {z : A | ∃ (x y : A), x ∈ B ∧ y ∈ B ∧ z = f x y}\n2. A natural way to try to find the closure of a set under a function of two variables would be to use the following definitions and theorems:\ndef rep_image2 {A : Type}\n    (f : A → A → A) (n : Nat) (B : Set A) : Set A :=\n  match n with\n    | 0 => B\n    | k + 1 => image2 f (rep_image2 f k B)\n\ntheorem rep_image2_base {A : Type} (f : A → A → A) (B : Set A) :\n    rep_image2 f 0 B = B := by rfl\n\ntheorem rep_image2_step {A : Type}\n    (f : A → A → A) (n : Nat) (B : Set A) :\n    rep_image2 f (n + 1) B = image2 f (rep_image2 f n B) := by rfl\n\ndef cumul_image2 {A : Type} (f : A → A → A) (B : Set A) : Set A :=\n  {x : A | ∃ (n : Nat), x ∈ rep_image2 f n B}\nWe could now try to prove that if f : A → A → A and B : Set A, then cumul_image2 f B is the closure of B under f. However, this approach doesn’t work, because cumul_image2 f B might not be closed under f.\nHere is an incorrect informal argument that cumul_image2 f B is closed under f. Suppose x and y are elements of cumul_image2 f B. This means that we can choose some natural number n such that x ∈ rep_image2 f n B and y ∈ rep_image2 f n B. This implies that f x y ∈ image2 f (rep_image2 f n B) = rep_image2 f (n + 1) B, so f x y ∈ cumul_image2 f B.\nFind the mistake in this informal argument by trying to turn it into a proof in Lean:\n--You won't be able to complete this proof\ntheorem Exercise_6_5_6 {A : Type} (f : A → A → A) (B : Set A) :\n    closed2 f (cumul_image2 f B) := sorry\n3. In this exercise, we fix the mistake in the attempted proof in the previous exercise. Instead of repeatedly taking the image of a set, we repeatedly take the union of a set with its image:\ndef un_image2 {A : Type} (f : A → A → A) (B : Set A) : Set A :=\n  B ∪ (image2 f B)\n\ndef rep_un_image2 {A : Type}\n    (f : A → A → A) (n : Nat) (B : Set A) : Set A :=\n  match n with\n    | 0 => B\n    | k + 1 => un_image2 f (rep_un_image2 f k B)\n\ntheorem rep_un_image2_base {A : Type} (f : A → A → A) (B : Set A) :\n    rep_un_image2 f 0 B = B := by rfl\n\ntheorem rep_un_image2_step {A : Type}\n    (f : A → A → A) (n : Nat) (B : Set A) :\n    rep_un_image2 f (n + 1) B =\n    un_image2 f (rep_un_image2 f n B) := by rfl\n\ndef cumul_un_image2 {A : Type}\n    (f : A → A → A) (B : Set A) : Set A :=\n  {x : A | ∃ (n : Nat), x ∈ rep_un_image2 f n B}\nNow prove that if f : A → A → A and B : Set A, then cumul_un_image2 f B is the closure of B under f by completing the following proofs:\ntheorem Exercise_6_5_8a {A : Type} (f : A → A → A) (B : Set A) :\n    ∀ (m n : Nat), m ≤ n →\n    rep_un_image2 f m B ⊆ rep_un_image2 f n B := sorry\n\nlemma rep_un_image2_sub_closed {A : Type} {f : A → A → A} {B D : Set A}\n    (h1 : B ⊆ D) (h2 : closed2 f D) :\n    ∀ (n : Nat), rep_un_image2 f n B ⊆ D := sorry\n\nlemma closed_lemma\n    {A : Type} {f : A → A → A} {B : Set A} {x y : A} {nx ny n : Nat}\n    (h1 : x ∈ rep_un_image2 f nx B) (h2 : y ∈ rep_un_image2 f ny B)\n    (h3 : nx ≤ n) (h4 : ny ≤ n) :\n    f x y ∈ cumul_un_image2 f B := sorry\n\ntheorem Exercise_6_5_8b {A : Type} (f : A → A → A) (B : Set A) :\n    closure2 f B (cumul_un_image2 f B) := sorry\n\n\n\nThe remaining exercises in this section use the following definitions:\ndef idExt (A : Type) : Set (A × A) := {(x, y) : A × A | x = y}\n\ndef rep_comp {A : Type} (R : Set (A × A)) (n : Nat) : Set (A × A) :=\n  match n with\n    | 0 => idExt A\n    | k + 1 => comp (rep_comp R k) R\n\ndef cumul_comp {A : Type} (R : Set (A × A)) : Set (A × A) :=\n  {(x, y) : A × A | ∃ n ≥ 1, (x, y) ∈ rep_comp R n}\n\ntheorem rep_comp_one {A : Type} (R : Set (A × A)) :\n    rep_comp R 1 = R := sorry\n\n\ntheorem Exercise_6_5_11 {A : Type} (R : Set (A × A)) :\n    ∀ (m n : Nat), rep_comp R (m + n) =\n    comp (rep_comp R m) (rep_comp R n) := sorry\n\n\nlemma rep_comp_sub_trans {A : Type} {R S : Set (A × A)}\n    (h1 : R ⊆ S) (h2 : transitive (RelFromExt S)) :\n    ∀ n ≥ 1, rep_comp R n ⊆ S := sorry\n\n\ntheorem Exercise_6_5_14 {A : Type} (R : Set (A × A)) :\n    smallestElt (sub (A × A)) (cumul_comp R)\n    {S : Set (A × A) | R ⊆ S ∧ transitive (RelFromExt S)} := sorry"
  },
  {
    "objectID": "Chap7.html",
    "href": "Chap7.html",
    "title": "7  Number Theory",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\mathbin{∆}}\n$$"
  },
  {
    "objectID": "Chap7.html#greatest-common-divisors",
    "href": "Chap7.html#greatest-common-divisors",
    "title": "7  Number Theory",
    "section": "7.1. Greatest Common Divisors",
    "text": "7.1. Greatest Common Divisors\nThe proofs in this chapter and the next are significantly longer than those in previous chapters. As a result, we will skip some details in the text, leaving proofs of a number of theorems as exercises for you. The most interesting of these exercises are included in the exercise lists at the ends of the sections; for the rest, you can compare your solutions to proofs that can be found in the Lean package that accompanies this book. Also, we will occasionally use theorems that we have not used before without explanation. If necessary, you can use #check to look up what they say.\nSection 7.1 of HTPI introduces the Euclidean algorithm for computing the greatest common divisor (gcd) of two positive integers \\(a\\) and \\(b\\). The motivation for the algorithm is the fact that if \\(r\\) is the remainder when \\(a\\) is divided by \\(b\\), then any natural number that divides both \\(a\\) and \\(b\\) also divides \\(r\\), and any natural number that divides both \\(b\\) and \\(r\\) also divides \\(a\\).\nLet’s prove these statements in Lean. Recall that in Lean, the remainder when a is divided by b is called a mod b, and it is denoted a % b. We’ll prove the first statement, and leave the second as an exercise for you. It will be convenient for our work with greatest common divisors in Lean to let a and b be natural numbers rather than positive integers (thus allowing either of them to be zero).\ntheorem dvd_mod_of_dvd_a_b {a b d : Nat}\n    (h1 : d ∣ a) (h2 : d ∣ b) : d ∣ (a % b) := by\n  set q : Nat := a / b\n  have h3 : b * q + a % b = a := Nat.div_add_mod a b\n  obtain (j : Nat) (h4 : a = d * j) from h1\n  obtain (k : Nat) (h5 : b = d * k) from h2\n  define    --Goal : ∃ (c : Nat), a % b = d * c\n  apply Exists.intro (j - k * q)\n  show a % b = d * (j - k * q) from\n    calc a % b\n      _ = b * q + a % b - b * q := (Nat.add_sub_cancel_left _ _).symm\n      _ = a - b * q := by rw [h3]\n      _ = d * j - d * (k * q) := by rw [h4, h5, mul_assoc]\n      _ = d * (j - k * q) := (Nat.mul_sub_left_distrib _ _ _).symm\n  done\n\ntheorem dvd_a_of_dvd_b_mod {a b d : Nat}\n    (h1 : d ∣ b) (h2 : d ∣ (a % b)) : d ∣ a := sorry\nThese theorems tell us that the gcd of a and b is the same as the gcd of b and a % b, which suggests that the following recursive definition should compute the gcd of a and b:\ndef **gcd:: (a b : Nat) : Nat :=\n  match b with\n    | 0 => a\n    | n + 1 => gcd (n + 1) (a % (n + 1))\nUnfortunately, Lean puts a red squiggle under gcd, and it displays in the Infoview a long error message that begins fail to show termination. What is Lean complaining about?\nThe problem is that recursive definitions are dangerous. To understand the danger, consider the following recursive definition:\ndef loop (n : Nat) : Nat := loop (n + 1)\nSuppose we try to use this definition to compute loop 3. The definition would lead us to perform the following calculation:\n\nloop 3 = loop 4 = loop 5 = loop 6 = ...\n\nClearly this calculation will go on forever and will never produce an answer. So the definition of loop does not actually succeed in defining a function from Nat to Nat.\nLean insists that recursive definitions must avoid such nonterminating calculations. Why did it accept all of our previous recursive definitions? The reason is that in each case, the definition of the value of the function at a natural number n referred only to values of the function at numbers smaller than n. Since a decreasing list of natural numbers cannot go on forever, such definitions lead to calculations that are guaranteed to terminate.\nWhat about our recursive definition of gcd a b? This function has two arguments, a and b, and when b = n + 1, the definition asks us to compute gcd (n + 1) (a % (n + 1)). The first argument here could actually be larger than the first argument in the value we are trying to compute, gcd a b. But the second argument will always be smaller, and that will suffice to guarantee that the calculation terminates. We can tell Lean to focus on the second argument b by adding a termination_by clause to the end of our recursive definition:\ndef gcd (a b : Nat) : Nat :=\n  match b with\n    | 0 => a\n    | n + 1 => **gcd (n + 1) (a % (n + 1))::\n  termination_by b\nUnfortunately, Lean still isn’t satisfied, but the error message this time is more helpful. The message says that Lean failed to prove termination, and at the end of the message it says that the goal it failed to prove is a % (n + 1) < n + 1, which is precisely what is needed to show that the second argument of gcd (n + 1) (a % (n + 1)) is smaller than the second argument of gcd a b when b = n + 1. We’ll need to provide a proof of this goal to convince Lean to accept our recursive definition. Fortunately, it’s not hard to prove:\nlemma mod_succ_lt (a n : Nat) : a % (n + 1) < n + 1 := by\n  have h : n + 1 > 0 := Nat.succ_pos n\n  show a % (n + 1) < n + 1 from Nat.mod_lt a h\n  done\nLean’s error message suggests several ways to fix the problem with our recursive definition. We’ll use the first suggestion: Use `have`-expressions to prove the remaining goals. Here, finally, is the definition of gcd that Lean is willing to accept. (You can ignore the initial line @[semireducible]. For technical reasons that we won’t go into, this line is needed to make this complicated recursive definition work in proofs the same way that our previous, simpler recursive definitions did.)\n@[semireducible]\ndef gcd (a b : Nat) : Nat :=\n  match b with\n    | 0 => a\n    | n + 1 =>\n      have : a % (n + 1) < n + 1 := mod_succ_lt a n\n      gcd (n + 1) (a % (n + 1))\n  termination_by b\nNotice that in the have expression, we have not bothered to specify an identifier for the assertion being proven, since we never need to refer to it. Let’s try out our gcd function:\n++#eval:: gcd 672 161    --Answer: 7.  Note 672 = 7 * 96 and 161 = 7 * 23.\nTo establish the main properties of gcd a b we’ll need several lemmas. We prove some of them and leave others as exercises.\nlemma gcd_base (a : Nat) : gcd a 0 = a := by rfl\n\nlemma gcd_nonzero (a : Nat) {b : Nat} (h : b ≠ 0) :\n    gcd a b = gcd b (a % b) := by\n  obtain (n : Nat) (h2 : b = n + 1) from exists_eq_add_one_of_ne_zero h\n  rewrite [h2]   --Goal : gcd a (n + 1) = gcd (n + 1) (a % (n + 1))\n  rfl\n  done\n\nlemma mod_nonzero_lt (a : Nat) {b : Nat} (h : b ≠ 0) : a % b < b := sorry\n\nlemma dvd_self (n : Nat) : n ∣ n := sorry\nOne of the most important properties of gcd a b is that it divides both a and b. We prove it by strong induction on b.\ntheorem gcd_dvd : ∀ (b a : Nat), (gcd a b) ∣ a ∧ (gcd a b) ∣ b := by\n  by_strong_induc\n  fix b : Nat\n  assume ih : ∀ b_1 < b, ∀ (a : Nat), (gcd a b_1) ∣ a ∧ (gcd a b_1) ∣ b_1\n  fix a : Nat\n  by_cases h1 : b = 0\n  · -- Case 1. h1 : b = 0\n    rewrite [h1, gcd_base]   --Goal: a ∣ a ∧ a ∣ 0\n    apply And.intro (dvd_self a)\n    define\n    apply Exists.intro 0\n    rfl\n    done\n  · -- Case 2. h1 : b ≠ 0\n    rewrite [gcd_nonzero a h1]\n      --Goal : gcd b (a % b) ∣ a ∧ gcd b (a % b) ∣ b\n    have h2 : a % b < b := mod_nonzero_lt a h1\n    have h3 : (gcd b (a % b)) ∣ b ∧ (gcd b (a % b)) ∣ (a % b) :=\n      ih (a % b) h2 b\n    apply And.intro _ h3.left\n    show (gcd b (a % b)) ∣ a from dvd_a_of_dvd_b_mod h3.left h3.right\n    done\n  done\nYou may wonder why we didn’t start the proof like this:\ntheorem gcd_dvd : ∀ (a b : Nat), (gcd a b) ∣ a ∧ (gcd a b) ∣ b := by\n  fix a : Nat\n  by_strong_induc\n  fix b : Nat\n  assume ih : ∀ b_1 < b, (gcd a b_1) ∣ a ∧ (gcd a b_1) ∣ b_1\nIn fact, this approach wouldn’t have worked. It is an interesting exercise to try to complete this version of the proof and see why it fails.\nAnother interesting question is why we asserted both (gcd a b) ∣ a and (gcd a b) ∣ b in the same theorem. Wouldn’t it have been easier to give separate proofs of the statements ∀ (b a : Nat), (gcd a b) ∣ a and ∀ (b a : Nat), (gcd a b) ∣ b? Again, you might find it enlightening to see why that wouldn’t have worked. However, now that we have proven both divisibility statements, we can state them as separate theorems:\ntheorem gcd_dvd_left (a b : Nat) : (gcd a b) ∣ a := (gcd_dvd b a).left\n\ntheorem gcd_dvd_right (a b : Nat) : (gcd a b) ∣ b := (gcd_dvd b a).right\nNext we turn to Theorem 7.1.4 in HTPI, which says that there are integers \\(s\\) and \\(t\\) such that \\(\\text{gcd}(a, b) = s a + t b\\). (We say that \\(\\text{gcd}(a, b)\\) can be written as a linear combination of \\(a\\) and \\(b\\).) In HTPI, this is proven by using an extended version of the Euclidean algorithm to compute the coefficients \\(s\\) and \\(t\\). Here we will use a different recursive procedure to compute \\(s\\) and \\(t\\). If \\(b = 0\\), then \\(\\text{gcd}(a, b) = a = 1 \\cdot a + 0 \\cdot b\\), so we can use the values \\(s = 1\\) and \\(t = 0\\). Otherwise, let \\(q\\) and \\(r\\) be the quotient and remainder when \\(a\\) is divided by \\(b\\). Then \\(a = bq + r\\) and \\(\\text{gcd}(a, b) = \\text{gcd}(b, r)\\). Now suppose that we have already computed integers \\(s'\\) and \\(t'\\) such that \\[\n\\text{gcd}(b, r) = s' b + t' r.\n\\] Then \\[\\begin{align*}\n\\text{gcd}(a, b) &= \\text{gcd}(b, r) = s' b + t' r\\\\\n&= s' b + t' (a - bq) = t' a + (s' - t'q)b.\n\\end{align*}\\] Thus, to write \\(\\text{gcd}(a, b) = s a + t b\\) we can use the values \\[\\begin{equation}\\tag{$*$}\ns = t', \\qquad t = s' - t'q.\n\\end{equation}\\]\nWe will use these equations as the basis for recursive definitions of Lean functions gcd_c1 and gcd_c2 such that the required coefficients can be obtained from the formulas s = gcd_c1 a b and t = gcd_c2 a b. Notice that s and t could be negative, so they must have type Int, not Nat. As a result, in definitions and theorems involving gcd_c1 and gcd_c2 we will sometimes have to deal with coercion of natural numbers to integers.\nThe functions gcd_c1 and gcd_c2 will be mutually recursive; in other words, each will be defined not only in terms of itself but also in terms of the other. Fortunately, Lean allows for such mutual recursion. Here are the definitions we will use.\nmutual\n  @[semireducible]\n  def gcd_c1 (a b : Nat) : Int :=\n    match b with\n      | 0 => 1\n      | n + 1 =>\n        have : a % (n + 1) < n + 1 := mod_succ_lt a n\n        gcd_c2 (n + 1) (a % (n + 1))\n          --Corresponds to s = t'\n    termination_by b\n\n  @[semireducible]\n  def gcd_c2 (a b : Nat) : Int :=\n    match b with\n      | 0 => 0\n      | n + 1 =>\n        have : a % (n + 1) < n + 1 := mod_succ_lt a n\n        gcd_c1 (n + 1) (a % (n + 1)) -\n          (gcd_c2 (n + 1) (a % (n + 1))) * ↑(a / (n + 1))\n          --Corresponds to t = s' - t'q\n    termination_by b\nend\nNotice that in the definition of gcd_c2, the quotient a / (n + 1) is computed using natural-number division, but it is then coerced to be an integer so that it can be multiplied by the integer gcd_c2 (n + 1) (a % (n + 1)).\nOur main theorem about these functions is that they give the coefficients needed to write gcd a b as a linear combination of a and b. As usual, stating a few lemmas first helps with the proof. We leave the proofs of two of them as exercises for you (hint: imitate the proof of gcd_nonzero above).\nlemma gcd_c1_base (a : Nat) : gcd_c1 a 0 = 1 := by rfl\n\nlemma gcd_c1_nonzero (a : Nat) {b : Nat} (h : b ≠ 0) :\n    gcd_c1 a b = gcd_c2 b (a % b) := sorry\n\nlemma gcd_c2_base (a : Nat) : gcd_c2 a 0 = 0 := by rfl\n\nlemma gcd_c2_nonzero (a : Nat) {b : Nat} (h : b ≠ 0) :\n    gcd_c2 a b = gcd_c1 b (a % b) - (gcd_c2 b (a % b)) * ↑(a / b) := sorry\nWith that preparation, we are ready to prove that gcd_c1 a b and gcd_c2 a b give coefficients for expressing gcd a b as a linear combination of a and b. Of course, the theorem is proven by strong induction. For clarity, we’ll write the coercions explicitly in this proof. We’ll make a few comments after the proof that may help you follow the details.\ntheorem gcd_lin_comb : ∀ (b a : Nat),\n    (gcd_c1 a b) * ↑a + (gcd_c2 a b) * ↑b = ↑(gcd a b) := by\n  by_strong_induc\n  fix b : Nat\n  assume ih : ∀ b_1 < b, ∀ (a : Nat),\n    (gcd_c1 a b_1) * ↑a + (gcd_c2 a b_1) * ↑b_1 = ↑(gcd a b_1)\n  fix a : Nat\n  by_cases h1 : b = 0\n  · -- Case 1. h1 : b = 0\n    rewrite [h1, gcd_c1_base, gcd_c2_base, gcd_base]\n      --Goal : 1 * ↑a + 0 * ↑0 = ↑a\n    ring\n    done\n  · -- Case 2. h1 : b ≠ 0\n    rewrite [gcd_c1_nonzero a h1, gcd_c2_nonzero a h1, gcd_nonzero a h1]\n      --Goal : gcd_c2 b (a % b) * ↑a +\n      -- (gcd_c1 b (a % b) - gcd_c2 b (a % b) * ↑(a / b)) * ↑b =\n      -- ↑(gcd b (a % b))\n    set r : Nat := a % b\n    set q : Nat := a / b\n    set s : Int := gcd_c1 b r\n    set t : Int := gcd_c2 b r\n      --Goal : t * ↑a + (s - t * ↑q) * ↑b = ↑(gcd b r)\n    have h2 : r < b := mod_nonzero_lt a h1\n    have h3 : s * ↑b + t * ↑r = ↑(gcd b r) := ih r h2 b\n    have h4 : b * q + r = a := Nat.div_add_mod a b\n    rewrite [←h3, ←h4]\n    rewrite [Nat.cast_add, Nat.cast_mul]\n      --Goal : t * (↑b * ↑q + ↑r) + (s - t * ↑q) * ↑b = s * ↑b + t * ↑r\n    ring\n    done\n  done\nIn case 2, we have introduced the variables r, q, s, and t to simplify the notation. Notice that the set tactic automatically plugs in this notation in the goal. After the step rewrite [←h3, ←h4], the goal contains the expression ↑(b * q + r). You can use the #check command to see why Nat.cast_add and Nat.cast_mul convert this expression to first ↑(b * q) + ↑r and then ↑b * ↑q + ↑r. Without those steps, the ring tactic would not have been able to complete the proof.\nWe can try out the functions gcd_c1 and gcd_c2 as follows:\n++#eval:: gcd_c1 672 161  --Answer: 6\n++#eval:: gcd_c2 672 161  --Answer: -25\n  --Note 6 * 672 - 25 * 161 = 4032 - 4025 = 7 = gcd 672 161\nFinally, we turn to Theorem 7.1.6 in HTPI, which expresses one of the senses in which gcd a b is the greatest common divisor of a and b. Our proof follows the strategy of the proof in HTPI, with one additional step: we begin by using the theorem Int.natCast_dvd_natCast to change the goal from d ∣ gcd a b to ↑d ∣ ↑(gcd a b) (where the coercions are from Nat to Int), so that the rest of the proof can work with integer algebra rather than natural-number algebra.\ntheorem Theorem_7_1_6 {d a b : Nat} (h1 : d ∣ a) (h2 : d ∣ b) :\n    d ∣ gcd a b := by\n  rewrite [←Int.natCast_dvd_natCast]    --Goal : ↑d ∣ ↑(gcd a b)\n  set s : Int := gcd_c1 a b\n  set t : Int := gcd_c2 a b\n  have h3 : s * ↑a + t * ↑b = ↑(gcd a b) := gcd_lin_comb b a\n  rewrite [←h3]                 --Goal : ↑d ∣ s * ↑a + t * ↑b\n  obtain (j : Nat) (h4 : a = d * j) from h1\n  obtain (k : Nat) (h5 : b = d * k) from h2\n  rewrite [h4, h5, Nat.cast_mul, Nat.cast_mul]\n    --Goal : ↑d ∣ s * (↑d * ↑j) + t * (↑d * ↑k)\n  define\n  apply Exists.intro (s * ↑j + t * ↑k)\n  ring\n  done\nWe will ask you in the exercises to prove that, among the common divisors of a and b, gcd a b is the greatest with respect to the usual ordering of the natural numbers (as long as gcd a b ≠ 0).\n\nExercises\n\ntheorem dvd_a_of_dvd_b_mod {a b d : Nat}\n    (h1 : d ∣ b) (h2 : d ∣ (a % b)) : d ∣ a := sorry\n\n\nlemma gcd_comm_lt {a b : Nat} (h : a < b) : gcd a b = gcd b a := sorry\n\ntheorem gcd_comm (a b : Nat) : gcd a b = gcd b a := sorry\n\n\ntheorem Exercise_7_1_5 (a b : Nat) (n : Int) :\n    (∃ (s t : Int), s * a + t * b = n) ↔ (↑(gcd a b) : Int) ∣ n := sorry\n\n\ntheorem Exercise_7_1_6 (a b c : Nat) :\n    gcd a b = gcd (a + b * c) b := sorry\n\n\ntheorem gcd_is_nonzero {a b : Nat} (h : a ≠ 0 ∨ b ≠ 0) :\n    gcd a b ≠ 0 := sorry\n\n\ntheorem gcd_greatest {a b d : Nat} (h1 : gcd a b ≠ 0)\n    (h2 : d ∣ a) (h3 : d ∣ b) : d ≤ gcd a b := sorry\n\n\nlemma Lemma_7_1_10a {a b : Nat}\n    (n : Nat) (h : a ∣ b) : (n * a) ∣ (n * b) := sorry\n\nlemma Lemma_7_1_10b {a b n : Nat}\n    (h1 : n ≠ 0) (h2 : (n * a) ∣ (n * b)) : a ∣ b := sorry\n\nlemma Lemma_7_1_10c {a b : Nat}\n    (h1 : a ∣ b) (h2 : b ∣ a) : a = b := sorry\n\ntheorem Exercise_7_1_10 (a b n : Nat) :\n    gcd (n * a) (n * b) = n * gcd a b := sorry"
  },
  {
    "objectID": "Chap7.html#prime-factorization",
    "href": "Chap7.html#prime-factorization",
    "title": "7  Number Theory",
    "section": "7.2. Prime Factorization",
    "text": "7.2. Prime Factorization\nA natural number \\(n\\) is said to be prime if it is at least 2 and it cannot be written as a product of two smaller natural numbers. Of course, we can write this definition in Lean.\ndef prime (n : Nat) : Prop :=\n  2 ≤ n ∧ ¬∃ (a b : Nat), a * b = n ∧ a < n ∧ b < n\nThe main goal of Section 7.2 of HTPI is to prove that every positive integer has a unique prime factorization; that is, it can be written in a unique way as the product of a nondecreasing list of prime numbers. To get started on this goal, we first prove that every number greater than or equal to 2 has a prime factor. We leave one lemma as an exercise for you (it is a natural-number version of Theorem_3_3_7).\ndef prime_factor (p n : Nat) : Prop := prime p ∧ p ∣ n\n\nlemma dvd_trans {a b c : Nat} (h1 : a ∣ b) (h2 : b ∣ c) : a ∣ c := sorry\n\nlemma exists_prime_factor : ∀ (n : Nat), 2 ≤ n →\n    ∃ (p : Nat), prime_factor p n := by\n  by_strong_induc\n  fix n : Nat\n  assume ih : ∀ n_1 < n, 2 ≤ n_1 → ∃ (p : Nat), prime_factor p n_1\n  assume h1 : 2 ≤ n\n  by_cases h2 : prime n\n  · -- Case 1. h2 : prime n\n    apply Exists.intro n\n    define            --Goal : prime n ∧ n ∣ n\n    show prime n ∧ n ∣ n from And.intro h2 (dvd_self n)\n    done\n  · -- Case 2. h2 : ¬prime n\n    define at h2\n      --h2 : ¬(2 ≤ n ∧ ¬∃ (a b : Nat), a * b = n ∧ a < n ∧ b < n)\n    demorgan at h2\n    disj_syll h2 h1\n    obtain (a : Nat) (h3 : ∃ (b : Nat), a * b = n ∧ a < n ∧ b < n) from h2\n    obtain (b : Nat) (h4 : a * b = n ∧ a < n ∧ b < n) from h3\n    have h5 : 2 ≤ a := by\n      by_contra h6\n      have h7 : a ≤ 1 := by linarith\n      have h8 : n ≤ b :=\n        calc n\n          _ = a * b := h4.left.symm\n          _ ≤ 1 * b := by rel [h7]\n          _ = b := by ring\n      linarith        --n ≤ b contradicts b < n\n      done\n    have h6 : ∃ (p : Nat), prime_factor p a := ih a h4.right.left h5\n    obtain (p : Nat) (h7 : prime_factor p a) from h6\n    apply Exists.intro p\n    define            --Goal : prime p ∧ p ∣ n\n    define at h7      --h7 : prime p ∧ p ∣ a\n    apply And.intro h7.left\n    have h8 : a ∣ n := by\n      apply Exists.intro b\n      show n = a * b from (h4.left).symm\n      done\n    show p ∣ n from dvd_trans h7.right h8\n    done\n  done\nOf course, by the well-ordering principle, an immediate consequence of this lemma is that every number greater than or equal to 2 has a smallest prime factor.\nlemma exists_least_prime_factor {n : Nat} (h : 2 ≤ n) :\n    ∃ (p : Nat), prime_factor p n ∧\n    ∀ (q : Nat), prime_factor q n → p ≤ q := by\n  set S : Set Nat := {p : Nat | prime_factor p n}\n  have h2 : ∃ (p : Nat), p ∈ S := exists_prime_factor n h\n  show ∃ (p : Nat), prime_factor p n ∧\n    ∀ (q : Nat), prime_factor q n → p ≤ q from well_ord_princ S h2\n  done\nTo talk about prime factorizations of positive integers, we’ll need to introduce a new type. If U is any type, then List U is the type of lists of objects of type U. Such a list is written in square brackets, with the entries separated by commas. For example, [3, 7, 1] has type List Nat. The notation [] denotes the empty list, and if a has type U and l has type List U, then a :: l denotes the list consisting of a followed by the entries of l. The empty list is sometimes called the nil list, and the operation of constructing a list a :: l from a and l is called cons (short for construct). Every list can be constructed by applying the cons operation repeatedly, starting with the nil list. For example,\n\n[3, 7, 1] = 3 :: [7, 1] = 3 :: (7 :: [1]) = 3 :: (7 :: (1 :: [])).\n\nIf l has type List U and a has type U, then a ∈ l means that a is one of the entries in the list l. For example, 7 ∈ [3, 7, 1]. Lean knows several theorems about this notation:\n\n@List.not_mem_nil : ∀ {α : Type u_1} {a : α},\n                        a ∉ []\n\n@List.mem_cons : ∀ {α : Type u_1} {b : α} {l : List α} {a : α},\n                        a ∈ b :: l ↔ a = b ∨ a ∈ l\n\n@List.mem_cons_self : ∀ {α : Type u_1} {a : α} {l : List α},\n                        a ∈ a :: l\n\n@List.mem_cons_of_mem : ∀ {α : Type u_1} (y : α) {a : α} {l : List α},\n                        a ∈ l → a ∈ y :: l\n\nThe first two theorems give the conditions under which something is a member of the nil list or a list constructed by cons, and the last two are easy consequences of the second.\nTo define prime factorizations, we must define several concepts first. Some of these concepts are most easily defined recursively.\ndef all_prime (l : List Nat) : Prop := ∀ p ∈ l, prime p\n\ndef nondec (l : List Nat) : Prop :=\n  match l with\n    | [] => True   --Of course, True is a proposition that is always true\n    | n :: L => (∀ m ∈ L, n ≤ m) ∧ nondec L\n\ndef nondec_prime_list (l : List Nat) : Prop := all_prime l ∧ nondec l\n\ndef prod (l : List Nat) : Nat :=\n  match l with\n    | [] => 1\n    | n :: L => n * (prod L)\n\ndef prime_factorization (n : Nat) (l : List Nat) : Prop :=\n  nondec_prime_list l ∧ prod l = n\nAccording to these definitions, all_prime l means that every member of the list l is prime, nondec l means that every member of l is less than or equal to all later members, prod l is the product of all members of l, and prime_factorization n l means that l is a nondecreasing list of prime numbers whose product is n. It will be convenient to spell out some consequences of these definitions in several lemmas:\nlemma all_prime_nil : all_prime [] := by\n  define     --Goal : ∀ p ∈ [], prime p\n  fix p : Nat\n  contrapos  --Goal : ¬prime p → p ∉ []\n  assume h1 : ¬prime p\n  show p ∉ [] from List.not_mem_nil\n  done\n\nlemma all_prime_cons (n : Nat) (L : List Nat) :\n    all_prime (n :: L) ↔ prime n ∧ all_prime L := by\n  apply Iff.intro\n  · -- (→)\n    assume h1 : all_prime (n :: L)  --Goal : prime n ∧ all_prime L\n    define at h1  --h1 : ∀ p ∈ n :: L, prime p\n    apply And.intro (h1 n List.mem_cons_self)\n    define        --Goal : ∀ p ∈ L, prime p\n    fix p : Nat\n    assume h2 : p ∈ L\n    show prime p from h1 p (List.mem_cons_of_mem n h2)\n    done\n  · -- (←)\n    assume h1 : prime n ∧ all_prime L  --Goal : all_prime (n :: l)\n    define : all_prime L at h1\n    define\n    fix p : Nat\n    assume h2 : p ∈ n :: L\n    rewrite [List.mem_cons] at h2   --h2 : p = n ∨ p ∈ L\n    by_cases on h2\n    · -- Case 1. h2 : p = n\n      rewrite [h2]\n      show prime n from h1.left\n      done\n    · -- Case 2. h2 : p ∈ L\n      show prime p from h1.right p h2\n      done\n    done\n  done\n\nlemma nondec_nil : nondec [] := by\n  define  --Goal : True\n  trivial --trivial proves some obviously true statements, such as True\n  done\n\nlemma nondec_cons (n : Nat) (L : List Nat) :\n    nondec (n :: L) ↔ (∀ m ∈ L, n ≤ m) ∧ nondec L := by rfl\n\nlemma prod_nil : prod [] = 1 := by rfl\n\nlemma prod_cons : prod (n :: L) = n * (prod L) := by rfl\nBefore we can prove the existence of prime factorizations, we will need one more fact: every member of a list of natural numbers divides the product of the list. The proof will be by induction on the length of the list, so we will need to know how to work with lengths of lists in Lean. If l is a list, then the length of l is List.length l, which can also be written more briefly as l.length. We’ll need a few more theorems about lists:\n\n@List.length_eq_zero_iff : ∀ {α : Type u_1} {l : List α},\n                      l.length = 0 ↔ l = []\n\n@List.length_cons : ∀ {α : Type u_1} {a : α} {as : List α},\n                      (a :: as).length = as.length + 1\n\n@List.exists_cons_of_ne_nil : ∀ {α : Type u_1} {l : List α},\n                      l ≠ [] → ∃ (b : α) (l' : List α), l = b :: l'\n\nAnd we’ll need one more lemma, which follows from the three theorems above; we leave the proof as an exercise for you:\nlemma exists_cons_of_length_eq_succ {A : Type}\n    {l : List A} {n : Nat} (h : l.length = n + 1) :\n    ∃ (a : A) (L : List A), l = a :: L ∧ L.length = n := sorry\nWe can now prove that every member of a list of natural numbers divides the product of the list. After proving it by induction on the length of the list, we restate the lemma in a more convenient form.\nlemma list_elt_dvd_prod_by_length (a : Nat) : ∀ (n : Nat),\n    ∀ (l : List Nat), l.length = n → a ∈ l → a ∣ prod l := by\n  by_induc\n  · --Base Case\n    fix l : List Nat\n    assume h1 : l.length = 0\n    rewrite [List.length_eq_zero_iff] at h1     --h1 : l = []\n    rewrite [h1]                            --Goal : a ∈ [] → a ∣ prod []\n    contrapos\n    assume h2 : ¬a ∣ prod []\n    show a ∉ [] from List.not_mem_nil\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : ∀ (l : List Nat), l.length = n → a ∈ l → a ∣ prod l\n    fix l : List Nat\n    assume h1 : l.length = n + 1            --Goal : a ∈ l → a ∣ prod l\n    obtain (b : Nat) (h2 : ∃ (L : List Nat),\n      l = b :: L ∧ L.length = n) from exists_cons_of_length_eq_succ h1\n    obtain (L : List Nat) (h3 : l = b :: L ∧ L.length = n) from h2\n    have h4 : a ∈ L → a ∣ prod L := ih L h3.right\n    assume h5 : a ∈ l\n    rewrite [h3.left, prod_cons]            --Goal : a ∣ b * prod L\n    rewrite [h3.left, List.mem_cons] at h5  --h5 : a = b ∨ a ∈ L\n    by_cases on h5\n    · -- Case 1. h5 : a = b\n      apply Exists.intro (prod L)\n      rewrite [h5]\n      rfl\n      done\n    · -- Case 2. h5 : a ∈ L\n      have h6 : a ∣ prod L := h4 h5\n      have h7 : prod L ∣ b * prod L := by\n        apply Exists.intro b\n        ring\n        done\n      show a ∣ b * prod L from dvd_trans h6 h7\n      done\n    done\n  done\n\nlemma list_elt_dvd_prod {a : Nat} {l : List Nat}\n    (h : a ∈ l) : a ∣ prod l := by\n  set n : Nat := l.length\n  have h1 : l.length = n := by rfl\n  show a ∣ prod l from list_elt_dvd_prod_by_length a n l h1 h\n  done\nThe proof that every positive integer has a prime factorization is now long but straightforward.\nlemma exists_prime_factorization : ∀ (n : Nat), n ≥ 1 →\n    ∃ (l : List Nat), prime_factorization n l := by\n  by_strong_induc\n  fix n : Nat\n  assume ih : ∀ n_1 < n, n_1 ≥ 1 →\n    ∃ (l : List Nat), prime_factorization n_1 l\n  assume h1 : n ≥ 1\n  by_cases h2 : n = 1\n  · -- Case 1. h2 : n = 1\n    apply Exists.intro []\n    define\n    apply And.intro\n    · -- Proof of nondec_prime_list []\n      define\n      show all_prime [] ∧ nondec [] from\n        And.intro all_prime_nil nondec_nil\n      done\n    · -- Proof of prod [] = n\n      rewrite [prod_nil, h2]\n      rfl\n      done\n    done\n  · -- Case 2. h2 : n ≠ 1\n    have h3 : n ≥ 2 := lt_of_le_of_ne' h1 h2\n    obtain (p : Nat) (h4 : prime_factor p n ∧ ∀ (q : Nat),\n      prime_factor q n → p ≤ q) from exists_least_prime_factor h3\n    have p_prime_factor : prime_factor p n := h4.left\n    define at p_prime_factor\n    have p_prime : prime p := p_prime_factor.left\n    have p_dvd_n : p ∣ n := p_prime_factor.right\n    have p_least : ∀ (q : Nat), prime_factor q n → p ≤ q := h4.right\n    obtain (m : Nat) (n_eq_pm : n = p * m) from p_dvd_n\n    have h5 : m ≠ 0 := by\n      contradict h1 with h6\n      have h7 : n = 0 :=\n        calc n\n          _ = p * m := n_eq_pm\n          _ = p * 0 := by rw [h6]\n          _ = 0 := by ring\n      rewrite [h7]\n      decide\n      done\n    have m_pos : 0 < m := Nat.pos_of_ne_zero h5\n    have m_lt_n : m < n := by\n      define at p_prime\n      show m < n from\n        calc m\n          _ < m + m := by linarith\n          _ = 2 * m := by ring\n          _ ≤ p * m := by rel [p_prime.left]\n          _ = n := n_eq_pm.symm\n      done\n    obtain (L : List Nat) (h6 : prime_factorization m L)\n      from ih m m_lt_n m_pos\n    define at h6\n    have ndpl_L : nondec_prime_list L := h6.left\n    define at ndpl_L\n    apply Exists.intro (p :: L)\n    define\n    apply And.intro\n    · -- Proof of nondec_prime_list (p :: L)\n      define\n      apply And.intro\n      · -- Proof of all_prime (p :: L)\n        rewrite [all_prime_cons]\n        show prime p ∧ all_prime L from And.intro p_prime ndpl_L.left\n        done\n      · -- Proof of nondec (p :: L)\n        rewrite [nondec_cons]\n        apply And.intro _ ndpl_L.right\n        fix q : Nat\n        assume q_in_L : q ∈ L\n        have h7 : q ∣ prod L := list_elt_dvd_prod q_in_L\n        rewrite [h6.right] at h7   --h7 : q ∣ m\n        have h8 : m ∣ n := by\n          apply Exists.intro p\n          rewrite [n_eq_pm]\n          ring\n          done\n        have q_dvd_n : q ∣ n := dvd_trans h7 h8\n        have ap_L : all_prime L := ndpl_L.left\n        define at ap_L\n        have q_prime_factor : prime_factor q n :=\n          And.intro (ap_L q q_in_L) q_dvd_n\n        show p ≤ q from p_least q q_prime_factor\n        done\n      done\n    · -- Proof of prod (p :: L) = n\n      rewrite [prod_cons, h6.right, n_eq_pm]\n      rfl\n      done\n    done\n  done\nWe now turn to the proof that the prime factorization of a positive integer is unique. In preparation for that proof, HTPI defines two numbers to be relatively prime if their greatest common divisor is 1, and then it uses that concept to prove two theorems, 7.2.2 and 7.2.3. Here are similar proofs of those theorems in Lean, with the proof of one lemma left as an exercise. In the proof of Theorem 7.2.2, we begin, as we did in the proof of Theorem 7.1.6, by converting the goal from natural numbers to integers so that we can use integer algebra.\ndef rel_prime (a b : Nat) : Prop := gcd a b = 1\n\ntheorem Theorem_7_2_2 {a b c : Nat}\n    (h1 : c ∣ a * b) (h2 : rel_prime a c) : c ∣ b := by\n  rewrite [←Int.natCast_dvd_natCast]  --Goal : ↑c ∣ ↑b\n  define at h1; define at h2; define\n  obtain (j : Nat) (h3 : a * b = c * j) from h1\n  set s : Int := gcd_c1 a c\n  set t : Int := gcd_c2 a c\n  have h4 : s * ↑a + t * ↑c = ↑(gcd a c) := gcd_lin_comb c a\n  rewrite [h2, Nat.cast_one] at h4  --h4 : s * ↑a + t * ↑c = (1 : Int)\n  apply Exists.intro (s * ↑j + t * ↑b)\n  show ↑b = ↑c * (s * ↑j + t * ↑b) from\n    calc ↑b\n      _ = (1 : Int) * ↑b := (one_mul _).symm\n      _ = (s * ↑a + t * ↑c) * ↑b := by rw [h4]\n      _ = s * (↑a * ↑b) + t * ↑c * ↑b := by ring\n      _ = s * (↑c * ↑j) + t * ↑c * ↑b := by\n            rw [←Nat.cast_mul a b, h3, Nat.cast_mul c j]\n      _ = ↑c * (s * ↑j + t * ↑b) := by ring\n  done\n\nlemma dvd_prime {a p : Nat}\n    (h1 : prime p) (h2 : a ∣ p) : a = 1 ∨ a = p := sorry\n\nlemma rel_prime_of_prime_not_dvd {a p : Nat}\n    (h1 : prime p) (h2 : ¬p ∣ a) : rel_prime a p := by\n  have h3 : gcd a p ∣ a := gcd_dvd_left a p\n  have h4 : gcd a p ∣ p := gcd_dvd_right a p\n  have h5 : gcd a p = 1 ∨ gcd a p = p := dvd_prime h1 h4\n  have h6 : gcd a p ≠ p := by\n    contradict h2 with h6\n    rewrite [h6] at h3\n    show p ∣ a from h3\n    done\n  disj_syll h5 h6\n  show rel_prime a p from h5\n  done\n\ntheorem Theorem_7_2_3 {a b p : Nat}\n    (h1 : prime p) (h2 : p ∣ a * b) : p ∣ a ∨ p ∣ b := by\n  or_right with h3\n  have h4 : rel_prime a p := rel_prime_of_prime_not_dvd h1 h3\n  show p ∣ b from Theorem_7_2_2 h2 h4\n  done\nTheorem 7.2.4 in HTPI extends Theorem 7.2.3 to show that if a prime number divides the product of a list of natural numbers, then it divides one of the numbers in the list. (Theorem 7.2.3 is the case of a list of length two.) The proof in HTPI is by induction on the length of the list, and we could use that method to prove the theorem in Lean. But look back at our proof of the lemma list_elt_dvd_prod_by_length, which also used induction on the length of a list. In the base case, we ended up proving that the nil list has the property stated in the lemma, and in the induction step we proved that if a list L has the property, then so does any list of the form b :: L. We could think of this as a kind of “induction on lists.” As we observed earlier, every list can be constructed by starting with the nil list and applying cons finitely many times. It follows that if the nil list has some property, and applying the cons operation to a list with the property produces another list with the property, then all lists have the property. (In fact, a similar principle was at work in our recursive definitions of nondec l and prod l.)\nLean has a theorem called List.rec that can be used to justify induction on lists. This is a little more convenient than induction on the length of a list, so we’ll use it to prove Theorem 7.2.4. The proof uses two lemmas, whose proofs we leave as exercises for you.\nlemma eq_one_of_dvd_one {n : Nat} (h : n ∣ 1) : n = 1 := sorry\n\nlemma prime_not_one {p : Nat} (h : prime p) : p ≠ 1 := sorry\n\ntheorem Theorem_7_2_4 {p : Nat} (h1 : prime p) :\n    ∀ (l : List Nat), p ∣ prod l → ∃ a ∈ l, p ∣ a := by\n  apply List.rec\n  · -- Base Case.  Goal : p ∣ prod [] → ∃ a ∈ [], p ∣ a\n    rewrite [prod_nil]\n    assume h2 : p ∣ 1\n    show ∃ a ∈ [], p ∣ a from\n      absurd (eq_one_of_dvd_one h2) (prime_not_one h1)\n    done\n  · -- Induction Step\n    fix b : Nat\n    fix L : List Nat\n    assume ih : p ∣ prod L → ∃ a ∈ L, p ∣ a\n      --Goal : p ∣ prod (b :: L) → ∃ a ∈ b :: L, p ∣ a\n    assume h2 : p ∣ prod (b :: L)\n    rewrite [prod_cons] at h2\n    have h3 : p ∣ b ∨ p ∣ prod L := Theorem_7_2_3 h1 h2\n    by_cases on h3\n    · -- Case 1. h3 : p ∣ b\n      apply Exists.intro b\n      show b ∈ b :: L ∧ p ∣ b from\n        And.intro List.mem_cons_self h3\n      done\n    · -- Case 2. h3 : p ∣ prod L\n      obtain (a : Nat) (h4 : a ∈ L ∧ p ∣ a) from ih h3\n      apply Exists.intro a\n      show a ∈ b :: L ∧ p ∣ a from\n        And.intro (List.mem_cons_of_mem b h4.left) h4.right\n      done\n    done\n  done\nIn Theorem 7.2.4, if all members of the list l are prime, then we can conclude not merely that p divides some member of l, but that p is one of the members.\nlemma prime_in_list {p : Nat} {l : List Nat}\n    (h1 : prime p) (h2 : all_prime l) (h3 : p ∣ prod l) : p ∈ l := by\n  obtain (a : Nat) (h4 : a ∈ l ∧ p ∣ a) from Theorem_7_2_4 h1 l h3\n  define at h2\n  have h5 : prime a := h2 a h4.left\n  have h6 : p = 1 ∨ p = a := dvd_prime h5 h4.right\n  disj_syll h6 (prime_not_one h1)\n  rewrite [h6]\n  show a ∈ l from h4.left\n  done\nThe uniqueness of prime factorizations follows from Theorem 7.2.5 of HTPI, which says that if two nondecreasing lists of prime numbers have the same product, then the two lists must be the same. In HTPI, a key step in the proof of Theorem 7.2.5 is to show that if two nondecreasing lists of prime numbers have the same product, then the last entry of one list is less than or equal to the last entry of the other. In Lean, because of the way the cons operation works, it is easier to work with the first entries of the lists.\nlemma first_le_first {p q : Nat} {l m : List Nat}\n    (h1 : nondec_prime_list (p :: l)) (h2 : nondec_prime_list (q :: m))\n    (h3 : prod (p :: l) = prod (q :: m)) : p ≤ q := by\n  define at h1; define at h2\n  have h4 : q ∣ prod (p :: l) := by\n    define\n    apply Exists.intro (prod m)\n    rewrite [←prod_cons]\n    show prod (p :: l) = prod (q :: m) from h3\n    done\n  have h5 : all_prime (q :: m) := h2.left\n  rewrite [all_prime_cons] at h5\n  have h6 : q ∈ p :: l := prime_in_list h5.left h1.left h4\n  have h7 : nondec (p :: l) := h1.right\n  rewrite [nondec_cons] at h7\n  rewrite [List.mem_cons] at h6\n  by_cases on h6\n  · -- Case 1. h6 : q = p\n    linarith\n    done\n  · -- Case 2. h6 : q ∈ l\n    have h8 : ∀ m ∈ l, p ≤ m := h7.left\n    show p ≤ q from h8 q h6\n    done\n  done\nThe proof of Theorem 7.2.5 is another proof by induction on lists. It uses a few more lemmas whose proofs we leave as exercises.\nlemma nondec_prime_list_tail {p : Nat} {l : List Nat}\n    (h : nondec_prime_list (p :: l)) : nondec_prime_list l := sorry\n\nlemma cons_prod_not_one {p : Nat} {l : List Nat}\n    (h : nondec_prime_list (p :: l)) : prod (p :: l) ≠ 1 := sorry\n\nlemma list_nil_iff_prod_one {l : List Nat} (h : nondec_prime_list l) :\n    l = [] ↔ prod l = 1 := sorry\n\nlemma prime_pos {p : Nat} (h : prime p) : p > 0 := sorry\n\ntheorem Theorem_7_2_5 : ∀ (l1 l2 : List Nat),\n    nondec_prime_list l1 → nondec_prime_list l2 →\n    prod l1 = prod l2 → l1 = l2 := by\n  apply List.rec\n  · -- Base Case.  Goal : ∀ (l2 : List Nat), nondec_prime_list [] →\n    -- nondec_prime_list l2 → prod [] = prod l2 → [] = l2\n    fix l2 : List Nat\n    assume h1 : nondec_prime_list []\n    assume h2 : nondec_prime_list l2\n    assume h3 : prod [] = prod l2\n    rewrite [prod_nil, eq_comm, ←list_nil_iff_prod_one h2] at h3\n    show [] = l2 from h3.symm\n    done\n  · -- Induction Step\n    fix p : Nat\n    fix L1 : List Nat\n    assume ih : ∀ (L2 : List Nat), nondec_prime_list L1 →\n      nondec_prime_list L2 → prod L1 = prod L2 → L1 = L2\n    -- Goal : ∀ (l2 : List Nat), nondec_prime_list (p :: L1) →\n    -- nondec_prime_list l2 → prod (p :: L1) = prod l2 → p :: L1 = l2\n    fix l2 : List Nat\n    assume h1 : nondec_prime_list (p :: L1)\n    assume h2 : nondec_prime_list l2\n    assume h3 : prod (p :: L1) = prod l2\n    have h4 : ¬prod (p :: L1) = 1 := cons_prod_not_one h1\n    rewrite [h3, ←list_nil_iff_prod_one h2] at h4\n    obtain (q : Nat) (h5 : ∃ (L : List Nat), l2 = q :: L) from\n      List.exists_cons_of_ne_nil h4\n    obtain (L2 : List Nat) (h6 : l2 = q :: L2) from h5\n    rewrite [h6] at h2    --h2 : nondec_prime_list (q :: L2)\n    rewrite [h6] at h3    --h3 : prod (p :: L1) = prod (q :: L2)\n    have h7 : p ≤ q := first_le_first h1 h2 h3\n    have h8 : q ≤ p := first_le_first h2 h1 h3.symm\n    have h9 : p = q := by linarith\n    rewrite [h9, prod_cons, prod_cons] at h3\n      --h3 : q * prod L1 = q * prod L2\n    have h10 : nondec_prime_list L1 := nondec_prime_list_tail h1\n    have h11 : nondec_prime_list L2 := nondec_prime_list_tail h2\n    define at h2\n    have h12 : all_prime (q :: L2) := h2.left\n    rewrite [all_prime_cons] at h12\n    have h13 : q > 0 := prime_pos h12.left\n    have h14 : prod L1 = prod L2 := Nat.eq_of_mul_eq_mul_left h13 h3\n    have h15 : L1 = L2 := ih L2 h10 h11 h14\n    rewrite [h6, h9, h15]\n    rfl\n    done\n  done\nPutting it all together, we can finally prove the fundamental theorem of arithmetic, which is stated as Theorem 7.2.6 in HTPI:\ntheorem fund_thm_arith (n : Nat) (h : n ≥ 1) :\n    ∃! (l : List Nat), prime_factorization n l := by\n  exists_unique\n  · -- Existence\n    show ∃ (l : List Nat), prime_factorization n l from\n      exists_prime_factorization n h\n    done\n  · -- Uniqueness\n    fix l1 : List Nat; fix l2 : List Nat\n    assume h1 : prime_factorization n l1\n    assume h2 : prime_factorization n l2\n    define at h1; define at h2\n    have h3 : prod l1 = n := h1.right\n    rewrite [←h2.right] at h3\n    show l1 = l2 from Theorem_7_2_5 l1 l2 h1.left h2.left h3\n    done\n  done\n\nExercises\n\nlemma dvd_prime {a p : Nat}\n    (h1 : prime p) (h2 : a ∣ p) : a = 1 ∨ a = p := sorry\n\n\n--Hints:  Start with apply List.rec.\n--You may find the theorem mul_ne_zero useful.\ntheorem prod_nonzero_nonzero : ∀ (l : List Nat),\n    (∀ a ∈ l, a ≠ 0) → prod l ≠ 0 := sorry\n\n\ntheorem rel_prime_iff_no_common_factor (a b : Nat) :\n    rel_prime a b ↔ ¬∃ (p : Nat), prime p ∧ p ∣ a ∧ p ∣ b := sorry\n\n\ntheorem rel_prime_symm {a b : Nat} (h : rel_prime a b) :\n    rel_prime b a := sorry\n\n\nlemma in_prime_factorization_iff_prime_factor {a : Nat} {l : List Nat}\n    (h1 : prime_factorization a l) (p : Nat) :\n    p ∈ l ↔ prime_factor p a := sorry\n\n\ntheorem Exercise_7_2_5 {a b : Nat} {l m : List Nat}\n    (h1 : prime_factorization a l) (h2 : prime_factorization b m) :\n    rel_prime a b ↔ (¬∃ (p : Nat), p ∈ l ∧ p ∈ m) := sorry\n\n\ntheorem Exercise_7_2_6 (a b : Nat) :\n    rel_prime a b ↔ ∃ (s t : Int), s * a + t * b = 1 := sorry\n\n\ntheorem Exercise_7_2_7 {a b a' b' : Nat}\n    (h1 : rel_prime a b) (h2 : a' ∣ a) (h3 : b' ∣ b) :\n    rel_prime a' b' := sorry\n\n\ntheorem Exercise_7_2_9 {a b j k : Nat}\n    (h1 : gcd a b ≠ 0) (h2 : a = j * gcd a b) (h3 : b = k * gcd a b) :\n    rel_prime j k := sorry\n\n\ntheorem Exercise_7_2_17a (a b c : Nat) :\n    gcd a (b * c) ∣ gcd a b * gcd a c := sorry"
  },
  {
    "objectID": "Chap7.html#modular-arithmetic",
    "href": "Chap7.html#modular-arithmetic",
    "title": "7  Number Theory",
    "section": "7.3. Modular Arithmetic",
    "text": "7.3. Modular Arithmetic\nIf \\(m\\) is a positive integer and \\(a\\) and \\(b\\) are integers, then HTPI uses the notation \\(a \\equiv b\\pmod m\\), or sometimes \\(a \\equiv_m b\\), to indicate that \\(a\\) is congruent to \\(b\\) modulo \\(m\\), which is defined to mean \\(m \\mid (a - b)\\). Congruence modulo \\(m\\) is an equivalence relation on the integers, and therefore it induces a partition \\(\\mathbb{Z}/{\\equiv_m}\\) of the integers, as shown in Section 4.5 of HTPI. The elements of this partition are the equivalence classes \\([a]_m\\) for \\(a \\in \\mathbb{Z}\\); we will call these congruence classes modulo \\(m\\). Section 7.3 of HTPI defines operations of addition and multiplication of congruence classes and proves algebraic properties of those operations.\nFor the purpose of working out the rules of modular arithmetic, the only important properties of congruence classes are the following:\n\nFor every integer \\(a\\), there is a corresponding congruence class \\([a]_m \\in \\mathbb{Z}/{\\equiv_m}\\).\nFor every congruence class \\(X \\in \\mathbb{Z}/{\\equiv_m}\\), there is some integer \\(a\\) such that \\(X = [a]_m\\).\nFor all integers \\(a\\) and \\(b\\), \\([a]_m = [b]_m\\) if and only if \\(a \\equiv_m b\\).\nFor all integers \\(a\\) and \\(b\\), \\([a]_m + [b]_m = [a + b]_m\\).\nFor all integers \\(a\\) and \\(b\\), \\([a]_m \\cdot [b]_m = [ab]_m\\).\n\nTo study congruence modulo m in Lean, we will declare m to have type Nat, which allows for the possibility that m = 0, but we will mostly focus on the case m ≠ 0. For a and b of type Int, we define congr_mod m a b to mean that a is congruent to b modulo m. Notice that to define this relation in Lean, we must coerce m to be an integer so that we can use it in the divisibility relation on the integers.\ndef congr_mod (m : Nat) (a b : Int) : Prop := (↑m : Int) ∣ (a - b)\nWe can teach Lean to use more familiar notation for congruence modulo m by giving the following command:\nnotation:50 a \" ≡ \" b \" (MOD \" m \")\" => congr_mod m a b\nThis tells Lean that if we type a ≡ b (MOD m), then Lean should interpret it as congr_mod m a b. (To enter the symbol ≡, type \\==. Don’t worry about the :50 in the notation command above. It is there to help Lean parse this new notation when it occurs together with other notation.)\nWe can now prove that congruence modulo m is reflexive, symmetric, and transitive. In these proofs, we leave it to Lean to fill in coercions when they are necessary, and as usual, we leave some details as exercises.\ntheorem congr_refl (m : Nat) : ∀ (a : Int), a ≡ a (MOD m) := sorry\n\ntheorem congr_symm {m : Nat} : ∀ {a b : Int},\n    a ≡ b (MOD m) → b ≡ a (MOD m) := by\n  fix a : Int; fix b : Int\n  assume h1 : a ≡ b (MOD m)\n  define at h1                 --h1 : ∃ (c : Int), a - b = ↑m * c\n  define                       --Goal : ∃ (c : Int), b - a = ↑m * c\n  obtain (c : Int) (h2 : a - b = m * c) from h1\n  apply Exists.intro (-c)\n  show b - a = m * (-c) from\n    calc b - a\n      _ = -(a - b) := by ring\n      _ = -(m * c) := by rw [h2]\n      _ = m * (-c) := by ring\n  done\n\ntheorem congr_trans {m : Nat} : ∀ {a b c : Int},\n    a ≡ b (MOD m) → b ≡ c (MOD m) → a ≡ c (MOD m) := sorry\nWe could now repeat the entire development of \\(\\mathbb{Z}/{\\equiv_m}\\) in Lean, but there is no need to do so; such a development is already included in Lean’s library of definitions and theorems. For each natural number m, Lean has a type ZMod m, and the objects of that type are Lean’s version of the congruence classes modulo m. We should warn you that Lean’s way of defining ZMod m differs in some ways from HTPI’s definition of \\(\\mathbb{Z}/{\\equiv_m}\\). In particular, objects of type ZMod m are not sets of integers. Thus, if X has type ZMod m and a has type Int, then Lean will not understand what you mean if you write a ∈ X. However, Lean’s congruence classes have the properties 1–5 listed above, and that’s all that will matter to us.\nProperty 1 says that if a has type Int, then there should be a corresponding congruence class in ZMod m. In fact, you can find the corresponding congruence class by simply coercing a to have type ZMod m. But for the sake of clarity, we will introduce a function for computing the congruence class modulo m of a:\ndef cc (m : Nat) (a : Int) : ZMod m := (↑a : ZMod m)\nThus, cc m a is the congruence class modulo m of a. Once again, it will be convenient to teach Lean to use more familiar notation for congruence classes, so we give the command:\nnotation:max \"[\"a\"]_\"m:max => cc m a\nNow if we type [a]_m, then Lean will interpret it as cc m a. Thus, from now on, [a]_m will be our notation in Lean for the congruence class modulo m of a; it corresponds to the HTPI notation \\([a]_m\\). (Once again, you can ignore the two occurrences of :max in the notation command above.)\nProperties 2–5 of congruence classes are established by the following theorems:\ntheorem cc_rep {m : Nat} (X : ZMod m) : ∃ (a : Int), X = [a]_m\n\ntheorem cc_eq_iff_congr (m : Nat) (a b : Int) :\n    [a]_m = [b]_m ↔ a ≡ b (MOD m)\n\ntheorem add_class (m : Nat) (a b : Int) :\n    [a]_m + [b]_m = [a + b]_m\n\ntheorem mul_class (m : Nat) (a b : Int) :\n    [a]_m * [b]_m = [a * b]_m\nWe won’t discuss the proofs of these theorems, since they depend on details of Lean’s representation of objects of type ZMod m that are beyond the scope of this book. But these theorems are all we will need to use to develop the theory of modular arithmetic.\nIn many of our theorems about ZMod m, we will need to include a hypothesis that m ≠ 0. We will usually state this hypothesis in the form NeZero m, which is a proposition that is equivalent to m ≠ 0. Indeed, there is a theorem in Lean’s library that asserts this equivalence:\n\n@neZero_iff : ∀ {R : Type u_1} [inst : Zero R] {n : R},\n              NeZero n ↔ n ≠ 0\n\nWhat distinguishes NeZero m from m ≠ 0 is that NeZero m is what is called a type class. What this means is that, once Lean has a proof of NeZero m for some natural number m, it will remember that proof and be able to recall it when necessary. As a result, NeZero m can be used as a new kind of implicit argument in the statement of a theorem. To make it an implicit argument, we write it in square brackets, like this: [NeZero m]. When applying a theorem that includes this hypothesis, there is no need to supply a proof that m ≠ 0; as long as Lean knows about such a proof, it will recall that proof on its own. When you are proving a theorem that includes the hypothesis [NeZero m], Lean will recognize NeZero.ne m as a proof that m ≠ 0.\nThe first theorem in Section 7.3 of HTPI, Theorem 7.3.1, says that if m ≠ 0, then every integer a is congruent modulo m to exactly one integer r satisfying 0 ≤ r < m. As explained in HTPI, we say that {0, 1, ..., m - 1} is a complete residue system modulo m. This implies that the objects of type ZMod m are precisely the congruence classes [0]_m, [1]_m, …, [m - 1]_m.\nThe proof of Theorem 7.3.1 makes use of the quotient and remainder when a is divided by m. In Section 6.4, we learned about the Lean theorems Nat.div_add_mod and Nat.mod_lt, but those theorems concerned quotients and remainders when dividing natural numbers. Fortunately, Lean has similar theorems for dealing with division of integers:\n\nInt.ediv_add_emod : ∀ (a b : ℤ), b * (a / b) + a % b = a\n\nInt.emod_lt_of_pos : ∀ (a : ℤ) {b : ℤ}, 0 < b → a % b < b\n\nInt.emod_nonneg : ∀ (a : ℤ) {b : ℤ}, b ≠ 0 → 0 ≤ a % b\n\nWe now have all the background we need to prove Theorem 7.3.1 in Lean. We begin with a few lemmas, some of which require the hypothesis [NeZero m]. The proof of the theorem closely follows the proof in HTPI. Note that all of the proofs below involve the expression a % m. Since a is an integer, the operator % in this expression must be the integer version of the mod operator, and therefore m must be coerced to be an integer. Thus, Lean interprets a % m as a % ↑m\nlemma mod_nonneg (m : Nat) [NeZero m] (a : Int) : 0 ≤ a % m := by\n  have h1 : (↑m : Int) ≠ 0 := (Nat.cast_ne_zero).rtl (NeZero.ne m)\n  show 0 ≤ a % m from Int.emod_nonneg a h1\n  done\n\nlemma mod_lt (m : Nat) [NeZero m] (a : Int) : a % m < m := sorry\n\nlemma congr_mod_mod (m : Nat) (a : Int) : a ≡ a % m (MOD m) := by\n  define\n  have h1 : m * (a / m) + a % m = a := Int.ediv_add_emod a m\n  apply Exists.intro (a / m)\n  show a - a % m = m * (a / m) from\n    calc a - (a % m)\n      _ = m * (a / m) + a % m - a % m := by rw [h1]\n      _ = m * (a / m) := by ring\n  done\n\nlemma mod_cmpl_res (m : Nat) [NeZero m] (a : Int) :\n    0 ≤ a % m ∧ a % m < m ∧ a ≡ a % m (MOD m) :=\n  And.intro (mod_nonneg m a) (And.intro (mod_lt m a) (congr_mod_mod m a))\n\ntheorem Theorem_7_3_1 (m : Nat) [NeZero m] (a : Int) :\n    ∃! (r : Int), 0 ≤ r ∧ r < m ∧ a ≡ r (MOD m) := by\n  exists_unique\n  · -- Existence\n    apply Exists.intro (a % m)\n    show 0 ≤ a % m ∧ a % m < m ∧ a ≡ a % m (MOD m)\n      from mod_cmpl_res m a\n    done\n  · -- Uniqueness\n    fix r1 : Int; fix r2 : Int\n    assume h1 : 0 ≤ r1 ∧ r1 < m ∧ a ≡ r1 (MOD m)\n    assume h2 : 0 ≤ r2 ∧ r2 < m ∧ a ≡ r2 (MOD m)\n    have h3 : r1 ≡ r2 (MOD m) :=\n      congr_trans (congr_symm h1.right.right) h2.right.right\n    obtain (d : Int) (h4 : r1 - r2 = m * d) from h3\n    have h5 : r1 - r2 < m * 1 := by linarith\n    have h6 : m * (-1) < r1 - r2 := by linarith\n    rewrite [h4] at h5   --h5 : m * d < m * 1\n    rewrite [h4] at h6   --h6 : m * -1 < m * d\n    have h7 : (↑m : Int) ≥ 0 := Nat.cast_nonneg m\n    have h8 : d < 1 := lt_of_mul_lt_mul_of_nonneg_left h5 h7\n    have h9 : -1 < d := lt_of_mul_lt_mul_of_nonneg_left h6 h7\n    have h10 : d = 0 := by linarith\n    show r1 = r2 from\n      calc r1\n        _ = r1 - r2 + r2 := by ring\n        _ = m * 0 + r2 := by rw [h4, h10]\n        _ = r2 := by ring\n    done\n  done\nThe lemma mod_cmpl_res above says that a % m is an element of the complete residue system {0, 1, ..., m - 1} that is congruent to a modulo m. The lemma requires the hypothesis [NeZero m], because its proof appeals to two previous lemmas, mod_nonneg and mod_lt, that require that hypothesis. But when the proof invokes those previous lemmas, this hypothesis is not mentioned, because it is an implicit argument.\nAn immediate consequence of the lemma congr_mod_mod is the following lemma, which will be useful to us later.\nlemma cc_eq_mod (m : Nat) (a : Int) : [a]_m = [a % m]_m := \n  (cc_eq_iff_congr m a (a % m)).rtl (congr_mod_mod m a)\nTheorem 7.3.6 in HTPI states a number of algebraic properties of modular arithmetic. These properties all follow easily from the theorems we have already stated. To illustrate this, we prove two parts of the theorem.\ntheorem Theorem_7_3_6_1 {m : Nat} (X Y : ZMod m) : X + Y = Y + X := by\n  obtain (a : Int) (h1 : X = [a]_m) from cc_rep X\n  obtain (b : Int) (h2 : Y = [b]_m) from cc_rep Y\n  rewrite [h1, h2]\n  have h3 : a + b = b + a := by ring\n  show [a]_m + [b]_m = [b]_m + [a]_m from\n    calc [a]_m + [b]_m\n      _ = [a + b]_m := add_class m a b\n      _ = [b + a]_m := by rw [h3]\n      _ = [b]_m + [a]_m := (add_class m b a).symm\n  done\n\ntheorem Theorem_7_3_6_7 {m : Nat} (X : ZMod m) : X * [1]_m = X := by\n  obtain (a : Int) (h1 : X = [a]_m) from cc_rep X\n  rewrite [h1]\n  have h2 : a * 1 = a := by ring\n  show [a]_m * [1]_m = [a]_m from\n    calc [a]_m * [1]_m\n      _ = [a * 1]_m := mul_class m a 1\n      _ = [a]_m := by rw [h2]\n  done\nTheorem_7_3_6_7 shows that [1]_m is the multiplicative identity element for ZMod m. We say that a congruence class Y is a multiplicative inverse of another class X if X * Y = [1]_m, and a congruence class is invertible if it has a multiplicative inverse:\ndef invertible {m : Nat} (X : ZMod m) : Prop :=\n  ∃ (Y : ZMod m), X * Y = [1]_m\nWhich congruence classes are invertible? The answer is given by Theorem 7.3.7 in HTPI, which says that if \\(a\\) is a positive integer, then \\([a]_m\\) is invertible if and only if \\(m\\) and \\(a\\) are relatively prime. The proof uses an exercise from the last section. Here is the Lean version of the proof.\ntheorem Exercise_7_2_6 (a b : Nat) :\n    rel_prime a b ↔ ∃ (s t : Int), s * a + t * b = 1 := sorry\n\nlemma gcd_c2_inv {m a : Nat} (h1 : rel_prime m a) :\n    [a]_m * [gcd_c2 m a]_m = [1]_m := by\n  set s : Int := gcd_c1 m a\n  have h2 : s * m + (gcd_c2 m a) * a = gcd m a := gcd_lin_comb a m\n  define at h1\n  rewrite [h1, Nat.cast_one] at h2  --h2 : s * ↑m + gcd_c2 m a * ↑a = 1\n  rewrite [mul_class, cc_eq_iff_congr]\n  define     --Goal : ∃ (c : Int), ↑a * gcd_c2 m a - 1 = ↑m * c\n  apply Exists.intro (-s)\n  show a * (gcd_c2 m a) - 1 = m * (-s) from\n    calc a * (gcd_c2 m a) - 1\n      _ = s * m + (gcd_c2 m a) * a + m * (-s) - 1 := by ring\n      _ = 1 + m * (-s) - 1 := by rw [h2]\n      _ = m * (-s) := by ring\n  done\n\ntheorem Theorem_7_3_7 (m a : Nat) :\n    invertible [a]_m ↔ rel_prime m a := by\n  apply Iff.intro\n  · -- (→)\n    assume h1 : invertible [a]_m\n    define at h1\n    obtain (Y : ZMod m) (h2 : [a]_m * Y = [1]_m) from h1\n    obtain (b : Int) (h3 : Y = [b]_m) from cc_rep Y\n    rewrite [h3, mul_class, cc_eq_iff_congr] at h2\n    define at h2\n    obtain (c : Int) (h4 : a * b - 1 = m * c) from h2\n    rewrite [Exercise_7_2_6]\n      --Goal : ∃ (s t : Int), s * ↑m + t * ↑a = 1\n    apply Exists.intro (-c)\n    apply Exists.intro b\n    show (-c) * m + b * a = 1 from\n      calc (-c) * m + b * a\n        _ = (-c) * m + (a * b - 1) + 1 := by ring\n        _ = (-c) * m + m * c + 1 := by rw [h4]\n        _ = 1 := by ring\n    done\n  · -- (←)\n    assume h1 : rel_prime m a\n    define\n    show ∃ (Y : ZMod m), [a]_m * Y = [1]_m from\n      Exists.intro [gcd_c2 m a]_m (gcd_c2_inv h1)\n    done\n  done\n\nExercises\n\ntheorem congr_trans {m : Nat} : ∀ {a b c : Int},\n    a ≡ b (MOD m) → b ≡ c (MOD m) → a ≡ c (MOD m) := sorry\n\n\ntheorem Theorem_7_3_6_3 {m : Nat} (X : ZMod m) : X + [0]_m = X := sorry\n\n\ntheorem Theorem_7_3_6_4 {m : Nat} (X : ZMod m) :\n    ∃ (Y : ZMod m), X + Y = [0]_m := sorry\n\n\ntheorem Exercise_7_3_4a {m : Nat} (Z1 Z2 : ZMod m)\n    (h1 : ∀ (X : ZMod m), X + Z1 = X)\n    (h2 : ∀ (X : ZMod m), X + Z2 = X) : Z1 = Z2 := sorry\n\n\ntheorem Exercise_7_3_4b {m : Nat} (X Y1 Y2 : ZMod m)\n    (h1 : X + Y1 = [0]_m) (h2 : X + Y2 = [0]_m) : Y1 = Y2 := sorry\n\n\ntheorem Theorem_7_3_10 (m a : Nat) (b : Int) :\n    ¬(↑(gcd m a) : Int) ∣ b → ¬∃ (x : Int), a * x ≡ b (MOD m) := sorry\n\n\ntheorem Theorem_7_3_11 (m n : Nat) (a b : Int) (h1 : n ≠ 0) :\n    n * a ≡ n * b (MOD n * m) ↔ a ≡ b (MOD m) := sorry\n\n\ntheorem Exercise_7_3_16 {m : Nat} {a b : Int} (h : a ≡ b (MOD m)) :\n    ∀ (n : Nat), a ^ n ≡ b ^ n (MOD m) := sorry\n\n\nexample {m : Nat} [NeZero m] (X : ZMod m) :\n    ∃! (a : Int), 0 ≤ a ∧ a < m ∧ X = [a]_m := sorry\n\n\ntheorem congr_rel_prime {m a b : Nat} (h1 : a ≡ b (MOD m)) :\n    rel_prime m a ↔ rel_prime m b := sorry\n\n\n--Hint: You may find the theorem Int.ofNat_mod_ofNat useful.\ntheorem rel_prime_mod (m a : Nat) :\n    rel_prime m (a % m) ↔ rel_prime m a := sorry\n\n\nlemma congr_iff_mod_eq_Int (m : Nat) (a b : Int) [NeZero m] :\n    a ≡ b (MOD m) ↔ a % ↑m = b % ↑m := sorry\n\n--Hint for next theorem: Use the lemma above,\n--together with the theorems Int.ofNat_mod_ofNat and Nat.cast_inj.\ntheorem congr_iff_mod_eq_Nat (m a b : Nat) [NeZero m] :\n    ↑a ≡ ↑b (MOD m) ↔ a % m = b % m := sorry"
  },
  {
    "objectID": "Chap7.html#eulers-theorem",
    "href": "Chap7.html#eulers-theorem",
    "title": "7  Number Theory",
    "section": "7.4. Euler’s Theorem",
    "text": "7.4. Euler’s Theorem\nThe main result of Section 7.4 of HTPI is Euler’s theorem. The statement of the theorem involves Euler’s totient function \\(\\varphi\\). For any positive integer \\(m\\), HTPI defines \\(\\varphi(m)\\) to be the number of elements of \\(\\mathbb{Z}/{\\equiv_m}\\) that have multiplicative inverses. In order to state and prove Euler’s theorem in Lean, our first task is to define a Lean function phi : Nat → Nat that computes the totient function.\nSince {0, 1, ..., m - 1} is a complete residue system modulo m, phi m can be described as the number of natural numbers a < m such that [a]_m is invertible. According to Theorem_7_3_7, [a]_m is invertible if and only if m and a are relatively prime, so phi m is also equal to the number of natural numbers a < m that are relatively prime to m. We begin by defining a function num_rp_below m k that counts the number of natural numbers less than k that are relatively prime to m.\ndef num_rp_below (m k : Nat) : Nat :=\n  match k with\n    | 0 => 0\n    | j + 1 => if gcd m j = 1 then (num_rp_below m j) + 1\n                else num_rp_below m j\nThis is the first time we have used an if ... then ... else expression in a Lean definition. To prove theorems about such expressions, we will need two theorems from Lean’s library, if_pos and if_neg. The #check command tells us what they say:\n\n@if_pos : ∀ {c : Prop} {h : Decidable c},\n          c → ∀ {α : Sort u_1} {t e : α}, (if c then t else e) = t\n          \n@if_neg : ∀ {c : Prop} {h : Decidable c},\n          ¬c → ∀ {α : Sort u_1} {t e : α}, (if c then t else e) = e\n\nIgnoring the implicit arguments, this tells us that if hc is a proof of a proposition c, then if_pos hc is a proof of (if c then t else e) = t, and if hnc is a proof of ¬c, then if_neg hnc is a proof of (if c then t else e) = e. (Technically, the implicit arguments say that c must be a “decidable” proposition, but we won’t worry about that detail.) We often use these theorems to evaluate an expression of the form if c then t else e as either t or e, depending on whether c is true or false.\nlemma num_rp_below_base {m : Nat} :\n    num_rp_below m 0 = 0 := by rfl\n\nlemma num_rp_below_step_rp {m j : Nat} (h : rel_prime m j) :\n    num_rp_below m (j + 1) = (num_rp_below m j) + 1 := if_pos h\n\nlemma num_rp_below_step_not_rp {m j : Nat} (h : ¬rel_prime m j) :\n    num_rp_below m (j + 1) = num_rp_below m j := if_neg h\nWe can now use num_rp_below to define the totient function.\ndef phi (m : Nat) : Nat := num_rp_below m m\n\nlemma phi_def (m : Nat) : phi m = num_rp_below m m := by rfl\n\n#eval phi 10   --Answer: 4\nWith this preparation, we can now state the theorem we will prove:\ntheorem Theorem_7_4_2 {m a : Nat} [NeZero m] (h1 : rel_prime m a) :\n    [a]_m ^ (phi m) = [1]_m\nIn preparation for proving this theorem, HTPI first shows that the set of invertible congruence classes is closed under inverses and multiplication. For our purposes, we will find it useful to prove a slightly different lemma. Note that Lean knows all of the basic algebraic laws of addition and multiplication of congruence classes, and as a result the ring tactic can be used to do algebraic reasoning in ZMod m, as illustrated in the proof below.\nlemma prod_inv_iff_inv {m : Nat} {X : ZMod m}\n    (h1 : invertible X) (Y : ZMod m) :\n    invertible (X * Y) ↔ invertible Y := by\n  apply Iff.intro\n  · -- (→)\n    assume h2 : invertible (X * Y)\n    obtain (Z : ZMod m) (h3 : X * Y * Z = [1]_m) from h2\n    apply Exists.intro (X * Z)\n    rewrite [←h3]  --Goal : Y * (X * Z) = X * Y * Z\n    ring     --Note that ring can do algebra in ZMod m\n    done\n  · -- (←)\n    assume h2 : invertible Y\n    obtain (Xi : ZMod m) (h3 : X * Xi = [1]_m) from h1\n    obtain (Yi : ZMod m) (h4 : Y * Yi = [1]_m) from h2\n    apply Exists.intro (Xi * Yi)\n    show (X * Y) * (Xi * Yi) = [1]_m from\n      calc X * Y * (Xi * Yi)\n        _ = (X * Xi) * (Y * Yi) := by ring\n        _ = [1]_m * [1]_m := by rw [h3, h4]\n        _ = [1]_m := Theorem_7_3_6_7 [1]_m\n    done\n  done\nOne of the key ideas in the proof of Theorem 7.4.2 in HTPI involves computing the product of all invertible congruence classes. To compute this product in Lean, we begin by defining a function F m : Nat → ZMod m as follows:\ndef F (m i : Nat) : ZMod m := if gcd m i = 1 then [i]_m else [1]_m\n\nlemma F_rp_def {m i : Nat} (h : rel_prime m i) :\n    F m i = [i]_m := if_pos h\n\nlemma F_not_rp_def {m i : Nat} (h : ¬rel_prime m i) :\n    F m i = [1]_m := if_neg h\nNote that F is defined as a function of two natural numbers, m and i, but as we discussed in Section 5.4, it follows that the partial application F m is a function from Nat to ZMod m.\nNow consider the product (F m 0) * (F m 1) * ... * (F m (m - 1)); we will call this the F-product. If m and i are not relatively prime, then F m i = [1]_m, and since [1]_m is the multiplicative identity element of ZMod m, the factor F m i contributes nothing to the product. Thus, the F-product is equal to the product of the factors F m i for which m and i are relatively prime. But for those values of i, F m i = [i]_m, so the product is equal to the product of all congruence classes [i]_m with m and i relatively prime. By Theorem_7_3_7, that is the product of all invertible congruence classes.\nTo express the F-product in Lean, we imitate our approach to summations, as described in Chapter 6. We begin by defining prod_seq j k f to be the product of a sequence of j consecutive values of the function f, starting with f k:\ndef prod_seq {m : Nat}\n    (j k : Nat) (f : Nat → ZMod m) : ZMod m :=\n  match j with\n    | 0 => [1]_m\n    | n + 1 => prod_seq n k f * f (k + n)\n\nlemma prod_seq_base {m : Nat}\n    (k : Nat) (f : Nat → ZMod m) : prod_seq 0 k f = [1]_m := by rfl\n\nlemma prod_seq_step {m : Nat}\n    (n k : Nat) (f : Nat → ZMod m) :\n    prod_seq (n + 1) k f = prod_seq n k f * f (k + n) := by rfl\n\nlemma prod_seq_zero_step {m : Nat}\n    (n : Nat) (f : Nat → ZMod m) :\n    prod_seq (n + 1) 0 f = prod_seq n 0 f * f n := sorry\nUsing this notation, the expression prod_seq m 0 (F m) denotes the F-product, which, as we saw earlier, is equal to the product of the invertible congruence classes.\nNow suppose a is a natural number that is relatively prime to m. The next step in the proof in HTPI is to multiply each factor in the product of the invertible congruence classes by [a]_m. To do this, we define a function G as follows:\ndef G (m a i : Nat) : Nat := (a * i) % m\nConsider the following product, which we will call the FG-product:\n\n(F m (G m a 0)) * (F m (G m a 1)) * ... * (F m (G m a (m - 1))).\n\nUsing the partial application G m a, which is a function from Nat to Nat, we can express this in Lean as prod_seq m 0 ((F m) ∘ (G m a)). To understand this product we will need two facts about G.\nlemma cc_G (m a i : Nat) : [G m a i]_m = [a]_m * [i]_m :=\n  calc [G m a i]_m\n    _ = [(a * i) % m]_m := by rfl\n    _ = [a * i]_m := (cc_eq_mod m (a * i)).symm\n    _ = [a]_m * [i]_m := (mul_class m a i).symm\n\nlemma G_rp_iff {m a : Nat} (h1 : rel_prime m a) (i : Nat) :\n    rel_prime m (G m a i) ↔ rel_prime m i := by\n  have h2 : invertible [a]_m := (Theorem_7_3_7 m a).rtl h1\n  show rel_prime m (G m a i) ↔ rel_prime m i from\n    calc rel_prime m (G m a i)\n      _ ↔ invertible [G m a i]_m := (Theorem_7_3_7 m (G m a i)).symm\n      _ ↔ invertible ([a]_m * [i]_m) := by rw [cc_G]\n      _ ↔ invertible [i]_m := prod_inv_iff_inv h2 ([i]_m)\n      _ ↔ rel_prime m i := Theorem_7_3_7 m i\n  done\nNow let’s analyze the FG-product. If i is not relatively prime to m, then by G_rp_iff, G m a i is also not relatively prime to m, so F m (G m a i) = [1]_m. As before, this means that these terms contribute nothing to the FG-product. If i is relatively prime to m, then so is G m a i, and therefore, by cc_G,\n\nF m (G m a i) = [G m a i]_m = [a]_m * [i]_m = [a]_m * (F m i).\n\nThis means that, in the FG-product, each factor contributed by a value of i that is relatively prime to m is [a]_m times the corresponding factor in the F-product. Since the number of such factors is phi m, it follows that the FG-product is [a]_m ^ (phi m) times the F-product.\nLet’s see if we can prove this in Lean.\nlemma FG_rp {m a i : Nat} (h1 : rel_prime m a) (h2 : rel_prime m i) :\n    F m (G m a i) = [a]_m * F m i := by\n  have h3 : rel_prime m (G m a i) := (G_rp_iff h1 i).rtl h2\n  show F m (G m a i) = [a]_m * F m i from\n    calc F m (G m a i)\n      _ = [G m a i]_m := F_rp_def h3\n      _ = [a]_m * [i]_m := cc_G m a i\n      _ = [a]_m * F m i := by rw [F_rp_def h2]\n  done\n\nlemma FG_not_rp {m a i : Nat} (h1 : rel_prime m a) (h2 : ¬rel_prime m i) :\n    F m (G m a i) = [1]_m := sorry\n\nlemma FG_prod {m a : Nat} (h1 : rel_prime m a) :\n    ∀ (k : Nat), prod_seq k 0 ((F m) ∘ (G m a)) =\n      [a]_m ^ (num_rp_below m k) * prod_seq k 0 (F m) := by\n  by_induc\n  · -- Base Case\n    show prod_seq 0 0 ((F m) ∘ (G m a)) =\n          [a]_m ^ (num_rp_below m 0) * prod_seq 0 0 (F m) from\n      calc prod_seq 0 0 ((F m) ∘ (G m a))\n        _ = [1]_m := prod_seq_base _ _\n        _ = [a]_m ^ 0 * [1]_m := by ring\n        _ = [a]_m ^ (num_rp_below m 0) * prod_seq 0 0 (F m) := by\n              rw [num_rp_below_base, prod_seq_base]\n    done\n  · -- Induction Step\n    fix k : Nat\n    assume ih : prod_seq k 0 ((F m) ∘ (G m a)) =\n      [a]_m ^ (num_rp_below m k) * prod_seq k 0 (F m)\n    by_cases h2 : rel_prime m k\n    · -- Case 1. h2 : rel_prime m k\n      show prod_seq (k + 1) 0 ((F m) ∘ (G m a)) =\n          [a]_m ^ (num_rp_below m (k + 1)) *\n          prod_seq (k + 1) 0 (F m) from\n        calc prod_seq (k + 1) 0 ((F m) ∘ (G m a))\n          _ = prod_seq k 0 ((F m) ∘ (G m a)) *\n              F m (G m a k) := prod_seq_zero_step _ _\n          _ = [a]_m ^ (num_rp_below m k) * prod_seq k 0 (F m) *\n              F m (G m a k) := by rw [ih]\n          _ = [a]_m ^ (num_rp_below m k) * prod_seq k 0 (F m) *\n              ([a]_m * F m k) := by rw [FG_rp h1 h2]\n          _ = [a]_m ^ ((num_rp_below m k) + 1) *\n              ((prod_seq k 0 (F m)) * F m k) := by ring\n          _ = [a]_m ^ (num_rp_below m (k + 1)) *\n              prod_seq (k + 1) 0 (F m) := by\n                rw [num_rp_below_step_rp h2, prod_seq_zero_step]\n      done\n    · -- Case 2. h2 : ¬rel_prime m k\n      show prod_seq (k + 1) 0 ((F m) ∘ (G m a)) =\n          [a]_m ^ (num_rp_below m (k + 1)) *\n          prod_seq (k + 1) 0 (F m) from\n        calc prod_seq (k + 1) 0 ((F m) ∘ (G m a))\n          _ = prod_seq k 0 ((F m) ∘ (G m a)) *\n              F m (G m a k) := prod_seq_zero_step _ _\n          _ = [a]_m ^ (num_rp_below m k) * prod_seq k 0 (F m) *\n              F m (G m a k) := by rw [ih]\n          _ = [a]_m ^ (num_rp_below m k) * prod_seq k 0 (F m) *\n              ([1]_m) := by rw [FG_not_rp h1 h2]\n          _ = [a]_m ^ (num_rp_below m k) *\n              (prod_seq k 0 (F m) * ([1]_m)) := by ring\n          _ = [a]_m ^ (num_rp_below m (k + 1)) *\n              prod_seq (k + 1) 0 (F m) := by\n                rw [num_rp_below_step_not_rp h2, prod_seq_zero_step,\n                F_not_rp_def h2]\n      done\n    done\n  done\nThe lemma FG_prod, in the case k = m, tells us that\n\nprod_seq m 0 ((F m) ∘ (G m a)) = [a]_m ^ (phi m) * prod_seq m 0 (F m).\n\nIn other words, we have proven that the FG-product is [a]_m ^ (phi m) times the F-product.\nAnd now we come to the central idea in the proof of Theorem_7_4_2: the congruence classes that are multiplied in the FG-product are exactly the same as the congruence classes multiplied in the F-product, but listed in a different order. The reason for this is that the function G m a permutes the natural numbers less than m. Since multiplication of congruence classes is commutative and associative, it follows that the FG-product and the F-product are equal.\nTo prove these claims, we first define what it means for a function to permute the natural numbers less than a natural number n.\ndef maps_below (n : Nat) (g : Nat → Nat) : Prop := ∀ i < n, g i < n\n\ndef one_one_below (n : Nat) (g : Nat → Nat) : Prop :=\n  ∀ i1 < n, ∀ i2 < n, g i1 = g i2 → i1 = i2\n\ndef onto_below (n : Nat) (g : Nat → Nat) : Prop :=\n  ∀ k < n, ∃ i < n, g i = k\n\ndef perm_below (n : Nat) (g : Nat → Nat) : Prop :=\n  maps_below n g ∧ one_one_below n g ∧ onto_below n g\nThe proofs of our next two lemmas are somewhat long. We state them now, but put off discussion of their proofs until the end of the section. The first lemma says that, if m and a are relatively prime, then G m a permutes the natural numbers less than m, and the second says that permuting the terms of a product does not change the value of the product.\nlemma G_perm_below {m a : Nat} [NeZero m]\n    (h1 : rel_prime m a) : perm_below m (G m a)\n\nlemma perm_prod {m : Nat} (f : Nat → ZMod m) :\n    ∀ (n : Nat), ∀ (g : Nat → Nat), perm_below n g →\n      prod_seq n 0 f = prod_seq n 0 (f ∘ g)\nThere is just one more fact we need before we can prove Theorem_7_4_2: all of the factors in the F-product are invertible, and therefore the F-product is invertible:\nlemma F_invertible (m i : Nat) : invertible (F m i) := by\n  by_cases h : rel_prime m i\n  · -- Case 1. h : rel_prime m i\n    rewrite [F_rp_def h]\n    show invertible [i]_m from (Theorem_7_3_7 m i).rtl h\n    done\n  · -- Case 2. h : ¬rel_prime m i\n    rewrite [F_not_rp_def h]\n    apply Exists.intro [1]_m\n    show [1]_m * [1]_m = [1]_m from Theorem_7_3_6_7 [1]_m\n    done\n  done\n\nlemma Fprod_invertible (m : Nat) :\n    ∀ (k : Nat), invertible (prod_seq k 0 (F m)) := by\n  by_induc\n  · -- Base Case\n    apply Exists.intro [1]_m\n    show prod_seq 0 0 (F m) * [1]_m = [1]_m from\n      calc prod_seq 0 0 (F m) * [1]_m\n        _ = [1]_m * [1]_m := by rw [prod_seq_base]\n        _ = [1]_m := Theorem_7_3_6_7 ([1]_m)\n    done\n  · -- Induction Step\n    fix k : Nat\n    assume ih : invertible (prod_seq k 0 (F m))\n    rewrite [prod_seq_zero_step]\n    show invertible (prod_seq k 0 (F m) * (F m k)) from\n      (prod_inv_iff_inv ih (F m k)).rtl (F_invertible m k)\n    done\n  done\nWe now have everything we need to prove Theorem_7_4_2:\ntheorem Theorem_7_4_2 {m a : Nat} [NeZero m] (h1 : rel_prime m a) :\n    [a]_m ^ (phi m) = [1]_m := by\n  have h2 : invertible (prod_seq m 0 (F m)) := Fprod_invertible m m\n  obtain (Y : ZMod m) (h3 : prod_seq m 0 (F m) * Y = [1]_m) from h2\n  show [a]_m ^ (phi m) = [1]_m from\n    calc [a]_m ^ (phi m)\n      _ = [a]_m ^ (phi m) * [1]_m := (Theorem_7_3_6_7 _).symm\n      _ = [a]_m ^ (phi m) * (prod_seq m 0 (F m) * Y) := by rw [h3]\n      _ = ([a]_m ^ (phi m) * prod_seq m 0 (F m)) * Y := by ring\n      _ = prod_seq m 0 (F m ∘ G m a) * Y := by rw [FG_prod h1 m, phi_def]\n      _ = prod_seq m 0 (F m) * Y := by\n            rw [perm_prod (F m) m (G m a) (G_perm_below h1)]\n      _ = [1]_m := by rw [h3]\n  done\nRephrasing this theorem in terms of numbers gives us the usual statement of Euler’s theorem:\nlemma Exercise_7_4_5_Int (m : Nat) (a : Int) :\n    ∀ (n : Nat), [a]_m ^ n = [a ^ n]_m := sorry\n\nlemma Exercise_7_4_5_Nat (m a n : Nat) :\n    [a]_m ^ n = [a ^ n]_m := by\n  rewrite [Exercise_7_4_5_Int]\n  rfl\n  done\n\ntheorem Euler's_theorem {m a : Nat} [NeZero m]\n    (h1 : rel_prime m a) : a ^ (phi m) ≡ 1 (MOD m) := by\n  have h2 : [a]_m ^ (phi m) = [1]_m := Theorem_7_4_2 h1\n  rewrite [Exercise_7_4_5_Nat m a (phi m)] at h2\n    --h2 : [a ^ phi m]_m = [1]_m\n  show a ^ (phi m) ≡ 1 (MOD m) from (cc_eq_iff_congr _ _ _).ltr h2\n  done\n\n#eval gcd 10 7     --Answer: 1.  So 10 and 7 are relatively prime\n\n#eval 7 ^ phi 10   --Answer: 2401, which is congruent to 1 mod 10.\nWe now turn to the two lemmas whose proofs we skipped over, G_perm_below and perm_prod. To prove G_perm_below, we must prove three facts: maps_below m (G m a), one_one_below m (G m a), and onto_below m (G m a). The first is straightforward:\nlemma G_maps_below (m a : Nat) [NeZero m] : maps_below m (G m a) := by\n  define             --Goal : ∀ i < m, G m a i < m\n  fix i : Nat\n  assume h1 : i < m\n  rewrite [G_def]    --Goal : a * i % m < m\n  show a * i % m < m from mod_nonzero_lt (a * i) (NeZero.ne m)\n  done\nFor the second and third, we start with lemmas that are reminiscent of Theorem 5.3.3.\nlemma right_inv_onto_below {n : Nat} {g g' : Nat → Nat}\n    (h1 : ∀ i < n, g (g' i) = i) (h2 : maps_below n g') :\n    onto_below n g := by\n  define at h2; define\n  fix k : Nat\n  assume h3 : k < n\n  apply Exists.intro (g' k)\n  show g' k < n ∧ g (g' k) = k from And.intro (h2 k h3) (h1 k h3)\n  done\n\nlemma left_inv_one_one_below {n : Nat} {g g' : Nat → Nat}\n    (h1 : ∀ i < n, g' (g i) = i) : one_one_below n g := sorry\nTo apply these lemmas with g = G m a, we need a function to play the role of g'. A natural choice is G m a', where a' is chosen so that [a']_m is the multiplicative inverse of [a]_m. We know from earlier work that if m and a are relatively prime then the multiplicative inverse of [a]_m is [gcd_c2 m a]_m. However, in the notation G m a', we can’t let a' = gcd_c2 m a, because a' must be a natural number and gcd_c2 a is an integer. And we can’t simply coerce an integer to be a natural number—what if it’s negative? But we know [gcd_c2 m a]_m = [(gcd_c2 m a) % m]_m and 0 ≤ (gcd_c2 m a) % m, and there is a function, Int.toNat, that will convert a nonnegative integer to a natural number. So we make the following definitions:\ndef inv_mod (m a : Nat) : Nat := Int.toNat ((gcd_c2 m a) % m)\n\ndef Ginv (m a i : Nat) : Nat := G m (inv_mod m a) i\nNow Ginv m a can play the role of g' in the last two lemmas. We’ll skip the details and just summarize the results.\nlemma Ginv_right_inv {m a : Nat} [NeZero m] (h1 : rel_prime m a) :\n    ∀ i < m, G m a (Ginv m a i) = i := sorry\n\nlemma Ginv_left_inv {m a : Nat} [NeZero m] (h1 : rel_prime m a) :\n    ∀ i < m, Ginv m a (G m a i) = i := sorry\n\nlemma Ginv_maps_below (m a : Nat) [NeZero m] :\n    maps_below m (Ginv m a) := G_maps_below m (inv_mod m a)\n\nlemma G_one_one_below {m a : Nat} [NeZero m] (h1 : rel_prime m a) :\n    one_one_below m (G m a) :=\n  left_inv_one_one_below (Ginv_left_inv h1)\n\nlemma G_onto_below {m a : Nat} [NeZero m] (h1 : rel_prime m a) :\n    onto_below m (G m a) :=\n  right_inv_onto_below (Ginv_right_inv h1) (Ginv_maps_below m a)\n\nlemma G_perm_below {m a : Nat} [NeZero m] (h1 : rel_prime m a) :\n    perm_below m (G m a) := And.intro (G_maps_below m a)\n  (And.intro (G_one_one_below h1) (G_onto_below h1))\nFinally, we turn to the proof of perm_prod. Our proof will be by mathematical induction. In the induction step, our induction hypothesis will be\n\n∀ (g : Nat → Nat), perm_below n g →\n    prod_seq n 0 f = prod_seq n 0 (f ∘ g),\n\nand we will have to prove\n\n∀ (g : Nat → Nat), perm_below (n + 1) g →\n    prod_seq (n + 1) 0 f = prod_seq (n + 1) 0 (f ∘ g).\n\nTo prove this, we’ll introduce an arbitrary function g : Nat → Nat and assume perm_below (n + 1) g. How can we make use of the inductive hypothesis? Here’s the key idea: Since g permutes the numbers below n + 1, there must be some u ≤ n such that g u = n. Now let s be a function that swaps u and n, but leaves all other numbers fixed. In other words, s u = n, s n = u, and s i = i if i ≠ u and i ≠ n. It is not hard to show that s permutes the numbers below n + 1, and using that fact we can prove that g ∘ s permutes the numbers below n + 1. But notice that\n\n(g ∘ s) n = g (s n) = g u = n.\n\nIn other words, g ∘ s leaves n fixed. Using that fact, we’ll be able to prove that g ∘ s permutes the numbers below n. We can therefore apply the inductive hypothesis to g ∘ s, which leads to the conclusion\n\nprod_seq n 0 f = prod_seq n 0 (f ∘ g ∘ s).\n\nSince we also have (f ∘ g ∘ s) n = f ((g ∘ s) n) = f n, we can extend this to\n\nprod_seq (n + 1) 0 f = prod_seq (n + 1) 0 (f ∘ g ∘ s).\n\nFinally, we can then reach the required conclusion by proving\n\nprod_seq (n + 1) 0 (f ∘ g ∘ s) = prod_seq (n + 1) 0 (f ∘ g).\n\nTo carry out this plan, we begin by defining our “swapping” function and proving its properties.\ndef swap (u v i : Nat) : Nat :=\n  if i = u then v else if i = v then u else i\n\nlemma swap_fst (u v : Nat) : swap u v u = v := by\n  define : swap u v u\n    --Goal : (if u = u then v else if u = v then u else u) = v\n  have h : u = u := by rfl\n  rewrite [if_pos h]\n  rfl\n  done\n\nlemma swap_snd (u v : Nat) : swap u v v = u := sorry\n\nlemma swap_perm_below {u v n} (h1 : u < n) (h2 : v < n) :\n    perm_below n (swap u v) := sorry\nFor the swapping function s in the proof outline above, we’ll use swap u n. To prove that g ∘ swap u n permutes the numbers below n, we’ll need two lemmas:\nlemma comp_perm_below {n : Nat} {f g : Nat → Nat}\n    (h1 : perm_below n f) (h2 : perm_below n g) :\n    perm_below n (f ∘ g) := sorry\n\nlemma perm_below_fixed {n : Nat} {g : Nat → Nat}\n    (h1 : perm_below (n + 1) g) (h2 : g n = n) : perm_below n g := sorry\nFor the final step of the proof, we’ll need several lemmas\nlemma break_prod_twice  {m u j n : Nat} (f : Nat → ZMod m)\n    (h1 : n = u + 1 + j) : prod_seq (n + 1) 0 f =\n      prod_seq u 0 f * f u * prod_seq j (u + 1) f * f n := sorry\n\nlemma swap_prod_eq_prod_below {m u n : Nat} (f : Nat → ZMod m)\n    (h1 : u ≤ n) : prod_seq u 0 (f ∘ swap u n) = prod_seq u 0 f := sorry\n\nlemma swap_prod_eq_prod_between {m u j n : Nat} (f : Nat → ZMod m)\n    (h1 : n = u + 1 + j) : prod_seq j (u + 1) (f ∘ swap u n) =\n      prod_seq j (u + 1) f := sorry\n\nlemma trivial_swap (u : Nat) : swap u u = id := sorry\nUsing these lemmas, we can prove the fact we’ll need in the final step.\nlemma swap_prod_eq_prod {m u n : Nat} (f : Nat → ZMod m) (h1 : u ≤ n) :\n    prod_seq (n + 1) 0 (f ∘ swap u n) = prod_seq (n + 1) 0 f := by\n  by_cases h2 : u = n\n  · -- Case 1. h2 : u = n\n    rewrite [h2, trivial_swap n]\n      --Goal : prod_seq (n + 1) 0 (f ∘ id) = prod_seq (n + 1) 0 f\n    rfl\n    done\n  · -- Case 2. h2 : ¬u = n\n    have h3 : u + 1 ≤ n := Nat.lt_of_le_of_ne h1 h2\n    obtain (j : Nat) (h4 : n = u + 1 + j) from Nat.exists_eq_add_of_le h3\n    have break_f : prod_seq (n + 1) 0 f =\n      prod_seq u 0 f * f u * prod_seq j (u + 1) f * f n :=\n      break_prod_twice f h4\n    have break_fs : prod_seq (n + 1) 0 (f ∘ swap u n) =\n      prod_seq u 0 (f ∘ swap u n) * (f ∘ swap u n) u *\n      prod_seq j (u + 1) (f ∘ swap u n) * (f ∘ swap u n) n :=\n      break_prod_twice (f ∘ swap u n) h4\n    have f_eq_fs_below : prod_seq u 0 (f ∘ swap u n) =\n      prod_seq u 0 f := swap_prod_eq_prod_below f h1\n    have f_eq_fs_btwn : prod_seq j (u + 1) (f ∘ swap u n) =\n      prod_seq j (u + 1) f := swap_prod_eq_prod_between f h4\n    show prod_seq (n + 1) 0 (f ∘ swap u n) = prod_seq (n + 1) 0 f from\n      calc prod_seq (n + 1) 0 (f ∘ swap u n)\n        _ = prod_seq u 0 (f ∘ swap u n) * (f ∘ swap u n) u *\n            prod_seq j (u + 1) (f ∘ swap u n) * (f ∘ swap u n) n :=\n              break_fs\n        _ = prod_seq u 0 f * (f ∘ swap u n) u *\n            prod_seq j (u + 1) f * (f ∘ swap u n) n := by\n              rw [f_eq_fs_below, f_eq_fs_btwn]\n        _ = prod_seq u 0 f * f (swap u n u) *\n            prod_seq j (u + 1) f * f (swap u n n) := by rfl\n        _ = prod_seq u 0 f * f n * prod_seq j (u + 1) f * f u := by\n              rw [swap_fst, swap_snd]\n        _ = prod_seq u 0 f * f u * prod_seq j (u + 1) f * f n := by ring\n        _ = prod_seq (n + 1) 0 f := break_f.symm\n    done\n  done\nWe finally have all the pieces in place to prove perm_prod:\nlemma perm_prod {m : Nat} (f : Nat → ZMod m) :\n    ∀ (n : Nat), ∀ (g : Nat → Nat), perm_below n g →\n      prod_seq n 0 f = prod_seq n 0 (f ∘ g) := by\n  by_induc\n  · -- Base Case\n    fix g : Nat → Nat\n    assume h1 : perm_below 0 g\n    rewrite [prod_seq_base, prod_seq_base]\n    rfl\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : ∀ (g : Nat → Nat), perm_below n g →\n      prod_seq n 0 f = prod_seq n 0 (f ∘ g)\n    fix g : Nat → Nat\n    assume g_pb : perm_below (n + 1) g\n    define at g_pb\n    have g_ob : onto_below (n + 1) g := g_pb.right.right\n    define at g_ob\n    have h1 : n < n + 1 := by linarith\n    obtain (u : Nat) (h2 : u < n + 1 ∧ g u = n) from g_ob n h1\n    have s_pb : perm_below (n + 1) (swap u n) :=\n      swap_perm_below h2.left h1\n    have gs_pb_n1 : perm_below (n + 1) (g ∘ swap u n) :=\n      comp_perm_below g_pb s_pb\n    have gs_fix_n : (g ∘ swap u n) n = n :=\n      calc (g ∘ swap u n) n\n        _ = g (swap u n n) := by rfl\n        _ = g u := by rw [swap_snd]\n        _ = n := h2.right\n    have gs_pb_n : perm_below n (g ∘ swap u n) :=\n      perm_below_fixed gs_pb_n1 gs_fix_n\n    have gs_prod : prod_seq n 0 f = prod_seq n 0 (f ∘ (g ∘ swap u n)) :=\n      ih (g ∘ swap u n) gs_pb_n\n    have h3 : u ≤ n := by linarith\n    show prod_seq (n + 1) 0 f = prod_seq (n + 1) 0 (f ∘ g) from\n      calc prod_seq (n + 1) 0 f\n        _ = prod_seq n 0 f * f n := prod_seq_zero_step n f\n        _ = prod_seq n 0 (f ∘ (g ∘ swap u n)) *\n            f ((g ∘ swap u n) n) := by rw [gs_prod, gs_fix_n]\n        _ = prod_seq n 0 (f ∘ g ∘ swap u n) *\n            (f ∘ g ∘ swap u n) n := by rfl\n        _ = prod_seq (n + 1) 0 (f ∘ g ∘ swap u n) :=\n              (prod_seq_zero_step n (f ∘ g ∘ swap u n)).symm\n        _ = prod_seq (n + 1) 0 ((f ∘ g) ∘ swap u n) := by rfl\n        _ = prod_seq (n + 1) 0 (f ∘ g) := swap_prod_eq_prod (f ∘ g) h3\n    done\n  done\nThere is one more theorem that is proven in Section 7.4 of HTPI: Theorem 7.4.4, which says that \\(\\varphi\\) is a multiplicative function. The proof requires ideas that we will not develop in Lean until Chapter 8, so we will put off the proof until Section 8.1½.\n\nExercises\n\n--Hint:  Use induction.\n--For the base case, compute [a]_m ^ 0 * [1]_m in two ways:\n--by Theorem_7_3_6_7, [a] ^ 0 * [1]_m = [a]_m ^ 0\n--by ring, [a]_m ^ 0 * [1]_m = [1]_m.\nlemma Exercise_7_4_5_Int (m : Nat) (a : Int) :\n    ∀ (n : Nat), [a]_m ^ n = [a ^ n]_m := sorry\n\n\nlemma left_inv_one_one_below {n : Nat} {g g' : Nat → Nat}\n    (h1 : ∀ i < n, g' (g i) = i) : one_one_below n g := sorry\n\n\nlemma comp_perm_below {n : Nat} {f g : Nat → Nat}\n    (h1 : perm_below n f) (h2 : perm_below n g) :\n    perm_below n (f ∘ g) := sorry\n\n\nlemma perm_below_fixed {n : Nat} {g : Nat → Nat}\n    (h1 : perm_below (n + 1) g) (h2 : g n = n) : perm_below n g := sorry\n\n\nlemma Lemma_7_4_6 {a b c : Nat} :\n    rel_prime (a * b) c ↔ rel_prime a c ∧ rel_prime b c := sorry\n\n\nexample {m a : Nat} [NeZero m] (h1 : rel_prime m a) :\n    a ^ (phi m + 1) ≡ a (MOD m) := sorry\n\n\ntheorem Like_Exercise_7_4_11 {m a p : Nat} [NeZero m]\n    (h1 : rel_prime m a) (h2 : p + 1 = phi m) :\n    [a]_m * [a ^ p]_m = [1]_m := sorry\n\n\ntheorem Like_Exercise_7_4_12 {m a p q k : Nat} [NeZero m]\n    (h1 : rel_prime m a) (h2 : p = q + (phi m) * k) :\n    a ^ p ≡ a ^ q (MOD m) := sorry"
  },
  {
    "objectID": "Chap7.html#public-key-cryptography",
    "href": "Chap7.html#public-key-cryptography",
    "title": "7  Number Theory",
    "section": "7.5. Public-Key Cryptography",
    "text": "7.5. Public-Key Cryptography\nSection 7.5 of HTPI discusses the RSA public-key cryptography system. The system is based on the following theorem:\ntheorem Theorem_7_5_1 (p q n e d k m c : Nat)\n    (p_prime : prime p) (q_prime : prime q) (p_ne_q : p ≠ q)\n    (n_pq : n = p * q) (ed_congr_1 : e * d = k * (p - 1) * (q - 1) + 1)\n    (h1 : [m]_n ^ e = [c]_n) : [c]_n ^ d = [m]_n\nFor an explanation of how the RSA system works and why Theorem_7_5_1 justifies it, see HTPI. Here we will focus on proving the theorem in Lean.\nWe will be applying Euler’s theorem to the prime numbers p and q, so we will need to know how to compute phi p and phi q. Fortunately, there is a simple formula we can use.\nlemma num_rp_prime {p : Nat} (h1 : prime p) :\n    ∀ k < p, num_rp_below p (k + 1) = k := sorry\n\nlemma phi_prime {p : Nat} (h1 : prime p) : phi p = p - 1 := by\n  have h2 : 1 ≤ p := prime_pos h1\n  have h3 : p - 1 + 1 = p := Nat.sub_add_cancel h2\n  have h4 : p - 1 < p := by linarith\n  have h5 : num_rp_below p (p - 1 + 1) = p - 1 :=\n    num_rp_prime h1 (p - 1) h4\n  rewrite [h3] at h5\n  show phi p = p - 1 from h5\n  done\nWe will also need to use Lemma 7.4.5 from HTPI. To prove that lemma in Lean, we will use Theorem_7_2_2, which says that for natural numbers a, b, and c, if c ∣ a * b and c and a are relatively prime, then c ∣ b. But we will need to extend the theorem to allow b to be an integer rather than a natural number. To prove this extension, we use the Lean function Int.natAbs : Int → Nat, which computes the absolute value of an integer. Lean knows several theorems about this function:\n\n@Int.natCast_dvd : ∀ {n : ℤ} {m : ℕ}, ↑m ∣ n ↔ m ∣ Int.natAbs n\n\nInt.natAbs_mul : ∀ (a b : ℤ),\n                  Int.natAbs (a * b) = Int.natAbs a * Int.natAbs b\n\nInt.natAbs_ofNat : ∀ (n : ℕ), Int.natAbs ↑n = n\n\nWith the help of these theorems, our extended version of Theorem_7_2_2 follows easily from the original version:\ntheorem Theorem_7_2_2_Int {a c : Nat} {b : Int}\n    (h1 : ↑c ∣ ↑a * b) (h2 : rel_prime a c) : ↑c ∣ b := by\n  rewrite [Int.natCast_dvd, Int.natAbs_mul,\n    Int.natAbs_ofNat] at h1        --h1 : c ∣ a * Int.natAbs b\n  rewrite [Int.natCast_dvd]        --Goal : c ∣ Int.natAbs b\n  show c ∣ Int.natAbs b from Theorem_7_2_2 h1 h2\n  done\nWith that preparation, we can now prove Lemma_7_4_5.\nlemma Lemma_7_4_5 {m n : Nat} (a b : Int) (h1 : rel_prime m n) :\n    a ≡ b (MOD m * n) ↔ a ≡ b (MOD m) ∧ a ≡ b (MOD n) := by\n  apply Iff.intro\n  · -- (→)\n    assume h2 : a ≡ b (MOD m * n)\n    obtain (j : Int) (h3 : a - b = (m * n) * j) from h2\n    apply And.intro\n    · -- Proof of a ≡ b (MOD m)\n      apply Exists.intro (n * j)\n      show a - b = m * (n * j) from\n        calc a - b\n          _ = m * n * j := h3\n          _ = m * (n * j) := by ring\n      done\n    · -- Proof of a ≡ b (MOD n)\n      apply Exists.intro (m * j)\n      show a - b = n * (m * j) from\n        calc a - b\n          _ = m * n * j := h3\n          _ = n * (m * j) := by ring\n      done\n    done\n  · -- (←)\n    assume h2 : a ≡ b (MOD m) ∧ a ≡ b (MOD n)\n    obtain (j : Int) (h3 : a - b = m * j) from h2.left\n    have h4 : (↑n : Int) ∣ a - b := h2.right\n    rewrite [h3] at h4      --h4 : ↑n ∣ ↑m * j\n    have h5 : ↑n ∣ j := Theorem_7_2_2_Int h4 h1\n    obtain (k : Int) (h6 : j = n * k) from h5\n    apply Exists.intro k    --Goal : a - b = ↑(m * n) * k\n    rewrite [Nat.cast_mul]  --Goal : a - b = ↑m * ↑n * k\n    show a - b = (m * n) * k from\n      calc a - b\n        _ = m * j := h3\n        _ = m * (n * k) := by rw [h6]\n        _ = (m * n) * k := by ring\n    done\n  done\nFinally, we will need an exercise from Section 7.2, and we will need to know NeZero p for prime numbers p:\ntheorem rel_prime_symm {a b : Nat} (h : rel_prime a b) :\n    rel_prime b a := sorry\n\nlemma prime_NeZero {p : Nat} (h : prime p) : NeZero p := by\n  rewrite [neZero_iff]     --Goal : p ≠ 0\n  define at h\n  linarith\n  done\nMuch of the reasoning about modular arithmetic that we need for the proof of Theorem_7_5_1 is contained in a technical lemma:\nlemma Lemma_7_5_1 {p e d m c s : Nat} {t : Int}\n    (h1 : prime p) (h2 : e * d = (p - 1) * s + 1)\n    (h3 : m ^ e - c = p * t) :\n    c ^ d ≡ m (MOD p) := by\n  have h4 : m ^ e ≡ c (MOD p) := Exists.intro t h3\n  have h5 : [m ^ e]_p = [c]_p := (cc_eq_iff_congr _ _ _).rtl h4\n  rewrite [←Exercise_7_4_5_Nat] at h5  --h5 : [m]_p ^ e = [c]_p\n  by_cases h6 : p ∣ m\n  · -- Case 1. h6 : p ∣ m\n    have h7 : m ≡ 0 (MOD p) := by\n      obtain (j : Nat) (h8 : m = p * j) from h6\n      apply Exists.intro (↑j : Int)   --Goal : ↑m - 0 = ↑p * ↑j\n      rewrite [h8, Nat.cast_mul]\n      ring\n      done\n    have h8 : [m]_p = [0]_p := (cc_eq_iff_congr _ _ _).rtl h7\n    have h9 : e * d ≠ 0 := by\n      rewrite [h2]\n      show (p - 1) * s + 1 ≠ 0 from Nat.add_one_ne_zero _\n      done\n    have h10 : (0 : Int) ^ (e * d) = 0 := zero_pow h9\n    have h11 : [c ^ d]_p = [m]_p :=\n      calc [c ^ d]_p\n        _ = [c]_p ^ d := by rw [Exercise_7_4_5_Nat]\n        _ = ([m]_p ^ e) ^ d := by rw [h5]\n        _ = [m]_p ^ (e * d) := by ring\n        _ = [0]_p ^ (e * d) := by rw [h8]\n        _ = [0 ^ (e * d)]_p := Exercise_7_4_5_Int _ _ _\n        _ = [0]_p := by rw [h10]\n        _ = [m]_p := by rw [h8]\n    show c ^ d ≡ m (MOD p) from (cc_eq_iff_congr _ _ _).ltr h11\n    done\n  · -- Case 2. h6 : ¬p ∣ m\n    have h7 : rel_prime m p := rel_prime_of_prime_not_dvd h1 h6\n    have h8 : rel_prime p m := rel_prime_symm h7\n    have h9 : NeZero p := prime_NeZero h1\n    have h10 : (1 : Int) ^ s = 1 := by ring\n    have h11 : [c ^ d]_p = [m]_p :=\n      calc [c ^ d]_p\n        _ = [c]_p ^ d := by rw [Exercise_7_4_5_Nat]\n        _ = ([m]_p ^ e) ^ d := by rw [h5]\n        _ = [m]_p ^ (e * d) := by ring\n        _ = [m]_p ^ ((p - 1) * s + 1) := by rw [h2]\n        _ = ([m]_p ^ (p - 1)) ^ s * [m]_p := by ring\n        _ = ([m]_p ^ (phi p)) ^ s * [m]_p := by rw [phi_prime h1]\n        _ = [1]_p ^ s * [m]_p := by rw [Theorem_7_4_2 h8]\n        _ = [1 ^ s]_p * [m]_p := by rw [Exercise_7_4_5_Int]\n        _ = [1]_p * [m]_p := by rw [h10]\n        _ = [m]_p * [1]_p := by ring\n        _ = [m]_p := Theorem_7_3_6_7 _\n    show c ^ d ≡ m (MOD p) from (cc_eq_iff_congr _ _ _).ltr h11\n    done\n  done\nHere, finally, is the proof of Theorem_7_5_1:\ntheorem Theorem_7_5_1 (p q n e d k m c : Nat)\n    (p_prime : prime p) (q_prime : prime q) (p_ne_q : p ≠ q)\n    (n_pq : n = p * q) (ed_congr_1 : e * d = k * (p - 1) * (q - 1) + 1)\n    (h1 : [m]_n ^ e = [c]_n) : [c]_n ^ d = [m]_n := by\n  rewrite [Exercise_7_4_5_Nat, cc_eq_iff_congr] at h1\n    --h1 : m ^ e ≡ c (MOD n)\n  rewrite [Exercise_7_4_5_Nat, cc_eq_iff_congr]\n    --Goal : c ^ d ≡ m (MOD n)\n  obtain (j : Int) (h2 : m ^ e - c = n * j) from h1\n  rewrite [n_pq, Nat.cast_mul] at h2\n    --h2 : m ^ e - c = p * q * j\n  have h3 : e * d = (p - 1) * (k * (q - 1)) + 1 := by\n    rewrite [ed_congr_1]\n    ring\n    done\n  have h4 : m ^ e - c = p * (q * j) := by\n    rewrite [h2]\n    ring\n    done\n  have congr_p : c ^ d ≡ m (MOD p) := Lemma_7_5_1 p_prime h3 h4\n  have h5 : e * d = (q - 1) * (k * (p - 1)) + 1 := by\n    rewrite [ed_congr_1]\n    ring\n    done\n  have h6 : m ^ e - c = q * (p * j) := by\n    rewrite [h2]\n    ring\n    done\n  have congr_q : c ^ d ≡ m (MOD q) := Lemma_7_5_1 q_prime h5 h6\n  have h7 : ¬q ∣ p := by\n    by_contra h8\n    have h9 : q = 1 ∨ q = p := dvd_prime p_prime h8\n    disj_syll h9 (prime_not_one q_prime)\n    show False from p_ne_q h9.symm\n    done\n  have h8 : rel_prime p q := rel_prime_of_prime_not_dvd q_prime h7\n  rewrite [n_pq, Lemma_7_4_5 _ _ h8]\n  show c ^ d ≡ m (MOD p) ∧ c ^ d ≡ m (MOD q) from\n    And.intro congr_p congr_q\n  done\n\nExercises\n\n--Hint:  Use induction.\nlemma num_rp_prime {p : Nat} (h1 : prime p) :\n    ∀ k < p, num_rp_below p (k + 1) = k := sorry\n\n\nlemma three_prime : prime 3 := sorry\n\n\n--Hint:  Use the previous exercise, Exercise_7_2_7, and Theorem_7_4_2.\ntheorem Exercise_7_5_13a (a : Nat) (h1 : rel_prime 561 a) :\n    a ^ 560 ≡ 1 (MOD 3) := sorry\n\n\n--Hint:  Imitate the way Theorem_7_2_2_Int was proven from Theorem_7_2_2.\nlemma Theorem_7_2_3_Int {p : Nat} {a b : Int}\n    (h1 : prime p) (h2 : ↑p ∣ a * b) : ↑p ∣ a ∨ ↑p ∣ b := sorry\n\n\n--Hint:  Use the previous exercise.\ntheorem Exercise_7_5_14b (n : Nat) (b : Int)\n    (h1 : prime n) (h2 : b ^ 2 ≡ 1 (MOD n)) :\n    b ≡ 1 (MOD n) ∨ b ≡ -1 (MOD n) := sorry"
  },
  {
    "objectID": "Chap8.html",
    "href": "Chap8.html",
    "title": "8  Infinite Sets",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\mathbin{∆}}\n$$"
  },
  {
    "objectID": "Chap8.html#equinumerous-sets",
    "href": "Chap8.html#equinumerous-sets",
    "title": "8  Infinite Sets",
    "section": "8.1. Equinumerous Sets",
    "text": "8.1. Equinumerous Sets\nChapter 8 of HTPI begins by defining a set \\(A\\) to be equinumerous with a set \\(B\\) if there is a function \\(f : A \\to B\\) that is one-to-one and onto. But in Lean, a function must go from a type to a type, not a set to a set. Thus, when we translate the HTPI definition into Lean’s language, we end up with a definition that tells us when one type is equinumerous with another. Throughout this chapter, we will use the letters U, V, … for types and A, B, … for sets, so we state the definition of equinumerous like this in Lean:\ndef equinum (U V : Type) : Prop :=\n  ∃ (f : U → V), one_to_one f ∧ onto f\nAs in HTPI, we introduce the notation U ∼ V to indicate that U is equinumerous with V (to enter the symbol ∼, type \\sim or \\~).\nnotation:50  U:50 \" ∼ \" V:50 => equinum U V\nSection 8.1 of HTPI begins the study of this concept with some examples. The first is a one-to-one, onto function from \\(\\mathbb{Z}^+\\) to \\(\\mathbb{Z}\\), which shows that \\(\\mathbb{Z}^+ \\sim \\mathbb{Z}\\). We will modify this example slightly to make it a function fnz from Nat to Int:\ndef fnz (n : Nat) : Int := if 2 ∣ n then ↑(n / 2) else -↑((n + 1) / 2)\nNote that, to get a result of type Int, coercion is necessary. We have specified that the coercion should be done after the computation of either n / 2 or (n + 1) / 2, with that computation being done using natural-number arithmetic. Checking a few values of this functions suggests a simple pattern:\n#eval [fnz 0, fnz 1, fnz 2, fnz 3, fnz 4, fnz 5, fnz 6]\n  --Answer: [0, -1, 1, -2, 2, -3, 3]\nPerhaps the easiest way to prove that fnz is one-to-one and onto is to define a function that turns out to be its inverse. This time, in order to get the right type for the value of the function, we use the function Int.toNat to convert a nonnegative integer to a natural number.\ndef fzn (a : Int) : Nat :=\n  if a ≥ 0 then 2 * Int.toNat a else 2 * Int.toNat (-a) - 1\n\n#eval [fzn 0, fzn (-1), fzn 1, fzn (-2), fzn 2, fzn (-3), fzn 3]\n  --Answer: [0, 1, 2, 3, 4, 5, 6]\nTo prove that fzn is the inverse of fnz, we begin by proving lemmas making it easier to compute the values of these functions\nlemma fnz_even (k : Nat) : fnz (2 * k) = ↑k := by\n  have h1 : 2 ∣ 2 * k := by\n    apply Exists.intro k\n    rfl\n    done\n  have h2 : 0 < 2 := by linarith\n  show fnz (2 * k) = ↑k from\n    calc fnz (2 * k)\n      _ = ↑(2 * k / 2) := if_pos h1\n      _ = ↑k := by rw [Nat.mul_div_cancel_left k h2]\n  done\n\nlemma fnz_odd (k : Nat) : fnz (2 * k + 1) = -↑(k + 1) := sorry\n\nlemma fzn_nat (k : Nat) : fzn ↑k = 2 * k := by rfl\n\nlemma fzn_neg_succ_nat (k : Nat) : fzn (-↑(k + 1)) = 2 * k + 1 := by rfl\nUsing these lemmas and reasoning by cases, it is straightforward to prove lemmas confirming that the composition of these functions, in either order, yields the identity function. The cases for the first lemma are based on an exercise from Section 6.1.\nlemma fzn_fnz : fzn ∘ fnz = id := by\n  apply funext        --Goal : ∀ (x : Nat), (fzn ∘ fnz) x = id x\n  fix n : Nat\n  rewrite [comp_def]  --Goal : fzn (fnz n) = id n\n  have h1 : nat_even n ∨ nat_odd n := Exercise_6_1_16a1 n\n  by_cases on h1\n  · -- Case 1. h1 : nat_even n\n    obtain (k : Nat) (h2 : n = 2 * k) from h1\n    rewrite [h2, fnz_even, fzn_nat]\n    rfl\n    done\n  · -- Case 2. h1 : nat_odd n\n    obtain (k : Nat) (h2 : n = 2 * k + 1) from h1\n    rewrite [h2, fnz_odd, fzn_neg_succ_nat]\n    rfl\n    done\n  done\n\nlemma fnz_fzn : fnz ∘ fzn = id  := sorry\nBy theorems from Chapter 5, it follows that both fnz and fzn are one-to-one and onto.\nlemma fzn_one_one : one_to_one fzn := Theorem_5_3_3_1 fzn fnz fnz_fzn\n\nlemma fzn_onto : onto fzn := Theorem_5_3_3_2 fzn fnz fzn_fnz\n\nlemma fnz_one_one : one_to_one fnz := Theorem_5_3_3_1 fnz fzn fzn_fnz\n\nlemma fnz_onto : onto fnz := Theorem_5_3_3_2 fnz fzn fnz_fzn\nWe conclude that Nat ∼ Int and Int ∼ Nat:\ntheorem N_equinum_Z : Nat ∼ Int :=\n  Exists.intro fnz (And.intro fnz_one_one fnz_onto)\n\ntheorem Z_equinum_N : Int ∼ Nat :=\n  Exists.intro fzn (And.intro fzn_one_one fzn_onto)\nWe’ll give one more example: a one-to-one, onto function fnnn from Nat × Nat to Nat, whose definition is modeled on a function from \\(\\mathbb{Z}^+ \\times \\mathbb{Z}^+\\) to \\(\\mathbb{Z}^+\\) in HTPI. The definition of fnnn will use numbers of the form k * (k + 1) / 2. These numbers are sometimes called triangular numbers, because they count the number of objects in a triangular grid with k rows.\ndef tri (k : Nat) : Nat := k * (k + 1) / 2\n\ndef fnnn (p : Nat × Nat) : Nat := tri (p.1 + p.2) + p.1\n\nlemma fnnn_def (a b : Nat) : fnnn (a, b) = tri (a + b) + a := by rfl\n\n#eval [fnnn (0, 0), fnnn (0, 1), fnnn (1, 0),\n  fnnn (0, 2), fnnn (1, 1), fnnn (2, 0)]\n  --Answer: [0, 1, 2, 3, 4, 5]\nTwo simple lemmas about tri, whose proofs we leave as exercises for you, help us prove the important properties of fnnn:\nlemma tri_step (k : Nat) : tri (k + 1) = tri k + k + 1 := sorry\n\nlemma tri_incr {j k : Nat} (h1 : j ≤ k) : tri j ≤ tri k := sorry\n\nlemma le_of_fnnn_eq {a1 b1 a2 b2 : Nat}\n    (h1 : fnnn (a1, b1) = fnnn (a2, b2)) : a1 + b1 ≤ a2 + b2 := by\n  by_contra h2\n  have h3 : a2 + b2 + 1 ≤ a1 + b1 := by linarith\n  have h4 : fnnn (a2, b2) < fnnn (a1, b1) :=\n    calc fnnn (a2, b2)\n      _ = tri (a2 + b2) + a2 := by rfl\n      _ < tri (a2 + b2) + (a2 + b2) + 1 := by linarith\n      _ = tri (a2 + b2 + 1) := (tri_step _).symm\n      _ ≤ tri (a1 + b1) := tri_incr h3\n      _ ≤ tri (a1 + b1) + a1 := by linarith\n      _ = fnnn (a1, b1) := by rfl\n  linarith\n  done\n\nlemma fnnn_one_one : one_to_one fnnn := by\n  fix (a1, b1) : Nat × Nat\n  fix (a2, b2) : Nat × Nat\n  assume h1 : fnnn (a1, b1) = fnnn (a2, b2)  --Goal : (a1, b1) = (a2, b2)\n  have h2 : a1 + b1 ≤ a2 + b2 := le_of_fnnn_eq h1\n  have h3 : a2 + b2 ≤ a1 + b1 := le_of_fnnn_eq h1.symm\n  have h4 : a1 + b1 = a2 + b2 := by linarith\n  rewrite [fnnn_def, fnnn_def, h4] at h1\n    --h1 : tri (a2 + b2) + a1 = tri (a2 + b2) + a2\n  have h6 : a1 = a2 := Nat.add_left_cancel h1\n  rewrite [h6] at h4   --h4 : a2 + b1 = a2 + b2\n  have h7 : b1 = b2 := Nat.add_left_cancel h4\n  rewrite [h6, h7]\n  rfl\n  done\n\nlemma fnnn_onto : onto fnnn := by\n  define  --Goal : ∀ (y : Nat), ∃ (x : Nat × Nat), fnnn x = y\n  by_induc\n  · -- Base Case\n    apply Exists.intro (0, 0)\n    rfl\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : ∃ (x : Nat × Nat), fnnn x = n\n    obtain ((a, b) : Nat × Nat) (h1 : fnnn (a, b) = n) from ih\n    by_cases h2 : b = 0\n    · -- Case 1. h2 : b = 0\n      apply Exists.intro (0, a + 1)\n      show fnnn (0, a + 1) = n + 1 from\n        calc fnnn (0, a + 1)\n          _ = tri (0 + (a + 1)) + 0 := by rfl\n          _ = tri (a + 1) := by ring\n          _ = tri a + a + 1 := tri_step a\n          _ = tri (a + 0) + a + 1 := by ring\n          _ = fnnn (a, b) + 1 := by rw [h2, fnnn_def]\n          _ = n + 1 := by rw [h1]\n      done\n    · -- Case 2. h2 : b ≠ 0\n      obtain (k : Nat) (h3 : b = k + 1) from\n        exists_eq_add_one_of_ne_zero h2\n      apply Exists.intro (a + 1, k)\n      show fnnn (a + 1, k) = n + 1 from\n        calc fnnn (a + 1, k)\n          _ = tri (a + 1 + k) + (a + 1) := by rfl\n          _ = tri (a + (k + 1)) + a + 1 := by ring\n          _ = tri (a + b) + a + 1 := by rw [h3]\n          _ = fnnn (a, b) + 1 := by rfl\n          _ = n + 1 := by rw [h1]\n      done\n    done\n  done\n\ntheorem NxN_equinum_N : (Nat × Nat) ∼ Nat :=\n  Exists.intro fnnn (And.intro fnnn_one_one fnnn_onto)\nOne of the most important theorems about the concept of equinumerosity is Theorem 8.1.3 in HTPI, which says that ∼ is reflexive, symmetric, and transitive. We’ll prove the three parts of this theorem separately. To prove that ∼ is reflexive, we use the identity function. (Recall from Section 5.1 that @id U is the identity function on the type U.)\nlemma id_one_one (U : Type) : one_to_one (@id U) := by\n  fix x1 : U; fix x2 : U\n  assume h : id x1 = id x2\n  show x1 = x2 from h\n  done\n\nlemma id_onto (U : Type) : onto (@id U) := by\n  fix y : U              --Goal : ∃ (x : U), id x = y\n  apply Exists.intro y   --Goal : id y = y\n  rfl\n  done\n\ntheorem Theorem_8_1_3_1 (U : Type) : U ∼ U := by\n  apply Exists.intro id\n  show one_to_one id ∧ onto id from And.intro (id_one_one U) (id_onto U)\n  done\nFor symmetry, we use some theorems from Chapter 5 about inverses of functions:\ntheorem Theorem_8_1_3_2 {U V : Type}\n    (h : U ∼ V) : V ∼ U := by\n  obtain (f : U → V) (h1 : one_to_one f ∧ onto f) from h\n  obtain (finv : V → U) (h2 : graph finv = inv (graph f)) from\n    Theorem_5_3_1 f h1.left h1.right\n  apply Exists.intro finv\n  have h3 : finv ∘ f = id := Theorem_5_3_2_1 f finv h2\n  have h4 : f ∘ finv = id := Theorem_5_3_2_2 f finv h2\n  show one_to_one finv ∧ onto finv from\n    And.intro (Theorem_5_3_3_1 finv f h4) (Theorem_5_3_3_2 finv f h3)\n  done\nFinally, for transitivity, we use theorems about composition of functions:\ntheorem Theorem_8_1_3_3 {U V W : Type}\n    (h1 : U ∼ V) (h2 : V ∼ W) : U ∼ W := by\n  obtain (f : U → V) (h3 : one_to_one f ∧ onto f) from h1\n  obtain (g : V → W) (h4 : one_to_one g ∧ onto g) from h2\n  apply Exists.intro (g ∘ f)\n  show one_to_one (g ∘ f) ∧ onto (g ∘ f) from\n    And.intro (Theorem_5_2_5_1 f g h3.left h4.left)\n    (Theorem_5_2_5_2 f g h3.right h4.right)\n  done\nSo far, we have only talked about types being equinumerous, but later in this chapter we are going to want to talk about sets being equinumerous. For example, it would be nice if we could give this proof:\ntheorem wishful_thinking?\n    {U : Type} (A : Set U) : A ∼ A := Theorem_8_1_3_1 A\nIt seems like Lean shouldn’t accept this theorem; the notation ∼ was defined to apply to types, and in this theorem, A is a set, not a type. But if you enter this theorem into Lean, you will find that Lean accepts it! How is that possible?\nWe can find out what Lean thinks this theorem means by giving the command #check @wishful_thinking?. Lean’s response is:\n\n@wishful_thinking? : ∀ {U : Type} (A : Set U), ↑A ∼ ↑A\n\nAha! The uparrows are the key to unlocking the mystery. As we know, uparrows in Lean represent coercions. So Lean must have coerced A into a type, so that it can be used with the ∼ notation. Although A is a set, ↑A is a type.\nWhat is the type ↑A? Intuitively, you can think of the objects of type ↑A as the elements of A. Since A has type Set U, the elements of A are some, but perhaps not all, of the objects of type U; for this reason, ↑A is called a subtype of U.\nHowever, this intuitive description can be misleading. The relationship between ↑A and U is actually similar to the relationship between Nat and Int. Recall that, although we think of the natural numbers as being contained in the integers, in Lean, the types Nat and Int are completely separate. If n has type Nat, then n is not an integer, but there is an integer that corresponds to n, and n can be coerced to that corresponding integer. Similarly, although we might think of ↑A as being contained in U, in fact the two types are completely separate. If a has type ↑A, then a does not have type U, but there is an object of type U that corresponds to a. That corresponding object is called the value of a, and it is denoted a.val. Furthermore, a can be coerced to a.val; using Lean’s notation for coercion, we can write ↑a = a.val.\nIf a has type ↑A, then not only is a.val an object of type U, but it is an element of A. Indeed, a supplies us with a proof of this fact. This proof is denoted a.property. In other words, we have a.property : a.val ∈ A. Indeed, you might think of any object a : ↑A as a bundle consisting of two pieces of data, a.val and a.property. Not only can we extract these two pieces of data from a by using the .val and .property notation, but we can go in the other direction. That is, if we have x : U and h : x ∈ A, then we can bundle these two pieces of data together to create an object of type ↑A. This object of type ↑A is denoted Subtype.mk x h. Thus, if a = Subtype.mk x h, then a has type ↑A, a.val = x, and a.property = h. We can make the creation of objects of type ↑A slightly simpler by introducing the following function:\ndef Subtype_elt {U : Type} {A : Set U} {x : U} (h : x ∈ A) : ↑A :=\n  Subtype.mk x h\nNote that the only nonimplicit argument of Subtype_elt is h. Thus, if we have h : x ∈ A, then Subtype_elt h is an object of type ↑A whose value is x.\nThere is one more important property of the type ↑A. For each element of A, there is only one corresponding object of type ↑A. That means that if a1 and a2 are two objects of type ↑A and a1.val = a2.val, then a1 = a2. We can think of this as an extensionality principle for subtypes. Recall that the extensionality principle for sets is called Set.ext, and it says that if two sets have the same elements, then they are equal. Similarly, the extensionality principle for subtypes is denoted Subtype.ext, and it says that if two objects of type ↑A have the same value, then they are equal. More precisely, if we have a1 a2 : ↑A, then Subtype.ext proves a.val = a2.val → a1 = a2. And just as we usually start a proof that two sets are equal with the tactic apply Set.ext, it is often useful to start a proof that two objects of type ↑A are equal with the tactic apply Subtype.ext.\nNow that we know that the concept of equinumerosity can be applied not only to types but also to sets, we can use this idea to make a number of definitions. For any natural number \\(n\\), HTPI defines \\(I_n\\) to be the set \\(\\{1, 2, \\ldots, n\\}\\), and then it defines a set to be finite if it is equinumerous with \\(I_n\\), for some \\(n\\). In Lean, it is a bit more convenient to use sets of the form \\(\\{0, 1, \\ldots, n - 1\\}\\). With that small change, we can repeat the definitions of finite, denumerable, and countable in HTPI.\ndef I (n : Nat) : Set Nat := {k : Nat | k < n}\n\nlemma I_def (k n : Nat) : k ∈ I n ↔ k < n := by rfl\n\ndef finite (U : Type) : Prop := ∃ (n : Nat), I n ∼ U\n\ndef denum (U : Type) : Prop := Nat ∼ U\n\nlemma denum_def (U : Type) : denum U ↔ Nat ∼ U := by rfl\n\ndef ctble (U : Type) : Prop := finite U ∨ denum U\nNote that in the definition of finite, I n ∼ U means ↑(I n) ∼ U, because I n is a set, not a type. But we will usually leave it to Lean to fill in such coercions when necessary.\nTheorem 8.1.5 in HTPI gives two useful ways to characterize countable sets. The proof of the theorem in HTPI uses the fact that every set of natural numbers is countable. HTPI gives an intuitive explanation of why this is true, but of course in Lean an intuitive explanation won’t do. So before proving a version of Theorem 8.1.5, we sketch a proof that every set of natural numbers is countable.\nSuppose A has type Set Nat. To prove that A is countable, we will define a function that numbers the elements of A by assigning the number 0 to the smallest element of A, 1 to the next element of A, 2 to the next, and so on. How do we tell which natural number should be assigned to any element m of A? Notice that if m is the smallest element of A, then there are 0 elements of A that are smaller than m; if it is the second smallest element of A, then there is 1 element of A that is smaller than m; and so on. In general, the number assigned to m should be the number of elements of A that are smaller than m. We therefore begin by defining a function num_elts_below A m that counts the number of elements of A that are smaller than any natural number m.\nThe definition of num_elts_below is recursive. The recursive step relates the number of elements of A below n + 1 to the number of elements below n. There are two possibilities: either n ∈ A and the number of elements below n + 1 is one larger than the number below n, or n ∉ A and the two numbers are the same. (This may remind you of the recursion we used to define num_rp_below in Chapter 7.)\ndef num_elts_below (A : Set Nat) (m : Nat) : Nat :=\n  match m with\n    | 0 => 0\n    | n + 1 => **if n ∈ A then (num_elts_below A n) + 1::\n                else num_elts_below A n\nUnfortunately, this definition results in an error message: failed to synthesize Decidable (n ∈ A). Lean is complaining because it doesn’t know how to decide, in general, whether or not n ∈ A. As a result, if we asked it to evaluate num_elts_below A m, for some particular set A and natural number m, it wouldn’t know how to compute it. But this won’t be an issue for us; we want to use num_elts_below to prove theorems, but we’re never going to ask Lean to compute it. For reasons that we won’t explain here, we can get Lean to ignore this issue by adding the line open Classical at the top of our Lean file. This change leads to a new error, but this time the message is more helpful: failed to compile definition, consider marking it as 'noncomputable'. Following Lean’s advice, we finally get a definition that is acceptable to Lean:\nopen Classical\n\nnoncomputable def num_elts_below (A : Set Nat) (m : Nat) : Nat :=\n  match m with\n    | 0 => 0\n    | n + 1 => if n ∈ A then (num_elts_below A n) + 1\n                else num_elts_below A n\nFor example, if A = {1, 3, 4}, then num_elts_below A 0 = num_elts_below A 1 = 0, num_elts_below A 2 = num_elts_below A 3 = 1, num_elts_below A 4 = 2, and num_elt_below A m = 3 for every m ≥ 5. Notice that num_elts_below A is a function from Nat to Nat, but it is neither one-to-one nor onto. To make it useful for proving that A is countable, we’ll need to modify it.\nSuppose f : U → V, but f is not one-to-one or onto. How can we modify f to get a function that is one-to-one or onto? We begin with the problem of getting a function that is onto. The range of f is the set of all y : V such that for some x : U, f x = y:\ndef range {U V : Type} (f : U → V) : Set V := {y : V | ∃ (x : U), f x = y}\n(In the exercises, we ask you to show that this is the same as Ran (graph f).) Since every value of f is in range f, we can convert f into a function from U to range f (that is, from U to ↑(range f)), as follows:\nlemma elt_range {U V : Type} (f : U → V) (x : U) : f x ∈ range f := by\n  define                 --Goal : ∃ (x_1 : U), f x_1 = f x\n  apply Exists.intro x\n  rfl\n  done\n\ndef func_to_range {U V : Type} (f : U → V) (x : U) : range f :=\n  Subtype_elt (elt_range f x)\nAccording to these definitions, func_to_range f is a function from U to ↑(range f); it is the same as f, except that each value of the function is converted to an object of type ↑(range f). And it is not hard to prove that this function is onto:\nlemma ftr_def {U V : Type} (f : U → V) (x : U) :\n    (func_to_range f x).val = f x := by rfl\n\nlemma ftr_onto {U V : Type} (f : U → V) : onto (func_to_range f) := by\n  fix y : range f              --y has type ↑(range f)\n  have h1 : y.val ∈ range f := y.property\n  define at h1                 --h1 : ∃ (x : U), f x = y.val\n  obtain (x : U) (h2 : f x = y.val) from h1\n  apply Exists.intro x         --Goal : func_to_range f x = y\n  apply Subtype.ext            --Goal : (func_to_range f x).val = y.val\n  rewrite [ftr_def, h2]\n  rfl\n  done\nIs func_to_range f one-to-one? It turns out that if f is one-to-one, then so is func_to_range f, and therefore func_to_range f can be used to prove that U ∼ range f. Here are the proofs:\nlemma ftr_one_one_of_one_one {U V : Type} {f : U → V}\n    (h : one_to_one f) : one_to_one (func_to_range f) := by\n  fix x1 : U; fix x2 : U\n  assume h1 : func_to_range f x1 = func_to_range f x2\n  have h2 : f x1 = f x2 :=\n    calc f x1\n      _ = (func_to_range f x1).val := (ftr_def f x1).symm\n      _ = (func_to_range f x2).val := by rw [h1]\n      _ = f x2 := ftr_def f x2\n  show x1 = x2 from h x1 x2 h2\n  done\n\ntheorem equinum_range {U V : Type} {f : U → V}\n    (h : one_to_one f) : U ∼ range f := by\n  apply Exists.intro (func_to_range f)\n  show one_to_one (func_to_range f) ∧ onto (func_to_range f) from\n    And.intro (ftr_one_one_of_one_one h) (ftr_onto f)\n  done\nWe have seen that, given a function f : U → V, we can turn f into an onto function by replacing V with a subtype of V. (This is similar to an idea that is mentioned briefly in HTPI; see Exercise 23 in Section 5.2 of HTPI.) It is perhaps not surprising that we can sometimes turn f into a one-to-one function by replacing U with a subtype of U. If A has type Set U, then the restriction of f to A is a function from A to V. It has the same values as f, but only when applied to elements of A. (This idea is also mentioned in HTPI; see Exercise 7 in Section 5.2 of HTPI.) We can define the restriction of f to A in Lean as follows:\ndef func_restrict {U V : Type}\n  (f : U → V) (A : Set U) (x : A) : V := f x.val\n\nlemma fr_def {U V : Type} (f : U → V) (A : Set U) (x : A) :\n    func_restrict f A x = f x.val := by rfl\nThus, func_restrict f A is a function from A to V (that is, from ↑A to V). Is it one-to-one? The answer is: sometimes. We will say that a function is one-to-one on A if it satisfies the definition of one-to-one when applied to elements of A:\ndef one_one_on {U V : Type} (f : U → V) (A : Set U) : Prop :=\n  ∀ ⦃x1 x2 : U⦄, x1 ∈ A → x2 ∈ A → f x1 = f x2 → x1 = x2\nNotice that in this definition, we have used the same double braces for the quantified variables x1 and x2 that were used in the definition of “subset.” This means that x1 and x2 are implicit arguments, and therefore if we have h : one_one_on f A, ha1 : a1 ∈ A, ha2 : a2 ∈ A, and heq : f a1 = f a2, then h h1a h2a heq is a proof of a1 = a2. There is no need to specify that a1 and a2 are the values to be assigned to x1 and x2; Lean will figure that out for itself. (To type the double braces ⦃ and ⦄, type \\{{ and \\}}. There were cases in previous chapters where it would have been appropriate to use such implicit arguments, but we chose not to do so to avoid confusion. But by now you should be comfortable enough with Lean that you won’t be confused by this new complication.)\nIt is now not hard to show that if f is one-to-one on A, then func_restrict f A is one-to-one:\nlemma fr_one_one_of_one_one_on {U V : Type} {f : U → V} {A : Set U}\n    (h : one_one_on f A) : one_to_one (func_restrict f A) := by\n  fix x1 : A; fix x2 : A                    --x1 and x2 have type ↑A\n  assume h1 : func_restrict f A x1 = func_restrict f A x2\n  rewrite [fr_def, fr_def] at h1            --h1 : f x1.val = f x2.val\n  apply Subtype.ext                         --Goal : x1.val = x2.val\n  show x1.val = x2.val from h x1.property x2.property h1\n  done\nNow we can combine our last two results: if f is one-to-one on A, then func_restrict f A is one-to-one, and therefore A is equinumerous with the range of func_restrict f A. And what is the range of func_restrict f A? It is just the image of A under f:\nlemma elt_image {U V : Type} {A : Set U} {x : U}\n    (f : U → V) (h : x ∈ A) : f x ∈ image f A := by\n  define                   --Goal : ∃ x_1 ∈ A, f x_1 = f x\n  apply Exists.intro x     --Goal : x ∈ A ∧ f x = f x\n  apply And.intro h\n  rfl\n  done\n\nlemma fr_range {U V : Type} (f : U → V) (A : Set U) :\n    range (func_restrict f A) = image f A := by\n  apply Set.ext\n  fix y : V\n  apply Iff.intro\n  · -- (→)\n    assume h1 : y ∈ range (func_restrict f A) --Goal : y ∈ image f A\n    define at h1\n    obtain (a : A) (h2 : func_restrict f A a = y) from h1\n    rewrite [←h2, fr_def]                     --Goal : f a.val ∈ image f A\n    show f a.val ∈ image f A from elt_image f a.property\n    done\n  · -- (←)\n    assume h1 : y ∈ image f A         --Goal : y ∈ range (func_restrict f A)\n    define at h1\n    obtain (a : U) (h2 : a ∈ A ∧ f a = y) from h1\n    set aA : A := Subtype_elt h2.left\n    have h3 : func_restrict f A aA = f a := fr_def f A aA\n    rewrite [←h2.right, ←h3]\n    show func_restrict f A aA ∈ range (func_restrict f A) from\n      elt_range (func_restrict f A) aA\n    done\n  done\nPutting it all together, we have another theorem that helps us prove that sets are equinumerous:\ntheorem equinum_image {U V : Type} {A : Set U} {f : U → V}\n    (h : one_one_on f A) : A ∼ image f A := by\n  rewrite [←fr_range f A]          --Goal : A ∼ range (func_restrict f A)\n  have h1 : one_to_one (func_restrict f A) :=\n    fr_one_one_of_one_one_on h\n  show A ∼ range (func_restrict f A) from equinum_range h1\n  done\nWe now return to the problem of showing that if A has type Set Nat, then it is countable. Recall that we have defined a function num_elts_below A : Nat → Nat that counts the number of elements of A below any natural number. Although num_elts_below A is not one-to-one, it turns out that it is one-to-one on A. We use this fact to show that if A has an upper bound then it is finite, and if it doesn’t then it is denumerable. The details of the proof are somewhat long. We’ll skip some of them here, but you can find them in the HTPI Lean package.\nlemma neb_one_one_on (A : Set Nat) :\n    one_one_on (num_elts_below A) A := sorry\n\nlemma neb_image_bdd {A : Set Nat} {m : Nat} (h : ∀ n ∈ A, n < m) :\n    image (num_elts_below A) A = I (num_elts_below A m) := sorry\n\nlemma bdd_subset_nat {A : Set Nat} {m : Nat}\n    (h : ∀ n ∈ A, n < m) : I (num_elts_below A m) ∼ A := by\n  have h2 : A ∼ image (num_elts_below A) A :=\n    equinum_image (neb_one_one_on A)\n  rewrite [neb_image_bdd h] at h2        --h2 : A ∼ I (num_elts_below A m)\n  show I (num_elts_below A m) ∼ A from Theorem_8_1_3_2 h2\n  done\n\nlemma neb_unbdd_onto {A : Set Nat}\n    (h : ∀ (m : Nat), ∃ n ∈ A, n ≥ m) :\n    onto (func_restrict (num_elts_below A) A) := sorry\n  \nlemma unbdd_subset_nat {A : Set Nat}\n    (h : ∀ (m : Nat), ∃ n ∈ A, n ≥ m) :\n    denum A := by\n  rewrite [denum_def]\n  set f : A → Nat := func_restrict (num_elts_below A) A\n  have h1 : one_to_one f :=\n    fr_one_one_of_one_one_on (neb_one_one_on A)\n  have h2 : onto f := neb_unbdd_onto h\n  have h3 : A ∼ Nat := Exists.intro f (And.intro h1 h2)\n  show Nat ∼ A from Theorem_8_1_3_2 h3\n  done\n\ntheorem set_nat_ctble (A : Set Nat) : ctble A := by\n  define          --Goal : finite A ∨ denum A\n  by_cases h1 : ∃ (m : Nat), ∀ n ∈ A, n < m\n  · -- Case 1. h1 : ∃ (m : Nat), ∀ n ∈ A, n < m\n    apply Or.inl  --Goal : finite A\n    obtain (m : Nat) (h2 : ∀ n ∈ A, n < m) from h1\n    define\n    apply Exists.intro (num_elts_below A m)\n    show I (num_elts_below A m) ∼ A from bdd_subset_nat h2\n    done\n  · -- Case 2. h1 : ¬∃ (m : Nat), ∀ n ∈ A, n < m\n    apply Or.inr  --Goal : denum A\n    push_neg at h1\n      --This tactic converts h1 to ∀ (m : Nat), ∃ n ∈ A, m ≤ n\n    show denum A from unbdd_subset_nat h1\n    done\n  done\nAs a consequence of our last theorem, we will get another characterization of countability: a type is countable if and only if it is equinumerous with some set of natural numbers. A finite type is equinumerous with the set I n, for some natural number n. But what set of natural numbers is a denumerable type equinumerous with? The obvious choice is the set of all natural numbers. We might call it the universal set for the type Nat. It will be convenient to have notation for the universal set of any type. It is not hard to show that any type is equinumerous with its universal set. We leave one lemma for the proof as an exercise for you:\ndef Univ (U : Type) : Set U := {x : U | True}\n\nlemma elt_univ {U : Type} (x : U) : x ∈ Univ U := by\n  define   --Goal : True\n  trivial\n  done\n\nlemma onto_iff_range_eq_univ {U V : Type} (f : U → V) :\n    onto f ↔ range f = Univ V := sorry\n\nlemma univ_equinum_type (U : Type) : Univ U ∼ U := by\n  set f : U → U := id\n  have h1 : one_to_one f := id_one_one U\n  have h2 : onto f := id_onto U\n  rewrite [onto_iff_range_eq_univ] at h2  --h2 : range f = Univ U\n  have h3 : U ∼ range f := equinum_range h1\n  rewrite [h2] at h3\n  show Univ U ∼ U from Theorem_8_1_3_2 h3\n  done\nWith this preparation, we can prove our next characterization of countability, leaving the poof of a lemma as an exercise for you:\nlemma ctble_of_ctble_equinum {U V : Type}\n    (h1 : U ∼ V) (h2 : ctble U) : ctble V := sorry\n\ntheorem ctble_iff_set_nat_equinum (U : Type) :\n    ctble U ↔ ∃ (J : Set Nat), J ∼ U := by\n  apply Iff.intro\n  · -- (→)\n    assume h1 : ctble U\n    define at h1  --h1 : finite U ∨ denum U\n    by_cases on h1\n    · -- Case 1. h1 : finite U\n      define at h1  --h1 : ∃ (n : Nat), I n ∼ U\n      obtain (n : Nat) (h2 : I n ∼ U) from h1\n      show ∃ (J : Set Nat), J ∼ U from Exists.intro (I n) h2\n      done\n    · -- Case 2. h1 : denum U\n      rewrite [denum_def] at h1  --h1 : Nat ∼ U\n      have h2 : Univ Nat ∼ Nat := univ_equinum_type Nat\n      apply Exists.intro (Univ Nat)\n      show Univ Nat ∼ U from Theorem_8_1_3_3 h2 h1\n      done\n    done\n  · -- (←)\n    assume h1 : ∃ (J : Set Nat), J ∼ U\n    obtain (J : Set Nat) (h2 : J ∼ U) from h1\n    have h3 : ctble J := set_nat_ctble J\n    show ctble U from ctble_of_ctble_equinum h2 h3\n    done\n  done\nWe have seen that if we have f : U → V, then it can be useful to replace either U or V with a subtype. We accomplished this by defining two functions, func_to_range and func_restrict. It is also sometimes useful to go in the other direction. To do this, we will define functions func_to_type and func_extend that are, in a sense, the reverses of func_to_range and func_restrict. If we have f : U → B, where B has type Set V, then func_to_type f is a function from U to V; it is the same as f, but with the values of the function coerced from B to V. If we have f : A → V and v : V, where A has type Set U, then func_extend f v is also a function from U to V; when applied to an element of A, it has the value specified by f, and when applied to anything else it has the value v. Here are the definitions:\ndef func_to_type {U V : Type} {B : Set V}\n  (f : U → B) (x : U) : V := (f x).val\n\nlemma ftt_def {U V : Type} {B : Set V} (f : U → B) (x : U) :\n    func_to_type f x = (f x).val := by rfl\n\nnoncomputable def func_extend {U V : Type} {A : Set U}\n  (f : A → V) (v : V) (u : U) : V :=\n  if test : u ∈ A then f (Subtype_elt test) else v\n\nlemma fe_elt {U V : Type} {A : Set U} (f : A → V) (v : V) (a : A) :\n    func_extend f v a.val = f a := dif_pos a.property\nNotice that in the definition of func_extend, we gave an identifier to the test in the if clause, so that we could refer to that test in the then clause. As a result, the if-then-else expression in the definition is what is called a dependent if-then-else. The proof of the lemma fe_elt uses the theorem dif_pos, which is just like if_pos, but for dependent if-then-else expressions. (Of course, dif_neg is the dependent version of if_neg.)\nThe sense in which func_to_type and func_extend reverse func_to_range and func_restrict is given by the following examples. We leave the proof of the second as an exercise for you:\nexample (U V : Type) (f : U → V) :\n    func_to_type (func_to_range f) = f := by rfl\n\nexample (U V : Type) (A : Set U) (f : A → V) (v : V) :\n    func_restrict (func_extend f v) A = f := sorry\nHow do func_to_type and func_extend interact with the properties one_to_one and onto? The answers are given by the following lemmas; you can find the straightforward proofs in the HTPI Lean package.\nlemma ftt_range_of_onto {U V : Type} {B : Set V} {f : U → B}\n    (h : onto f) : range (func_to_type f) = B := sorry\n\nlemma ftt_one_one_of_one_one {U V : Type} {B : Set V} {f : U → B}\n    (h : one_to_one f) : one_to_one (func_to_type f) := sorry\n\nlemma fe_image {U V : Type} {A : Set U}\n    (f : A → V) (v : V) : image (func_extend f v) A = range f := sorry\n\nlemma fe_one_one_on_of_one_one {U V : Type} {A : Set U} {f : A → V}\n    (h : one_to_one f) (v : V) : one_one_on (func_extend f v) A := sorry\nJust as func_to_type and func_extend can be thought of as reversing func_to_range and func_restrict, the following theorem can be thought of as reversing the theorem equinum_image:\ntheorem type_to_type_of_equinum {U V : Type} {A : Set U} {B : Set V}\n    (h : A ∼ B) (v : V) :\n    ∃ (f : U → V), one_one_on f A ∧ image f A = B := by\n  obtain (g : A → B) (h1 : one_to_one g ∧ onto g) from h\n  set gtt : A → V := func_to_type g\n  set f : U → V := func_extend gtt v\n  apply Exists.intro f\n  apply And.intro\n  · -- Proof of one_to_one_on f A\n    have h2 : one_to_one gtt := ftt_one_one_of_one_one h1.left\n    show one_one_on f A from fe_one_one_on_of_one_one h2 v\n    done\n  · -- Proof of image f A = B\n    have h2 : range gtt = B := ftt_range_of_onto h1.right\n    have h3 : image f A = range gtt := fe_image gtt v\n    rewrite [h3, h2]\n    rfl\n    done\n  done\nWe are finally ready to turn to Theorem 8.1.5 in HTPI. The theorem says that for any set \\(A\\), the following statements are equivalent:\n\n\\(A\\) is countable.\nEither \\(A = \\varnothing\\) or there is a function \\(f : \\mathbb{Z}^+ \\to A\\) that is onto.\nThere is a function \\(f : A \\to \\mathbb{Z}^+\\) that is one-to-one.\n\nWe will find it convenient to phrase these statements a little differently in Lean. Our plan is to prove that if A has type Set U then the following statements are equivalent:\n\nctble A\nempty A ∨ ∃ (f : Nat → U), A ⊆ range f\n∃ (f : U → Nat), one_one_on f A\n\nAs in HTPI, we will do this by proving 1 → 2 → 3 → 1. Here is the proof of 1 → 2.\ntheorem Theorem_8_1_5_1_to_2 {U : Type} {A : Set U} (h : ctble A) :\n    empty A ∨ ∃ (f : Nat → U), A ⊆ range f := by\n  or_right with h1                          --h1 : ∃ (x : U), x ∈ A\n  obtain (a : U) (h2 : a ∈ A) from h1\n  rewrite [ctble_iff_set_nat_equinum] at h  --h : ∃ (J : Set Nat), J ∼ A\n  obtain (J : Set Nat) (h3 : J ∼ A) from h\n  obtain (f : Nat → U) (h4 : one_one_on f J ∧ image f J = A) from\n    type_to_type_of_equinum h3 a\n  apply Exists.intro f                      --Goal : A ⊆ range f\n  fix y : U\n  assume h5 : y ∈ A\n  rewrite [←h4.right] at h5\n  define at h5\n  obtain (n : Nat) (h6 : n ∈ J ∧ f n = y) from h5\n  rewrite [←h6.right]\n  show f n ∈ range f from elt_range f n\n  done\nFor the proof of 2 → 3, we need to consider two cases. If A is empty, then any function f : U → Nat will do to prove statement 3; we use the constant function that always takes the value 0. The easiest way to prove that this function is one-to-one on A is to use a lemma that says if A is empty, then any statement of the form x ∈ A → P is true.\nThe more interesting case is when we have a function g : Nat → U with A ⊆ range g. This means that for each x ∈ A, there is at least one natural number n such that g n = x. As in HTPI, we first define a function F : A → Nat that picks out the smallest such n; we could call it the smallest preimage of x. We then use func_extend to get the required function from U to Nat. The easiest way to define F is to first define its graph, which we call smallest_preimage_graph g A.\ndef constant_func (U : Type) {V : Type} (v : V) (x : U) : V := v\n\nlemma elt_empty_implies {U : Type} {A : Set U} {x : U} {P : Prop}\n    (h : empty A) : x ∈ A → P := by\n  assume h1 : x ∈ A\n  contradict h\n  show ∃ (x : U), x ∈ A from Exists.intro x h1\n  done\n\nlemma one_one_on_empty {U : Type} {A : Set U}\n    (f : U → Nat) (h : empty A) : one_one_on f A := by\n  fix x1 : U; fix x2 : U\n  show x1 ∈ A → x2 ∈ A → f x1 = f x2 → x1 = x2 from\n    elt_empty_implies h\n  done\n\ndef smallest_preimage_graph {U : Type}\n  (g : Nat → U) (A : Set U) : Set (A × Nat) :=\n  {(x, n) : A × Nat | g n = x.val ∧ ∀ (m : Nat), g m = x.val → n ≤ m}\n\nlemma spg_is_func_graph {U : Type} {g : Nat → U} {A : Set U}\n    (h : A ⊆ range g) : is_func_graph (smallest_preimage_graph g A) := by\n  define\n  fix x : A\n  exists_unique\n  · -- Existence\n    set W : Set Nat := {n : Nat | g n = x.val}\n    have h1 : ∃ (n : Nat), n ∈ W := h x.property\n    show ∃ (y : Nat), (x, y) ∈ smallest_preimage_graph g A from\n      well_ord_princ W h1\n    done\n  · -- Uniqueness\n    fix n1 : Nat; fix n2 : Nat\n    assume h1 : (x, n1) ∈ smallest_preimage_graph g A\n    assume h2 : (x, n2) ∈ smallest_preimage_graph g A\n    define at h1; define at h2\n    have h3 : n1 ≤ n2 := h1.right n2 h2.left\n    have h4 : n2 ≤ n1 := h2.right n1 h1.left\n    linarith\n    done\n  done\n\nlemma spg_one_one {U : Type} {g : Nat → U} {A : Set U} {f : A → Nat}\n    (h : graph f = smallest_preimage_graph g A) : one_to_one f := by\n  fix a1 : A; fix a2 : A\n  assume h1 : f a1 = f a2\n  set y : Nat := f a2           --h1 : f a1 = y\n  have h2 : f a2 = y := by rfl\n  rewrite [←graph_def, h] at h1 --h1 : (a1, y) ∈ smallest_preimage_graph g A\n  rewrite [←graph_def, h] at h2 --h2 : (a2, y) ∈ smallest_preimage_graph g A\n  define at h1                  --h1 : g y = a1.val ∧ ...\n  define at h2                  --h2 : g y = a2.val ∧ ...\n  apply Subtype.ext             --Goal : a1.val = a2.val\n  rewrite [←h1.left, ←h2.left]\n  rfl\n  done\n\ntheorem Theorem_8_1_5_2_to_3 {U : Type} {A : Set U}\n    (h : empty A ∨ ∃ (f : Nat → U), A ⊆ range f) :\n    ∃ (f : U → Nat), one_one_on f A := by\n  by_cases on h\n  · -- Case 1. h : empty A\n    set f : U → Nat := constant_func U 0\n    apply Exists.intro f\n    show one_one_on f A from one_one_on_empty f h\n    done\n  · -- Case 2. h : ∃ (f : Nat → U), A ⊆ range f\n    obtain (g : Nat → U) (h1 : A ⊆ range g) from h\n    have h2 : is_func_graph (smallest_preimage_graph g A) :=\n      spg_is_func_graph h1\n    rewrite [←func_from_graph] at h2\n    obtain (F : A → Nat)\n      (h3 : graph F = smallest_preimage_graph g A) from h2\n    have h4 : one_to_one F := spg_one_one h3\n    set f : U → Nat := func_extend F 0\n    apply Exists.intro f\n    show one_one_on f A from fe_one_one_on_of_one_one h4 0\n    done\n  done\nFinally, the proof of 3 → 1 is straightforward, using the theorem equinum_image.\ntheorem Theorem_8_1_5_3_to_1 {U : Type} {A : Set U}\n    (h1 : ∃ (f : U → Nat), one_one_on f A) :\n    ctble A := by\n  obtain (f : U → Nat) (h2 : one_one_on f A) from h1\n  have h3 : A ∼ image f A := equinum_image h2\n  rewrite [ctble_iff_set_nat_equinum]\n  show ∃ (J : Set Nat), J ∼ A from\n    Exists.intro (image f A) (Theorem_8_1_3_2 h3)\n  done\nWe now know that statements 1–3 are equivalent, which means that statements 2 and 3 can be thought of as alternative ways to think about countability:\ntheorem Theorem_8_1_5_2 {U : Type} (A : Set U) :\n    ctble A ↔ empty A ∨ ∃ (f : Nat → U), A ⊆ range f := by\n  apply Iff.intro Theorem_8_1_5_1_to_2\n  assume h1 : empty A ∨ ∃ (f : Nat → U), A ⊆ range f\n  have h2 : ∃ (f : U → Nat), one_one_on f A := Theorem_8_1_5_2_to_3 h1\n  show ctble A from Theorem_8_1_5_3_to_1 h2\n  done\n\ntheorem Theorem_8_1_5_3 {U : Type} (A : Set U) :\n    ctble A ↔ ∃ (f : U → Nat), one_one_on f A := sorry\nWe end this section with a proof of Theorem 8.1.6 in HTPI, which says that the set of rational numbers is denumerable. Our strategy is to define a one-to-one function from Rat (the type of rational numbers) to Nat. We will need to know a little bit about the way rational numbers are represented in Lean. If q has type Rat, then when q is written in lowest terms, q.num is the numerator, which is an integer, and q.den is the denominator, which is a nonzero natural number. The theorem Rat.ext says that if two rational numbers have the same numerator and denominator, then they are equal. And we will also use the theorem Prod.mk.inj, which says that if two ordered pairs are equal, then their first coordinates are equal, as are their second coordinates.\ndef fqn (q : Rat) : Nat := fnnn (fzn q.num, q.den)\n\nlemma fqn_def (q : Rat) : fqn q = fnnn (fzn q.num, q.den) := by rfl\n\nlemma fqn_one_one : one_to_one fqn := by\n  define\n  fix q1 : Rat; fix q2 : Rat\n  assume h1 : fqn q1 = fqn q2\n  rewrite [fqn_def, fqn_def] at h1\n    --h1 : fnnn (fzn q1.num, q1.den) = fnnn (fzn q2.num, q2.den)\n  have h2 : (fzn q1.num, q1.den) = (fzn q2.num, q2.den) :=\n    fnnn_one_one _ _ h1\n  have h3 : fzn q1.num = fzn q2.num ∧ q1.den = q2.den :=\n    Prod.mk.inj h2\n  have h4 : q1.num = q2.num := fzn_one_one _ _ h3.left\n  show q1 = q2 from Rat.ext h4 h3.right\n  done\n\nlemma range_fqn_unbdd :\n    ∀ (m : Nat), ∃ n ∈ range fqn, n ≥ m := by\n  fix m : Nat\n  set n : Nat := fqn ↑m\n  apply Exists.intro n\n  apply And.intro\n  · -- Proof that n ∈ range fqn\n    define\n    apply Exists.intro ↑m\n    rfl\n    done\n  · -- Proof that n ≥ m\n    show n ≥ m from\n      calc n\n        _ = tri (2 * m + 1) + 2 * m := by rfl\n        _ ≥ m := by linarith\n    done\n  done\n\ntheorem Theorem_8_1_6 : denum Rat := by\n  set I : Set Nat := range fqn\n  have h1 : Nat ∼ I := unbdd_subset_nat range_fqn_unbdd\n  have h2 : Rat ∼ I := equinum_range fqn_one_one\n  have h3 : I ∼ Rat := Theorem_8_1_3_2 h2\n  show denum Rat from Theorem_8_1_3_3 h1 h3\n  done\n\nExercises\n\n--Hint:  Use Exercise_6_1_16a2 from the exercises of Section 6.1.\nlemma fnz_odd (k : Nat) : fnz (2 * k + 1) = -↑(k + 1) := sorry\n\n\nlemma fnz_fzn : fnz ∘ fzn = id  := sorry\n\n\nlemma tri_step (k : Nat) : tri (k + 1) = tri k + k + 1 := sorry\n\n\nlemma tri_incr {j k : Nat} (h1 : j ≤ k) : tri j ≤ tri k := sorry\n\n\nexample {U V : Type} (f : U → V) : range f = Ran (graph f) := sorry\n\n\nlemma onto_iff_range_eq_univ {U V : Type} (f : U → V) :\n    onto f ↔ range f = Univ V := sorry\n\n7. Notice that ctble_of_ctble_equinum was used in the proof of ctble_iff_set_nat_equinum. Therefore, to avoid circularity, you must not use ctble_iff_set_nat_equinum in your solution to this exercise.\nlemma ctble_of_ctble_equinum {U V : Type}\n    (h1 : U ∼ V) (h2 : ctble U) : ctble V := sorry\n\ntheorem Exercise_8_1_1_b : denum {n : Int | even n} := sorry\n\n\ntheorem equinum_iff_inverse_pair (U V : Type) :\n    U ∼ V ↔ ∃ (f : U → V) (g : V → U), f ∘ g = id ∧ g ∘ f = id := sorry\n\n10. Notice that if f is a function from U to V, then for every X of type Set U, image f X has type Set V. Therefore image f is a function from Set U to Set V.\nlemma image_comp_id {U V : Type} {f : U → V} {g : V → U}\n    (h : g ∘ f = id) : (image g) ∘ (image f) = id := sorry\n\ntheorem Exercise_8_1_5_1 {U V : Type}\n    (h : U ∼ V) : Set U ∼ Set V := sorry\n\n\n\n\nIf A has type Set U and X has type Set A (that is, Set ↑A), then every element of X has type ↑A, which means that its value is an element of A. Thus, if we take the values of all the elements of X, we will get a set of type Set U that is a subset of A. We will call this the value image of X. We can define a function that computes the value image of any X : Set A:\ndef val_image {U : Type} (A : Set U) (X : Set A) : Set U :=\n  {y : U | ∃ x ∈ X, x.val = y}\nThe next three exercises ask you to prove properties of this function.\n\nlemma subset_of_val_image_eq {U : Type} {A : Set U} {X1 X2 : Set A}\n    (h : val_image A X1 = val_image A X2) : X1 ⊆ X2 := sorry\n\n\nlemma val_image_one_one {U : Type} (A : Set U) :\n    one_to_one (val_image A) := sorry\n\n\nlemma range_val_image {U : Type} (A : Set U) :\n    range (val_image A) = 𝒫 A := sorry\n\n\nlemma Set_equinum_powerset {U : Type} (A : Set U) :\n    Set A ∼ 𝒫 A := sorry\n\n\n--Hint:  Use Exercise_8_1_5_1 and Set_equinum_powerset.\ntheorem Exercise_8_1_5_2 {U V : Type} {A : Set U} {B : Set V}\n    (h : A ∼ B) : 𝒫 A ∼ 𝒫 B := sorry\n\n\nexample (U V : Type) (A : Set U) (f : A → V) (v : V) :\n    func_restrict (func_extend f v) A = f := sorry\n\n18. We proved the implications in Theorem 8.1.5 for sets, but we could prove similar theorems for types. Here is a version of Theorem_8_1_5_3 for types.\ntheorem Theorem_8_1_5_3_type {U : Type} :\n    ctble U ↔ ∃ (f : U → Nat), one_to_one f := sorry\n\ntheorem ctble_set_of_ctble_type {U : Type}\n    (h : ctble U) (A : Set U) : ctble A := sorry\n\n\ntheorem Exercise_8_1_17 {U : Type} {A B : Set U}\n    (h1 : B ⊆ A) (h2 : ctble A) : ctble B := sorry"
  },
  {
    "objectID": "Chap8.html#debts-paid",
    "href": "Chap8.html#debts-paid",
    "title": "8  Infinite Sets",
    "section": "8.1½. Debts Paid",
    "text": "8.1½. Debts Paid\nIt is time to fulfill promises we made in two earlier chapters.\nIn Section 6.2, we promised to define a proposition numElts A n to express the idea that the set A has n elements. It should now be clear how to define this proposition:\ndef numElts {U : Type} (A : Set U) (n : Nat) : Prop := I n ∼ A\n\nlemma numElts_def {U : Type} (A : Set U) (n : Nat) :\n    numElts A n ↔ I n ∼ A := by rfl\nIt is sometimes convenient to phrase the definition of finite in terms of numElts, so we state that version of the definition as a lemma.\nlemma finite_def {U : Type} (A : Set U) :\n    finite A ↔ ∃ (n : Nat), numElts A n := by rfl\nWe also owe you the proofs of several theorems about numElts. We’ll skip the details of some of these proofs, but for those that are not left as exercises, you can find all the details in the HTPI Lean package. We begin with the fact that a set has zero elements if and only if it is empty. If A has type Set U and A is empty, then any function from U to Nat can be used to prove A ∼ I 0. In the proof below, we use a constant function.\nlemma not_empty_iff_exists_elt {U : Type} {A : Set U} :\n    ¬empty A ↔ ∃ (x : U), x ∈ A := by\n  define : empty A\n  double_neg\n  rfl\n  done\n\nlemma image_empty {U : Type} {A : Set U}\n    (f : U → Nat) (h : empty A) : image f A = I 0 := sorry\n\ntheorem zero_elts_iff_empty {U : Type} (A : Set U) :\n    numElts A 0 ↔ empty A := by\n  apply Iff.intro\n  · -- (→)\n    assume h1 : numElts A 0\n    define at h1\n    obtain (f : I 0 → A) (h2 : one_to_one f ∧ onto f) from h1\n    by_contra h3\n    rewrite [not_empty_iff_exists_elt] at h3\n    obtain (x : U) (h4 : x ∈ A) from h3\n    set xA : A := Subtype_elt h4\n    obtain (n : I 0) (h5 : f n = xA) from h2.right xA\n    have h6 : n.val < 0 := n.property\n    linarith\n    done\n  · -- (←)\n    assume h1 : empty A\n    rewrite [numElts_def]\n    set f : U → Nat := constant_func U 0\n    have h2 : one_one_on f A := one_one_on_empty f h1\n    have h3 : image f A = I 0 := image_empty f h1\n    have h4 : A ∼ image f A := equinum_image h2\n    rewrite [h3] at h4\n    show I 0 ∼ A from Theorem_8_1_3_2 h4\n    done\n  done\nNext, we prove that if a set has a positive number of elements then it is not empty. The proof is straightforward.\ntheorem nonempty_of_pos_numElts {U : Type} {A : Set U} {n : Nat}\n    (h1 : numElts A n) (h2 : n > 0) : ∃ (x : U), x ∈ A := by\n  define at h1\n  obtain (f : I n → A) (h3 : one_to_one f ∧ onto f) from h1\n  have h4 : 0 ∈ I n := h2\n  set x : A := f (Subtype_elt h4)\n  show ∃ (x : U), x ∈ A from Exists.intro x.val x.property\n  done\nOur next theorem is remove_one_numElts, which says that if a set has n + 1 elements, and we remove one element, then the resulting set has n elements. We begin by proving that for any k < n + 1, if we remove k from I (n + 1) then the resulting set is equinumerous with I n. To do this, we define a function that matches up I n with I (n + 1) \\ {k}.\ndef incr_from (k n : Nat) : Nat := if n < k then n else n + 1\nThe function incr_from k increments natural numbers from k on, while leaving numbers less than k fixed. Our strategy now is to prove that incr_from k is one-to-one on I n, and the image of I n under incr_from k is I (n + 1) \\ {k}. The proof is a bit long, so we skip some of the details. Notice that Lean gets confused when coercing I (n + 1) \\ {k} to a subtype unless we specify that we want ↑(I (n + 1) \\ {k}) rather than ↑(I (n + 1)) \\ ↑{k}.\nlemma incr_from_one_one (k : Nat) :\n    one_to_one (incr_from k) := sorry\n\nlemma incr_from_image {k n : Nat} (h : k < n + 1) :\n    image (incr_from k) (I n) = I (n + 1) \\ {k} := sorry\n\nlemma one_one_on_of_one_one {U V : Type} {f : U → V}\n    (h : one_to_one f) (A : Set U) : one_one_on f A := by\n  define\n  fix x1 : U; fix x2 : U\n  assume h1 : x1 ∈ A\n  assume h2 : x2 ∈ A\n  show f x1 = f x2 → x1 = x2 from h x1 x2\n  done\n\nlemma I_equinum_I_remove_one {k n : Nat}\n    (h : k < n + 1) : I n ∼ ↑(I (n + 1) \\ {k}) := by\n  rewrite [←incr_from_image h]\n  show I n ∼ image (incr_from k) (I n) from\n    equinum_image (one_one_on_of_one_one (incr_from_one_one k) (I n))\n  done\nUsing one more lemma, whose proof we leave as an exercise for you, we can prove remove_one_numElts.\nlemma remove_one_equinum\n    {U V : Type} {A : Set U} {B : Set V} {a : U} {b : V} {f : U → V}\n    (h1 : one_one_on f A) (h2 : image f A = B)\n    (h3 : a ∈ A) (h4 : f a = b) : ↑(A \\ {a}) ∼ ↑(B \\ {b}) := sorry\n\ntheorem remove_one_numElts {U : Type} {A : Set U} {n : Nat} {a : U}\n    (h1 : numElts A (n + 1)) (h2 : a ∈ A) : numElts (A \\ {a}) n := by\n  rewrite [numElts_def] at h1; rewrite [numElts_def]\n    --h1 : I (n + 1) ∼ A;  Goal : I n ∼ ↑(A \\ {a})\n  obtain (f : Nat → U) (h3 : one_one_on f (I (n + 1)) ∧\n    image f (I (n + 1)) = A) from type_to_type_of_equinum h1 a\n  rewrite [←h3.right] at h2\n  obtain (k : Nat) (h4 : k ∈ I (n + 1) ∧ f k = a) from h2\n  have h5 : ↑(I (n + 1) \\ {k}) ∼ ↑(A \\ {a}) :=\n    remove_one_equinum h3.left h3.right h4.left h4.right\n  have h6 : k < n + 1 := h4.left\n  have h7 : I n ∼ ↑(I (n + 1) \\ {k}) := I_equinum_I_remove_one h6\n  show I n ∼ ↑(A \\ {a}) from Theorem_8_1_3_3 h7 h5\n  done\nFinally, we prove that a set has one element if and only if it is a singleton set, leaving the proof of one lemma as an exercise for you.\nlemma singleton_of_diff_empty {U : Type} {A : Set U} {a : U}\n    (h1 : a ∈ A) (h2 : empty (A \\ {a})) : A = {a} := sorry\n\nlemma one_one_on_I_1 {U : Type} (f : Nat → U) : one_one_on f (I 1) := by\n  fix x1 : Nat; fix x2 : Nat\n  assume h1 : x1 ∈ I 1\n  assume h2 : x2 ∈ I 1\n  assume h3 : f x1 = f x2\n  define at h1; define at h2   --h1 : x1 < 1; h2 : x2 < 1\n  linarith\n  done\n\nlemma image_I_1 {U : Type} (f : Nat → U) : image f (I 1) = {f 0} := by\n  apply Set.ext\n  fix y\n  apply Iff.intro\n  · -- (→)\n    assume h1 : y ∈ image f (I 1)\n    define at h1; define\n    obtain (x : Nat) (h2 : x ∈ I 1 ∧ f x = y) from h1\n    have h3 : x < 1 := h2.left\n    have h4 : x = 0 := by linarith\n    rewrite [←h2.right, h4]\n    rfl\n    done\n  · -- (←)\n    assume h1 : y ∈ {f 0}\n    define at h1; define\n    apply Exists.intro 0\n    apply And.intro _ h1.symm\n    define\n    linarith\n    done\n  done\n\nlemma singleton_one_elt {U : Type} (u : U) : numElts {u} 1 := by\n  rewrite [numElts_def]  --Goal : I 1 ∼ {u}\n  set f : Nat → U := constant_func Nat u\n  have h1 : one_one_on f (I 1) := one_one_on_I_1 f\n  have h2 : image f (I 1) = {f 0} := image_I_1 f\n  have h3 : f 0 = u := by rfl\n  rewrite [←h3, ←h2]\n  show I 1 ∼ image f (I 1) from equinum_image h1\n  done\n\ntheorem one_elt_iff_singleton {U : Type} (A : Set U) :\n    numElts A 1 ↔ ∃ (x : U), A = {x} := by\n  apply Iff.intro\n  · -- (→)\n    assume h1 : numElts A 1  --Goal : ∃ (x : U), A = {x}\n    have h2 : 1 > 0 := by linarith\n    obtain (x : U) (h3 : x ∈ A) from nonempty_of_pos_numElts h1 h2\n    have h4 : numElts (A \\ {x}) 0 := remove_one_numElts h1 h3\n    rewrite [zero_elts_iff_empty] at h4\n    show ∃ (x : U), A = {x} from\n      Exists.intro x (singleton_of_diff_empty h3 h4)\n    done\n  · -- (←)\n    assume h1 : ∃ (x : U), A = {x}\n    obtain (x : U) (h2 : A = {x}) from h1\n    rewrite [h2]\n    show numElts {x} 1 from singleton_one_elt x\n    done\n  done\nWe have now proven all of the theorems about numElts whose proofs were promised in Section 6.2. However, there is still one important issue that we have not addressed. Could there be a set A such that, say, numElts A 5 and numElts A 6 are both true? Surely the answer is no—a set can’t have five elements and also have six elements! But it requires proof. We ask you to prove the following theorem in the exercises.\ntheorem Exercise_8_1_6b {U : Type} {A : Set U} {m n : Nat}\n    (h1 : numElts A m) (h2 : numElts A n) : m = n := sorry\nNext, we turn to our promise, at the end of Section 7.4, to prove Theorem 7.4.4 of HTPI, which says that the totient function \\(\\varphi\\) is multiplicative.\nTo define the totient function in Lean, in Chapter 7 we defined phi m to be num_rp_below m m, where num_rp_below m k is the number of natural numbers less than k that are relatively prime to m. But in this chapter we have developed new methods for counting things. Our first task is to show that these new methods agree with the method used in Chapter 7.\nWe have already remarked that the definition of num_elts_below in this chapter bears some resemblance to the definition of num_rp_below in Chapter 7. It should not be surprising, therefore, that these two counting methods give results that agree.\ndef set_rp_below (m : Nat) : Set Nat := {n : Nat | rel_prime m n ∧ n < m}\n\nlemma set_rp_below_def (a m : Nat) :\n    a ∈ set_rp_below m ↔ rel_prime m a ∧ a < m := by rfl\n\nlemma neb_nrpb (m : Nat) : ∀ ⦃k : Nat⦄, k ≤ m →\n    num_elts_below (set_rp_below m) k = num_rp_below m k := sorry\n\nlemma neb_phi (m : Nat) :\n    num_elts_below (set_rp_below m) m = phi m := by\n  rewrite [phi_def]\n  have h1 : m ≤ m := by linarith\n  show num_elts_below (set_rp_below m) m = num_rp_below m m from\n    neb_nrpb m h1\n  done\n\nlemma phi_is_numElts (m : Nat) :\n    numElts (set_rp_below m) (phi m) := by\n  rewrite [numElts_def, ←neb_phi m]\n    --Goal : I (num_elts_below (set_rp_below m) m) ∼ set_rp_below m\n  have h1 : ∀ n ∈ set_rp_below m, n < m := by\n    fix n : Nat\n    assume h2 : n ∈ set_rp_below m\n    define at h2\n    show n < m from h2.right\n    done\n  show I (num_elts_below (set_rp_below m) m) ∼ set_rp_below m from\n    bdd_subset_nat h1\n  done\nAccording to the last lemma, we can now think of phi m as the number of elements of the set set_rp_below m.\nWe will need one more number-theoretic fact: Lemma 7.4.7 from HTPI. We follow the strategy of the proof in HTPI, separating out one calculation as an auxiliary lemma before giving the main proof.\nlemma Lemma_7_4_7_aux {m n : Nat} {s t : Int}\n    (h : s * m + t * n = 1) (a b : Nat) :\n    t * n * a + s * m * b ≡ a (MOD m) := by\n  define\n  apply Exists.intro (s * (b - a))\n  show t * n * a + s * m * b - a = m * (s * (b - a)) from\n    calc t * n * a + s * m * b - a\n      _ = (t * n - 1) * a + s * m * b := by ring\n      _ = (t * n - (s * m + t * n)) * a + s * m * b := by rw [h]\n      _ = m * (s * (b - a)) := by ring\n  done\n\nlemma Lemma_7_4_7 {m n : Nat} [NeZero m] [NeZero n]\n    (h1 : rel_prime m n) (a b : Nat) :\n    ∃ (r : Nat), r < m * n ∧ r ≡ a (MOD m) ∧ r ≡ b (MOD n) := by\n  set s : Int := gcd_c1 m n\n  set t : Int := gcd_c2 m n\n  have h4 : s * m + t * n = gcd m n := gcd_lin_comb n m\n  define at h1                      --h1 : gcd m n = 1\n  rewrite [h1, Nat.cast_one] at h4  --h4 : s * m + t * n = 1\n  set x : Int := t * n * a + s * m * b\n  have h5 : x ≡ a (MOD m) := Lemma_7_4_7_aux h4 a b\n  rewrite [add_comm] at h4          --h4 : t * n + s * m = 1\n  have h6 : s * m * b + t * n * a ≡ b (MOD n) :=\n    Lemma_7_4_7_aux h4 b a\n  have h7 : s * m * b + t * n * a = x := by ring\n  rewrite [h7] at h6                --h6 : x ≡ b (MOD n)\n  have h8 : m * n ≠ 0 := mul_ne_zero (NeZero.ne m) (NeZero.ne n)\n  rewrite [←neZero_iff] at h8       --h8 : NeZero (m * n)\n  have h9 : 0 ≤ x % ↑(m * n) ∧ x % ↑(m * n) < ↑(m * n) ∧\n    x ≡ x % ↑(m * n) (MOD m * n) := mod_cmpl_res (m * n) x\n  have h10 : x % ↑(m * n) < ↑(m * n) ∧\n    x ≡ x % ↑(m * n) (MOD m * n) := h9.right\n  set r : Nat := Int.toNat (x % ↑(m * n))\n  have h11 : x % ↑(m * n) = ↑r := (Int.toNat_of_nonneg h9.left).symm\n  rewrite [h11, Nat.cast_lt] at h10 --h10 : r < m * n ∧ x ≡ r (MOD m * n)\n  apply Exists.intro r\n  apply And.intro h10.left\n  have h12 : r ≡ x (MOD (m * n)) := congr_symm h10.right\n  rewrite [Lemma_7_4_5 _ _ h1] at h12 --h12 : r ≡ x (MOD m) ∧ r ≡ x (MOD n)\n  apply And.intro\n  · -- Proof that r ≡ a (MOD m)\n    show r ≡ a (MOD m) from congr_trans h12.left h5\n    done\n  · -- Proof that r ≡ b (MOD n)\n    show r ≡ b (MOD n) from congr_trans h12.right h6\n    done\n  done\nThe next fact we need is part 1 of Theorem 8.1.2 in HTPI, which says that if \\(A\\), \\(B\\), \\(C\\), and \\(D\\) are sets such that \\(A \\sim B\\) and \\(C \\sim D\\), then \\(A \\times C \\sim B \\times D\\). It is straightforward to translate the HTPI proof into a Lean proof about Cartesian products of equinumerous types.\ndef func_prod {U V W X : Type} (f : U → V) (g : W → X)\n  (p : U × W) : V × X := (f p.1, g p.2)\n\nlemma func_prod_def {U V W X : Type}\n    (f : U → V) (g : W → X) (u : U) (w : W) :\n    func_prod f g (u, w) = (f u, g w) := by rfl\n\ntheorem Theorem_8_1_2_1_type {U V W X : Type}\n    (h1 : U ∼ V) (h2 : W ∼ X) : (U × W) ∼ (V × X) := by\n  obtain (f : U → V) (h3 : one_to_one f ∧ onto f) from h1\n  obtain (g : W → X) (h4 : one_to_one g ∧ onto g) from h2\n  apply Exists.intro (func_prod f g)\n  apply And.intro\n  · -- Proof of one_to_one (func_prod f g)\n    fix (u1, w1) : U × W; fix (u2, w2) : U × W\n    assume h5 : func_prod f g (u1, w1) = func_prod f g (u2, w2)\n    rewrite [func_prod_def, func_prod_def] at h5\n    have h6 : f u1 = f u2 ∧ g w1 = g w2 := Prod.mk.inj h5\n    have h7 : u1 = u2 := h3.left u1 u2 h6.left\n    have h8 : w1 = w2 := h4.left w1 w2 h6.right\n    rewrite [h7, h8]\n    rfl\n    done\n  · -- Proof of onto (func_prod f g)\n    fix (v, x) : V × X\n    obtain (u : U) (h5 : f u = v) from h3.right v\n    obtain (w : W) (h6 : g w = x) from h4.right x\n    apply Exists.intro (u, w)\n    rewrite [func_prod_def, h5, h6]\n    rfl\n  done\nUsing coercions to subtypes, we can also apply this theorem to sets. If A, B, C, and D are sets and we have A ∼ B and C ∼ D, then Theorem_8_1_2_1_type implies that ↑A × ↑C ∼ ↑B × ↑D. Unfortunately, Cartesian products of subtypes are somewhat inconvenient to work with. It will turn out to be easier to work with subtypes of Cartesian products. To make this possible, we define a Cartesian product operation on sets:\ndef set_prod {U V : Type} (A : Set U) (B : Set V) : Set (U × V) :=\n  {(a, b) : U × V | a ∈ A ∧ b ∈ B}\n\nnotation:75 A:75 \" ×ₛ \" B:75 => set_prod A B\n\nlemma set_prod_def {U V : Type} (A : Set U) (B : Set V) (a : U) (b : V) :\n    (a, b) ∈ A ×ₛ B ↔ a ∈ A ∧ b ∈ B := by rfl\nTo type the subscript s after ×, type \\_s. Thus, to type ×ₛ, you can type \\times\\_s or \\x\\_s. Notice that in the notation command that introduces the symbol ×ₛ, we have used the number 75 in positions where we used 50 when defining the notation ∼. Without going into detail about exactly what the three occurrences of 50 and 75 mean, we will just say that this tells Lean that ×ₛ is to be given higher precedence than ∼, and as a result an expression like A ∼ B ×ₛ C will be interpreted as A ∼ (B ×ₛ C) rather than (A ∼ B) ×ₛ C.\nAccording to this definition, if A has type Set U and B has type Set V, then A ×ₛ B has type Set (U × V), and therefore ↑(A ×ₛ B) is a subtype of U × V. There is an obvious correspondence between ↑(A ×ₛ B) and ↑A × ↑B that can be used to prove that they are equinumerous:\nlemma elt_set_prod {U V : Type} {A : Set U} {B : Set V} (p : ↑A × ↑B) :\n    (p.1.val, p.2.val) ∈ A ×ₛ B := And.intro p.1.property p.2.property\n\ndef prod_type_to_prod_set {U V : Type}\n  (A : Set U) (B : Set V) (p : ↑A × ↑B) : ↑(A ×ₛ B) :=\n  Subtype_elt (elt_set_prod p)\n\ndef prod_set_to_prod_type {U V : Type}\n  (A : Set U) (B : Set V) (p : ↑(A ×ₛ B)) : ↑A × ↑B :=\n  (Subtype_elt p.property.left, Subtype_elt p.property.right)\n\nlemma set_prod_equinum_type_prod {U V : Type} (A : Set U) (B : Set V) :\n    ↑(A ×ₛ B) ∼ (↑A × ↑B) := by\n  set F : ↑(A ×ₛ B) → ↑A × ↑B := prod_set_to_prod_type A B\n  set G : ↑A × ↑B → ↑(A ×ₛ B) := prod_type_to_prod_set A B\n  have h1 : F ∘ G = id := by rfl\n  have h2 : G ∘ F = id := by rfl\n  have h3 : one_to_one F := Theorem_5_3_3_1 F G h2\n  have h4 : onto F := Theorem_5_3_3_2 F G h1\n  show ↑(A ×ₛ B) ∼ (↑A × ↑B) from Exists.intro F (And.intro h3 h4)\n  done\nUsing this lemma we can now prove a more convenient set version of the first part of Theorem 8.1.2.\ntheorem Theorem_8_1_2_1_set\n    {U V W X : Type} {A : Set U} {B : Set V} {C : Set W} {D : Set X}\n    (h1 : A ∼ B) (h2 : C ∼ D) : A ×ₛ C ∼ B ×ₛ D := by\n  have h3 : ↑(A ×ₛ C) ∼ (↑A × ↑C) := set_prod_equinum_type_prod A C\n  have h4 : (↑A × ↑C) ∼ (↑B × ↑D) := Theorem_8_1_2_1_type h1 h2\n  have h5 : ↑(B ×ₛ D) ∼ (↑B × ↑D) := set_prod_equinum_type_prod B D\n  have h6 : (↑B × ↑D) ∼ ↑(B ×ₛ D) := Theorem_8_1_3_2 h5\n  have h7 : ↑(A ×ₛ C) ∼ (↑B × ↑D) := Theorem_8_1_3_3 h3 h4\n  show ↑(A ×ₛ C) ∼ ↑(B ×ₛ D) from Theorem_8_1_3_3 h7 h6\n  done\nAs explained in Section 7.4 of HTPI, a key fact used in the proof of Theorem 7.4.4 is that if \\(A\\) is a set with \\(m\\) elements and \\(B\\) is a set with \\(n\\) elements, then \\(A \\times B\\) has \\(mn\\) elements. Section 7.4 of HTPI gives an intuitive explanation of this fact, but we’ll need to prove it in Lean. In other words, we need to prove the following theorem:\ntheorem numElts_prod {U V : Type} {A : Set U} {B : Set V} {m n : Nat}\n    (h1 : numElts A m) (h2 : numElts B n) : numElts (A ×ₛ B) (m * n)\nHere’s our plan for this proof: The hypotheses numElts A m and numElts B n mean I m ∼ A and I n ∼ B. Applying Theorem_8_1_2_1_set to these hypotheses, we can infer I m ×ₛ I n ∼ A ×ₛ B. If we can prove that I (m * n) ∼ I m ×ₛ I n, then we’ll be able to conclude I (m * n) ∼ A ×ₛ B, or in other words numElts (A ×ₛ B) (m * n), as required. Thus, the key to the proof is to show that I (m * n) ∼ I m ×ₛ I n.\nTo prove this, we’ll define a function from Nat to Nat × Nat that maps I (m * n) to I m ×ₛ I n. The function we will use maps a natural number a to the quotient and remainder when a is divided by n.\ndef qr (n a : Nat) : Nat × Nat := (a / n, a % n)\n\nlemma qr_def (n a : Nat) : qr n a = (a / n, a % n) := by rfl\n\nlemma qr_one_one (n : Nat) : one_to_one (qr n) := by\n  define\n  fix a1 : Nat; fix a2 : Nat\n  assume h1 : qr n a1 = qr n a2       --Goal : a1 = a2\n  rewrite [qr_def, qr_def] at h1\n  have h2 : a1 / n = a2 / n ∧ a1 % n = a2 % n := Prod.mk.inj h1\n  show a1 = a2 from\n    calc a1\n      _ = n * (a1 / n) + a1 % n := (Nat.div_add_mod a1 n).symm\n      _ = n * (a2 / n) + a2 % n := by rw [h2.left, h2.right]\n      _ = a2 := Nat.div_add_mod a2 n\n  done\n\nlemma qr_image (m n : Nat) :\n    image (qr n) (I (m * n)) = (I m) ×ₛ (I n) := sorry\n\nlemma I_prod (m n : Nat) : I (m * n) ∼ I m ×ₛ I n := by\n  rewrite [←qr_image m n]\n  show I (m * n) ∼ image (qr n) (I (m * n)) from\n    equinum_image (one_one_on_of_one_one (qr_one_one n) (I (m * n)))\n  done\n\ntheorem numElts_prod {U V : Type} {A : Set U} {B : Set V} {m n : Nat}\n    (h1 : numElts A m) (h2 : numElts B n) : numElts (A ×ₛ B) (m * n) := by\n  rewrite [numElts_def] at h1     --h1 : I m ∼ A\n  rewrite [numElts_def] at h2     --h2 : I n ∼ B\n  rewrite [numElts_def]           --Goal : I (m * n) ∼ A ×ₛ B\n  have h3 : I m ×ₛ I n ∼ A ×ₛ B := Theorem_8_1_2_1_set h1 h2\n  have h4 : I (m * n) ∼ I m ×ₛ I n := I_prod m n\n  show I (m * n) ∼ A ×ₛ B from Theorem_8_1_3_3 h4 h3\n  done\nOur strategy for proving Theorem 7.4.4 will be to show that if m and n are relatively prime, then set_rp_below (m * n) ∼ set_rp_below m ×ₛ set_rp_below n. Once again, we use a function from Nat to Nat × Nat to show that these sets are equinumerous. This time, the function will map a to (a % m, a % n).\ndef mod_mod (m n a : Nat) : Nat × Nat := (a % m, a % n)\n\nlemma mod_mod_def (m n a : Nat) : mod_mod m n a = (a % m, a % n) := by rfl\nOur proof will make use of several theorems from the exercises of Sections 7.3 and 7.4:\ntheorem congr_rel_prime {m a b : Nat} (h1 : a ≡ b (MOD m)) :\n    rel_prime m a ↔ rel_prime m b := sorry\n\ntheorem rel_prime_mod (m a : Nat) :\n    rel_prime m (a % m) ↔ rel_prime m a := sorry\n\ntheorem congr_iff_mod_eq_Nat (m a b : Nat) [NeZero m] :\n    ↑a ≡ ↑b (MOD m) ↔ a % m = b % m := sorry\n\nlemma Lemma_7_4_6 {a b c : Nat} :\n    rel_prime (a * b) c ↔ rel_prime a c ∧ rel_prime b c := sorry\nCombining these with other theorems from Chapter 7, we can now use mod_mod m n to show that set_rp_below (m * n) ∼ set_rp_below m ×ₛ set_rp_below n.\nlemma left_NeZero_of_mul {m n : Nat} (h : m * n ≠ 0) : NeZero m :=\n  neZero_iff.rtl (left_ne_zero_of_mul h)\n\nlemma right_NeZero_of_mul {m n : Nat} (h : m * n ≠ 0) : NeZero n :=\n  neZero_iff.rtl (right_ne_zero_of_mul h)\n\nlemma mod_mod_one_one_on {m n : Nat} (h1 : rel_prime m n) :\n    one_one_on (mod_mod m n) (set_rp_below (m * n)) := by\n  define\n  fix a1 : Nat; fix a2 : Nat\n  assume h2 : a1 ∈ set_rp_below (m * n)\n  assume h3 : a2 ∈ set_rp_below (m * n)\n  assume h4 : mod_mod m n a1 = mod_mod m n a2   --Goal : a1 = a2\n  define at h2; define at h3\n  rewrite [mod_mod_def, mod_mod_def] at h4\n  have h5 : a1 % m = a2 % m ∧ a1 % n = a2 % n := Prod.mk.inj h4\n  have h6 : m * n ≠ 0 := by linarith\n  have h7 : NeZero m := left_NeZero_of_mul h6\n  have h8 : NeZero n := right_NeZero_of_mul h6\n  rewrite [←congr_iff_mod_eq_Nat, ←congr_iff_mod_eq_Nat] at h5\n      --h5 : ↑a1 ≡ ↑a2 (MOD m) ∧ ↑a1 ≡ ↑a2 (MOD n)\n  rewrite [←Lemma_7_4_5 _ _ h1] at h5  --h5 : ↑a1 ≡ ↑a2 (MOD m * n)\n  rewrite [congr_iff_mod_eq_Nat] at h5 --h5 : a1 % (m * n) = a2 % (m * n)\n  rewrite [Nat.mod_eq_of_lt h2.right, Nat.mod_eq_of_lt h3.right] at h5\n  show a1 = a2 from h5\n  done\n\nlemma mod_elt_set_rp_below {a m : Nat} [NeZero m] (h1 : rel_prime m a) :\n    a % m ∈ set_rp_below m := by\n  define                  --Goal : rel_prime m (a % m) ∧ a % m < m\n  rewrite [rel_prime_mod] --Goal : rel_prime m a ∧ a % m < m\n  show rel_prime m a ∧ a % m < m from\n    And.intro h1 (mod_nonzero_lt a (NeZero.ne m))\n  done\n\nlemma mod_mod_image {m n : Nat} (h1 : rel_prime m n) :\n    image (mod_mod m n) (set_rp_below (m * n)) =\n      (set_rp_below m) ×ₛ (set_rp_below n) := by\n  apply Set.ext\n  fix (b, c) : Nat × Nat\n  apply Iff.intro\n  · -- (→)\n    assume h2 : (b, c) ∈ image (mod_mod m n) (set_rp_below (m * n))\n    define at h2\n    obtain (a : Nat)\n      (h3 : a ∈ set_rp_below (m * n) ∧ mod_mod m n a = (b, c)) from h2\n    rewrite [set_rp_below_def, mod_mod_def] at h3\n    have h4 : rel_prime (m * n) a := h3.left.left\n    rewrite [Lemma_7_4_6] at h4   --h4 : rel_prime m a ∧ rel_prime n a\n    have h5 : a % m = b ∧ a % n = c := Prod.mk.inj h3.right\n    define\n    rewrite [←h5.left, ←h5.right]\n      --Goal : a % m ∈ set_rp_below m ∧ a % n ∈ set_rp_below n\n    have h6 : m * n ≠ 0 := by linarith\n    have h7 : NeZero m := left_NeZero_of_mul h6\n    have h8 : NeZero n := right_NeZero_of_mul h6\n    apply And.intro\n    · -- Proof that a % m ∈ set_rp_below m\n      show a % m ∈ set_rp_below m from mod_elt_set_rp_below h4.left\n      done\n    · -- Proof that a % n ∈ set_rp_below n\n      show a % n ∈ set_rp_below n from mod_elt_set_rp_below h4.right\n      done\n    done\n  · -- (←)\n    assume h2 : (b, c) ∈ set_rp_below m ×ₛ set_rp_below n\n    rewrite [set_prod_def, set_rp_below_def, set_rp_below_def] at h2\n      --h2 : (rel_prime m b ∧ b < m) ∧ (rel_prime n c ∧ c < n)\n    define\n    have h3 : m ≠ 0 := by linarith\n    have h4 : n ≠ 0 := by linarith\n    rewrite [←neZero_iff] at h3\n    rewrite [←neZero_iff] at h4\n    obtain (a : Nat) (h5 : a < m * n ∧ a ≡ b (MOD m) ∧ a ≡ c (MOD n))\n      from Lemma_7_4_7 h1 b c\n    apply Exists.intro a\n    apply And.intro\n    · -- Proof of a ∈ set_rp_below (m * n)\n      define                  --Goal : rel_prime (m * n) a ∧ a < m * n\n      apply And.intro _ h5.left\n      rewrite [Lemma_7_4_6]   --Goal : rel_prime m a ∧ rel_prime n a\n      rewrite [congr_rel_prime h5.right.left,\n        congr_rel_prime h5.right.right]\n      show rel_prime m b ∧ rel_prime n c from\n        And.intro h2.left.left h2.right.left\n      done\n    · -- Proof of mod_mod m n a = (b, c)\n      rewrite [congr_iff_mod_eq_Nat, congr_iff_mod_eq_Nat] at h5\n      rewrite [mod_mod_def, h5.right.left, h5.right.right]\n        --Goal : (b % m, c % n) = (b, c)\n      rewrite [Nat.mod_eq_of_lt h2.left.right,\n        Nat.mod_eq_of_lt h2.right.right]\n      rfl\n      done\n    done\n  done\n\nlemma set_rp_below_prod {m n : Nat} (h1 : rel_prime m n) :\n    set_rp_below (m * n) ∼ (set_rp_below m) ×ₛ (set_rp_below n) := by\n  rewrite [←mod_mod_image h1]\n  show set_rp_below (m * n) ∼\n    image (mod_mod m n) (set_rp_below (m * n)) from\n    equinum_image (mod_mod_one_one_on h1)\n  done\nWe finally have everything we need to prove Theorem 7.4.4.\nlemma eq_numElts_of_equinum {U V : Type} {A : Set U} {B : Set V} {n : Nat}\n    (h1 : A ∼ B) (h2 : numElts A n) : numElts B n := by\n  rewrite [numElts_def] at h2   --h2 : I n ∼ A\n  rewrite [numElts_def]         --Goal : I n ∼ B\n  show I n ∼ B from Theorem_8_1_3_3 h2 h1\n  done\n\ntheorem Theorem_7_4_4 {m n : Nat} (h1 : rel_prime m n) :\n    phi (m * n) = (phi m) * (phi n) := by\n  have h2 : numElts (set_rp_below m) (phi m) := phi_is_numElts m\n  have h3 : numElts (set_rp_below n) (phi n) := phi_is_numElts n\n  have h4 : numElts (set_rp_below (m * n)) (phi (m * n)) :=\n    phi_is_numElts (m * n)\n  have h5 : numElts (set_rp_below m ×ₛ set_rp_below n) (phi (m * n)) :=\n    eq_numElts_of_equinum (set_rp_below_prod h1) h4\n  have h6 : numElts (set_rp_below m ×ₛ set_rp_below n) (phi m * phi n) :=\n    numElts_prod h2 h3\n  show phi (m * n) = phi m * phi n from Exercise_8_1_6b h5 h6\n  done\n\nExercises\n\nlemma image_empty {U : Type} {A : Set U}\n    (f : U → Nat) (h : empty A) : image f A = I 0 := sorry\n\n\nlemma remove_one_equinum\n    {U V : Type} {A : Set U} {B : Set V} {a : U} {b : V} {f : U → V}\n    (h1 : one_one_on f A) (h2 : image f A = B)\n    (h3 : a ∈ A) (h4 : f a = b) : ↑(A \\ {a}) ∼ ↑(B \\ {b}) := sorry\n\n\nlemma singleton_of_diff_empty {U : Type} {A : Set U} {a : U}\n    (h1 : a ∈ A) (h2 : empty (A \\ {a})) : A = {a} := sorry\n\n\nlemma eq_zero_of_I_zero_equinum {n : Nat} (h : I 0 ∼ I n) : n = 0 := sorry\n\n\n--Hint: Use mathematical induction.\ntheorem Exercise_8_1_6a : ∀ ⦃m n : Nat⦄, (I m ∼ I n) → m = n := sorry\n\n\ntheorem Exercise_8_1_6b {U : Type} {A : Set U} {m n : Nat}\n    (h1 : numElts A m) (h2 : numElts A n) : m = n := sorry\n\n\nlemma neb_nrpb (m : Nat) : ∀ ⦃k : Nat⦄, k ≤ m →\n    num_elts_below (set_rp_below m) k = num_rp_below m k := sorry\n\n\n--Hint:  You might find it helpful to apply the theorem div_mod_char\n--from the exercises of Section 6.4.\nlemma qr_image (m n : Nat) :\n    image (qr n) (I (m * n)) = I m ×ₛ I n := sorry\n\n\n\n\nSuppose U and V are types, A and C have type Set U, and we have two functions f : A → V and g : C → V. Then we can define a new function func_union f g : A ∪ C → V as follows:\nlemma is_elt_snd_of_not_fst {U : Type} {A C : Set U} {x : U}\n    (h1 : x ∈ A ∪ C) (h2 : x ∉ A) : x ∈ C := by\n  disj_syll h1 h2\n  show x ∈ C from h1\n  done\n\ndef elt_snd_of_not_fst {U : Type} {A C : Set U} {x : ↑(A ∪ C)}\n  (h : x.val ∉ A) : C :=\n  Subtype_elt (is_elt_snd_of_not_fst x.property h)\n\nnoncomputable def func_union {U V : Type} {A C : Set U}\n  (f : A → V) (g : C → V) (x : ↑(A ∪ C)) : V :=\n  if test : x.val ∈ A then f (Subtype_elt test)\n    else g (elt_snd_of_not_fst test)\nNote that in the definition of func_union, we have test : x.val ∈ A in the then clause and test : x.val ∉ A in the else clause. If x.val ∈ A then the value of func_union f g x is determined by f, and if x.val ∉ A then it is determined by g. The next two exercises ask you to prove properties of this function\n\nlemma func_union_one_one {U V : Type} {A C : Set U}\n    {f : A → V} {g : C → V} (h1 : empty (range f ∩ range g))\n    (h2 : one_to_one f) (h3 : one_to_one g) :\n    one_to_one (func_union f g) := sorry\n\n\nlemma func_union_range {U V : Type} {A C : Set U}\n    (f : A → V) (g : C → V) (h : empty (A ∩ C)) :\n    range (func_union f g) = range f ∪ range g := sorry\n\n\n--Hint:  Use the last two exercises.\ntheorem Theorem_8_1_2_2\n    {U V : Type} {A C : Set U} {B D : Set V}\n    (h1 : empty (A ∩ C)) (h2 : empty (B ∩ D))\n    (h3 : A ∼ B) (h4 : C ∼ D) : ↑(A ∪ C) ∼ ↑(B ∪ D) := sorry\n\n\nlemma shift_I_equinum (n m : Nat) : I m ∼ ↑(I (n + m) \\ I n) := sorry\n\n\ntheorem Theorem_8_1_7 {U : Type} {A B : Set U} {n m : Nat}\n    (h1 : empty (A ∩ B)) (h2 : numElts A n) (h3 : numElts B m) :\n    numElts (A ∪ B) (n + m) := sorry\n\n\ntheorem equinum_sub {U V : Type} {A C : Set U} {B : Set V}\n    (h1 : A ∼ B) (h2 : C ⊆ A) : ∃ (D : Set V), D ⊆ B ∧ C ∼ D := sorry\n\n\ntheorem Exercise_8_1_8b {U : Type} {A B : Set U}\n    (h1 : finite A) (h2 : B ⊆ A) : finite B := sorry\n\n\ntheorem finite_bdd {A : Set Nat} (h : finite A) :\n    ∃ (m : Nat), ∀ n ∈ A, n < m := sorry\n\n\nlemma N_not_finite : ¬finite Nat := sorry\n\n\ntheorem denum_not_finite (U : Type)\n    (h : denum U) : ¬finite U := sorry\n\n\n--Hint:  Use Like_Exercise_6_2_16 from the exercises of Section 6.2.\ntheorem Exercise_6_2_16 {U : Type} {f : U → U}\n    (h1 : one_to_one f) (h2 : finite U) : onto f := sorry"
  },
  {
    "objectID": "Chap8.html#countable-and-uncountable-sets",
    "href": "Chap8.html#countable-and-uncountable-sets",
    "title": "8  Infinite Sets",
    "section": "8.2. Countable and Uncountable Sets",
    "text": "8.2. Countable and Uncountable Sets\nSection 8.2 of HTPI shows that many set-theoretic operations, when applied to countable sets, produce results that are countable. For example, the first part of Theorem 8.2.1 shows that a Cartesian product of countable sets is countable. Our proof of this statement in Lean is based on Theorem_8_1_2_1_set and the denumerability of Nat × Nat. We also use an exercise from Section 8.1.\ntheorem NxN_denum : denum (Nat × Nat) := Theorem_8_1_3_2 NxN_equinum_N\n\ntheorem Theorem_8_2_1_1 {U V : Type} {A : Set U} {B : Set V}\n    (h1 : ctble A) (h2 : ctble B) : ctble (A ×ₛ B) := by\n  rewrite [ctble_iff_set_nat_equinum] at h1\n  rewrite [ctble_iff_set_nat_equinum] at h2\n  obtain (J : Set Nat) (h3 : J ∼ A) from h1\n  obtain (K : Set Nat) (h4 : K ∼ B) from h2\n  have h5 : J ×ₛ K ∼ A ×ₛ B := Theorem_8_1_2_1_set h3 h4\n  have h6 : ctble (Nat × Nat) := Or.inr NxN_denum\n  have h7 : ctble (J ×ₛ K) := ctble_set_of_ctble_type h6 (J ×ₛ K)\n  show ctble (A ×ₛ B) from ctble_of_ctble_equinum h5 h7\n  done\nThe second part of Theorem 8.2.1 shows that a union of two countable sets is countable, but, as we ask you to show in the exercises, it is superseded by Theorem 8.2.2, which says that a union of a countable family of countable sets is countable. So we will skip ahead to Theorem 8.2.2. Here’s how we state the theorem in Lean:\ntheorem Theorem_8_2_2 {U : Type} {F : Set (Set U)}\n    (h1 : ctble F) (h2 : ∀ A ∈ F, ctble A) : ctble (⋃₀ F)\nAs in the proof in HTPI, we will use the characterization of countability from Theorem_8_1_5_2. We first consider the case in which F is nonempty and also all elements of F are nonempty. According to Theorem_8_1_5_2, the hypotheses h1 and h2 then imply that there is a function j : Nat → Set U such that F ⊆ range j, and also for each A ∈ F there is a function gA : Nat → U with A ⊆ range gA. We begin by proving an easier version of the theorem, where we assume that we have a function g from Set U to Nat → U that supplies, for each A ∈ F, the required function gA. Imitating the proof in HTPI, we can use j and g to construct the function needed to prove that ⋃₀ F is countable.\nlemma Lemma_8_2_2_1 {U : Type} {F : Set (Set U)} {g : Set U → Nat → U}\n    (h1 : ctble F) (h2 : ¬empty F) (h3 : ∀ A ∈ F, A ⊆ range (g A)) :\n    ctble (⋃₀ F) := by\n  rewrite [Theorem_8_1_5_2] at h1\n  disj_syll h1 h2               --h1 : ∃ (f : Nat → Set U), F ⊆ range f\n  rewrite [Theorem_8_1_5_2]\n  apply Or.inr                  --Goal : ∃ (f : Nat → Set U), ⋃₀F ⊆ range f\n  obtain (j : Nat → Set U) (h4 : F ⊆ range j) from h1\n  obtain (p : Nat → Nat × Nat) (h5 : one_to_one p ∧ onto p) from NxN_denum\n  set f : Nat → U := fun (n : Nat) => g (j (p n).1) (p n).2\n  apply Exists.intro f\n  fix x : U\n  assume h6 : x ∈ ⋃₀ F\n  obtain (A : Set U) (h7 : A ∈ F ∧ x ∈ A) from h6\n  obtain (n1 : Nat) (h8 : j n1 = A) from h4 h7.left\n  obtain (n2 : Nat) (h9 : g A n2 = x) from h3 A h7.left h7.right\n  obtain (n : Nat) (h10 : p n = (n1, n2)) from h5.right (n1, n2)\n  apply Exists.intro n\n  show f n = x from\n    calc f n\n      _ = g (j (p n).1) (p n).2 := by rfl\n      _ = g (j n1) n2 := by rw [h10]\n      _ = g A n2 := by rw [h8]\n      _ = x := by rw [h9]\n  done\nHow can we use Lemma_8_2_2_1 to prove Theorem_8_2_2 (still assuming that F and every element of F are nonempty)? We must use the hypothesis h2 : ∀ A ∈ F, ctble A in Theorem_8_2_2 to produce the function g required in Lemma_8_2_2_1. As we have already observed, Theorem_8_1_5_2 guarantees that for each A ∈ F, an appropriate function gA : Nat → U exists. We need a function g that will choose such a function gA for each A. A function with this property is often called a choice function. And now we come to a delicate point that was skipped over in HTPI: to prove the existence of a choice function, we must use a statement called the axiom of choice.1\nThe distinction between the existence of an appropriate function gA for each A and the existence of a function that chooses such a function for each A is a subtle one. Perhaps for this reason, many people find the axiom of choice to be intuitively obvious. HTPI took advantage of this intuition to skip over this step in the proof without comment. But of course Lean won’t let us skip anything!\nTo implement the axiom of choice, Lean uses a function called Classical.choose. Given a proof h of a statement of the form ∃ (x : U), P x, the expression Classical.choose h produces (“chooses”) some u : U such that P u is true. There is also a theorem Classical.choose_spec that guarantees that the Classical.choose function meets its specification—that is, P (Classical.choose h) is true. Using these, we can prove a lemma that will bridge the gap between Lemma_8_2_2_1 and Theorem_8_2_2.\nlemma Lemma_8_2_2_2 {U : Type} {F : Set (Set U)} (h1 : ∀ A ∈ F, ctble A)\n    (h2 : ¬empty F) (h3 : ∀ A ∈ F, ¬empty A):\n    ∃ (g : Set U → Nat → U), ∀ A ∈ F, A ⊆ range (g A) := by\n  have h4 : ∀ (A : Set U), ∃ (gA : Nat → U),\n      A ∈ F → A ⊆ range gA := by\n    fix A : Set U\n    by_cases h4 : A ∈ F\n    · -- Case 1. h4 : A ∈ F\n      have h5 : ctble A := h1 A h4\n      rewrite [Theorem_8_1_5_2] at h5\n      disj_syll h5 (h3 A h4)    --h5 : ∃ (f : Nat → U), A ⊆ range f\n      obtain (gA : Nat → U) (h6 : A ⊆ range gA) from h5\n      apply Exists.intro gA\n      assume h7 : A ∈ F\n      show A ⊆ range gA from h6\n      done\n    · -- Case 2. h4 : A ∉ F\n      rewrite [not_empty_iff_exists_elt] at h2\n      obtain (A0 : Set U) (h5 : A0 ∈ F) from h2\n      have h6 : ¬empty A0 := h3 A0 h5\n      rewrite [not_empty_iff_exists_elt] at h6\n      obtain (x0 : U) (h7 : x0 ∈ A0) from h6\n      set gA : Nat → U := constant_func Nat x0\n      apply Exists.intro gA\n      contrapos\n      assume h8 : A ⊈ range gA\n      show A ∉ F from h4\n      done\n    done\n  set g : Set U → Nat → U := fun (A : Set U) => Classical.choose (h4 A)\n  apply Exists.intro g\n  fix A : Set U\n  show A ∈ F → A ⊆ range (g A) from Classical.choose_spec (h4 A)\n  done\nNotice that the domain of the function g in Lemma_8_2_2_2 is Set U, not F. Thus, we must produce a function gA for every A : Set U, but it is only when A ∈ F that we care what gA is. Thus, the proof above just picks a default value (constant_func Nat x0) when A ∉ F.\nWe can now prove the theorem, still under the assumption that all elements of F are nonempty. If F is empty, then we can show that ⋃₀ F is empty, so it has zero elements, which implies that it is finite and therefore countable. If F is not empty, then we can combine Lemma_8_2_2_1 and Lemma_8_2_2_2 to prove the theorem.\nlemma Lemma_8_2_2_3 {U : Type} {F : Set (Set U)}\n    (h1 : ctble F) (h2 : ∀ A ∈ F, ctble A) (h3 : ∀ A ∈ F, ¬empty A) :\n    ctble (⋃₀ F) := by\n  by_cases h4 : empty F\n  · -- Case 1. h4 : empty F\n    have h5 : empty (⋃₀ F) := by\n      contradict h4 with h5\n      rewrite [not_empty_iff_exists_elt] at h5\n      obtain (x : U) (h6 : x ∈ ⋃₀ F) from h5\n      obtain (A : Set U) (h7 : A ∈ F ∧ x ∈ A) from h6\n      show ∃ (x : Set U), x ∈ F from Exists.intro A h7.left\n      done\n    rewrite [←zero_elts_iff_empty] at h5    --h5 : numElts (⋃₀ F) 0\n    define\n    apply Or.inl\n    rewrite [finite_def]\n    show ∃ (n : Nat), numElts (⋃₀ F) n from Exists.intro 0 h5\n    done\n  · -- Case 2. h4 : ¬empty F\n    obtain (g : Set U → Nat → U) (h5 : ∀ A ∈ F, A ⊆ range (g A)) from\n      Lemma_8_2_2_2 h2 h4 h3\n    show ctble (⋃₀ F) from Lemma_8_2_2_1 h1 h4 h5\n    done\nFinally, we deal with the possibility that F contains the empty set. As in HTPI, we show that we can simply remove the empty set from F and then apply our earlier reasoning.\nlemma remove_empty_subset {U : Type} (F : Set (Set U)) :\n    {A : Set U | A ∈ F ∧ ¬empty A} ⊆ F := by\n  fix X : Set U\n  assume h1 : X ∈ {A : Set U | A ∈ F ∧ ¬empty A}\n  define at h1\n  show X ∈ F from h1.left\n  done\n\nlemma remove_empty_union_eq {U : Type} (F : Set (Set U)) :\n    ⋃₀ {A : Set U | A ∈ F ∧ ¬empty A} = ⋃₀ F := sorry\n\ntheorem Theorem_8_2_2 {U : Type} {F : Set (Set U)}\n    (h1 : ctble F) (h2 : ∀ A ∈ F, ctble A) : ctble (⋃₀ F) := by\n  set G : Set (Set U) := {A : Set U | A ∈ F ∧ ¬empty A}\n  have h3 : G ⊆ F := remove_empty_subset F\n  have h4 : ⋃₀ G = ⋃₀ F := remove_empty_union_eq F\n  rewrite [←h4]\n  have h5 : ctble G := Exercise_8_1_17 h3 h1\n  have h6 : ∀ A ∈ G, ctble A := by\n    fix A : Set U\n    assume h6 : A ∈ G\n    show ctble A from h2 A (h3 h6)\n    done\n  have h7 : ∀ A ∈ G, ¬empty A := by\n    fix A : Set U\n    assume h7 : A ∈ G\n    define at h7\n    show ¬empty A from h7.right\n    done\n  show ctble (⋃₀ G) from Lemma_8_2_2_3 h5 h6 h7\n  done\nBy the way, we can now explain a mystery from Section 5.1. The reason we skipped the proof of the right-to-left direction of func_from_graph is that the proof uses Classical.choose and Classical.choose_spec. Now that you know about this function and theorem, we can show you the proof.\ntheorem func_from_graph_rtl {A B : Type} (F : Set (A × B)) :\n    is_func_graph F → (∃ (f : A → B), graph f = F) := by\n  assume h1 : is_func_graph F\n  define at h1    --h1 : ∀ (x : A), ∃! (y : B), (x, y) ∈ F\n  have h2 : ∀ (x : A), ∃ (y : B), (x, y) ∈ F := by\n    fix x : A\n    obtain (y : B) (h3 : (x, y) ∈ F)\n      (h4 : ∀ (y1 y2 : B), (x, y1) ∈ F → (x, y2) ∈ F → y1 = y2) from h1 x\n    show ∃ (y : B), (x, y) ∈ F from Exists.intro y h3\n    done\n  set f : A → B := fun (x : A) => Classical.choose (h2 x)\n  apply Exists.intro f\n  apply Set.ext\n  fix (x, y) : A × B\n  have h3 : (x, f x) ∈ F := Classical.choose_spec (h2 x)\n  apply Iff.intro\n  · -- (→)\n    assume h4 : (x, y) ∈ graph f\n    define at h4        --h4 : f x = y\n    rewrite [h4] at h3\n    show (x, y) ∈ F from h3\n    done\n  · -- (←)\n    assume h4 : (x, y) ∈ F\n    define              --Goal : f x = y\n    obtain (z : B) (h5 : (x, z) ∈ F)\n      (h6 : ∀ (y1 y2 : B), (x, y1) ∈ F → (x, y2) ∈ F → y1 = y2) from h1 x\n    show f x = y from h6 (f x) y h3 h4\n    done\n  done\nThere is one more theorem in Section 8.2 of HTPI showing that a set-theoretic operation, when applied to a countable set, gives a countable result. Theorem 8.2.4 says that if a set \\(A\\) is countable, then the set of all finite sequences of elements of \\(A\\) is also countable. In HTPI, finite sequences are represented by functions, but in Lean it is easier to use lists. Thus, if A has type Set U, then we define a finite sequence of elements of A to be a list l : List U with the property that every entry of l is an element of A. Letting seq A denote the set of all finite sequences of elements of A, our version of Theorem 8.2.4 will say that if A is countable, then so is seq A.\ndef seq {U : Type} (A : Set U) : Set (List U) :=\n  {l : List U | ∀ x ∈ l, x ∈ A}\n\nlemma seq_def {U : Type} (A : Set U) (l : List U) :\n    l ∈ seq A ↔ ∀ x ∈ l, x ∈ A := by rfl\n\ntheorem Theorem_8_2_4 {U : Type} {A : Set U}\n    (h1 : ctble A) : ctble (seq A)\nOur proof of Theorem_8_2_4 will use exactly the same strategy as the proof in HTPI. We begin by showing that, for every natural number n, the set of sequences of elements of A of length n is countable. The proof is by mathematical induction. The base case is easy, because the only sequence of length 0 is the nil list.\ndef seq_by_length {U : Type} (A : Set U) (n : Nat) : Set (List U) :=\n  {l : List U | l ∈ seq A ∧ l.length = n}\n\nlemma sbl_base {U : Type} (A : Set U) : seq_by_length A 0 = {[]} := by\n  apply Set.ext\n  fix l : List U\n  apply Iff.intro\n  · -- (→)\n    assume h1 : l ∈ seq_by_length A 0\n    define at h1   --h1 : l ∈ seq A ∧ l.length = 0\n    rewrite [List.length_eq_zero_iff] at h1\n    define\n    show l = [] from h1.right\n    done\n  · -- (←)\n    assume h1 : l ∈ {[]}\n    define at h1     --h1 : l = []\n    define           --Goal : l ∈ seq A ∧ l.length = 0\n    apply And.intro _ (List.length_eq_zero_iff.rtl h1)\n    define           --Goal : ∀ x ∈ l, x ∈ A\n    fix x : U\n    contrapos\n    assume h2 : x ∉ A\n    rewrite [h1]\n    show x ∉ [] from List.not_mem_nil\n    done\n  done\nFor the induction step, the key idea is that A ×ₛ (seq_by_length A n) ∼ seq_by_length A (n + 1). To prove this, we define a function seq_cons U that matches up A ×ₛ (seq_by_length A n) with seq_by_length A (n + 1).\ndef seq_cons (U : Type) (p : U × (List U)) : List U := p.1 :: p.2\n\nlemma seq_cons_def {U : Type} (x : U) (l : List U) :\n    seq_cons U (x, l) = x :: l := by rfl\n\nlemma seq_cons_one_one (U : Type) : one_to_one (seq_cons U) := by\n  fix (a1, l1) : U × List U; fix (a2, l2) : U × List U\n  assume h1 : seq_cons U (a1, l1) = seq_cons U (a2, l2)\n  rewrite [seq_cons_def, seq_cons_def] at h1  --h1 : a1 :: l1 = a2 :: l2\n  rewrite [List.cons_eq_cons] at h1           --h1 : a1 = a2 ∧ l1 = l2\n  rewrite [h1.left, h1.right]\n  rfl\n  done\n\nlemma seq_cons_image {U : Type} (A : Set U) (n : Nat) :\n    image (seq_cons U) (A ×ₛ (seq_by_length A n)) =\n      seq_by_length A (n + 1) := sorry\n\nlemma Lemma_8_2_4_1 {U : Type} (A : Set U) (n : Nat) :\n    A ×ₛ (seq_by_length A n) ∼ seq_by_length A (n + 1) := by\n  rewrite [←seq_cons_image A n]\n  show A ×ₛ seq_by_length A n ∼\n    image (seq_cons U) (A ×ₛ seq_by_length A n) from equinum_image\n    (one_one_on_of_one_one (seq_cons_one_one U) (A ×ₛ (seq_by_length A n)))\n  done\nWith this preparation, we can now use singleton_one_elt to justify the base case of our induction proof and Theorem_8_2_1_1 for the induction step.\nlemma Lemma_8_2_4_2 {U : Type} {A : Set U} (h1 : ctble A) :\n    ∀ (n : Nat), ctble (seq_by_length A n) := by\n  by_induc\n  · -- Base Case\n    rewrite [sbl_base]   --Goal : ctble {[]}\n    define\n    apply Or.inl         --Goal : finite {[]}\n    rewrite [finite_def]\n    apply Exists.intro 1 --Goal : numElts {[]} 1\n    show numElts {[]} 1 from singleton_one_elt []\n    done\n  · -- Induction Step\n    fix n : Nat\n    assume ih : ctble (seq_by_length A n)\n    have h2 : A ×ₛ (seq_by_length A n) ∼ seq_by_length A (n + 1) :=\n      Lemma_8_2_4_1 A n\n    have h3 : ctble (A ×ₛ (seq_by_length A n)) := Theorem_8_2_1_1 h1 ih\n    show ctble (seq_by_length A (n + 1)) from ctble_of_ctble_equinum h2 h3\n    done\n  done\nOur next step is to show that the union of all of the sets seq_by_length A n, for n : Nat, is seq A.\ndef sbl_set {U : Type} (A : Set U) : Set (Set (List U)) :=\n  {S : Set (List U) | ∃ (n : Nat), seq_by_length A n = S}\n\nlemma Lemma_8_2_4_3 {U : Type} (A : Set U) : ⋃₀ (sbl_set A) = seq A := by\n  apply Set.ext\n  fix l : List U\n  apply Iff.intro\n  · -- (→)\n    assume h1 : l ∈ ⋃₀ (sbl_set A)\n    define at h1\n    obtain (S : Set (List U)) (h2 :  S ∈ sbl_set A ∧ l ∈ S) from h1\n    have h3 : S ∈ sbl_set A := h2.left\n    define at h3\n    obtain (n : Nat) (h4 : seq_by_length A n = S) from h3\n    have h5 : l ∈ S := h2.right\n    rewrite [←h4] at h5\n    define at h5\n    show l ∈ seq A from h5.left\n    done\n  · -- (←)\n    assume h1 : l ∈ seq A\n    define\n    set n : Nat := l.length\n    apply Exists.intro (seq_by_length A n)\n    apply And.intro\n    · -- Proof of seq_by_length A n ∈ sbl_set A\n      define\n      apply Exists.intro n\n      rfl\n      done\n    · -- Proof of l ∈ seq_by_length A n\n      define\n      apply And.intro h1\n      rfl\n      done\n    done\n  done\nOf course, sbl_set A is countable. The easiest way to prove this is to note that seq_by_length A is a function from Nat to Set (List U) whose range contains all of the sets in sbl_set A.\nlemma Lemma_8_2_4_4 {U : Type} (A : Set U) : ctble (sbl_set A) := by\n  rewrite [Theorem_8_1_5_2]\n  apply Or.inr   --Goal : ∃ (f : Nat → Set (List U)), sbl_set A ⊆ range f\n  apply Exists.intro (seq_by_length A)\n  fix S : Set (List U)\n  assume h1 : S ∈ sbl_set A\n  define at h1; define\n  show ∃ (x : Nat), seq_by_length A x = S from h1\n  done\nWe now have everything we need to prove Theorem_8_2_4 as an application of Theorem_8_2_2.\ntheorem Theorem_8_2_4 {U : Type} {A : Set U}\n    (h1 : ctble A) : ctble (seq A) := by\n  set F : Set (Set (List U)) := sbl_set A\n  have h2 : ctble F := Lemma_8_2_4_4 A\n  have h3 : ∀ S ∈ F, ctble S := by\n    fix S : Set (List U)\n    assume h3 : S ∈ F\n    define at h3\n    obtain (n : Nat) (h4 : seq_by_length A n = S) from h3\n    rewrite [←h4]\n    show ctble (seq_by_length A n) from Lemma_8_2_4_2 h1 n\n    done\n  rewrite [←Lemma_8_2_4_3 A]\n  show ctble (⋃₀ sbl_set A) from Theorem_8_2_2 h2 h3\n  done\nThere is a set-theoretic operation that can produce an uncountable set from a countable set: the power set operation. HTPI demonstrates this by proving Cantor’s theorem (Theorem 8.2.5), which says that \\(\\mathscr{P}(\\mathbb{Z}^+)\\) is uncountable. The strategy for this proof is tricky; it involves defining a set \\(D\\) using a method called diagonalization. For an explanation of the motivation behind this strategy, see HTPI.\nHere we will prove in Lean that the collection of all sets of natural numbers is uncountable. There is no need to use the power set operation for this, because we have a type, namely Set Nat, that contains all sets of natural numbers. So our Lean version of Cantor’s theorem says that Set Nat is uncountable.\ntheorem Cantor's_theorem : ¬ctble (Set Nat) := by\n  by_contra h1\n  rewrite [ctble_iff_set_nat_equinum] at h1\n  obtain (J : Set Nat) (h2 : J ∼ Set Nat) from h1\n  obtain (F : J → Set Nat) (h3 : one_to_one F ∧ onto F) from h2\n  set f : Nat → Set Nat := func_extend F ∅\n  set D : Set Nat := {n : Nat | n ∉ f n}\n  obtain (nJ : J) (h4 : F nJ = D) from h3.right D\n  set n : Nat := nJ.val\n  have h5 : n ∈ D ↔ n ∉ f n := by rfl\n  have h6 : f n = F nJ := fe_elt F ∅ nJ\n  rewrite [h6, h4] at h5      --h5 : n ∈ D ↔ n ∉ D\n  by_cases h7 : n ∈ D\n  · -- Case 1. h7 : n ∈ D\n    contradict h7\n    show n ∉ D from h5.ltr h7\n    done\n  · -- Case 2. h7 : n ∉ D\n    contradict h7\n    show n ∈ D from h5.rtl h7\n    done\n  done\nAs a consequence of Theorem 8.2.5, HTPI shows that \\(\\mathbb{R}\\) is uncountable. The proof is not hard, but it requires facts about the decimal expansions of real numbers. Developing those facts in Lean would take us too far afield, so we will skip the proof.\n\nExercises\n\nlemma pair_ctble {U : Type}\n    (a b : U) : ctble ↑({a, b} : Set U) := sorry\n\n\n--Hint:  Use the previous exercise and Theorem_8_2_2.\ntheorem Theorem_8_2_1_2 {U : Type} {A B : Set U}\n    (h1 : ctble A) (h2 : ctble B) : ctble ↑(A ∪ B) := sorry\n\n\nlemma remove_empty_union_eq {U : Type} (F : Set (Set U)) :\n    ⋃₀ {A : Set U | A ∈ F ∧ ¬empty A} = ⋃₀ F := sorry\n\n\nlemma seq_cons_image {U : Type} (A : Set U) (n : Nat) :\n    image (seq_cons U) (A ×ₛ (seq_by_length A n)) =\n      seq_by_length A (n + 1) := sorry\n\n\n--Hint:  Apply Theorem_8_2_4 to the set Univ U.\ntheorem Theorem_8_2_4_type {U : Type}\n    (h : ctble U) : ctble (List U) := sorry\n\n\ndef list_to_set (U : Type) (l : List U) : Set U := {x : U | x ∈ l}\n\nlemma list_to_set_def (U : Type) (l : List U) (x : U) :\n    x ∈ list_to_set U l ↔ x ∈ l := by rfl\n\n--Hint:  Use induction on the size of A.\nlemma set_from_list {U : Type} {A : Set U} (h : finite A) :\n    ∃ (l : List U), list_to_set U l = A := sorry\n\n\n--Hint:  Use the previous exercise and Theorem_8_2_4_type.\ntheorem Like_Exercise_8_2_4 (U : Type) (h : ctble U) :\n    ctble {X : Set U | finite X} := sorry\n\n\ntheorem Exercise_8_2_6b (U V W : Type) :\n     ((U × V) → W) ∼ (U → V → W) := sorry\n\n\ntheorem Like_Exercise_8_2_7 : ∃ (P : Set (Set Nat)),\n    partition P ∧ denum P ∧ ∀ X ∈ P, denum X := sorry\n\n\ntheorem unctbly_many_inf_set_nat :\n    ¬ctble {X : Set Nat | ¬finite X} := sorry\n\n\ntheorem Exercise_8_2_8 {U : Type} {A B : Set U}\n    (h : empty (A ∩ B)) : 𝒫 (A ∪ B) ∼ 𝒫 A ×ₛ 𝒫 B := sorry"
  },
  {
    "objectID": "Chap8.html#the-cantorschröderbernstein-theorem",
    "href": "Chap8.html#the-cantorschröderbernstein-theorem",
    "title": "8  Infinite Sets",
    "section": "8.3. The Cantor–Schröder–Bernstein Theorem",
    "text": "8.3. The Cantor–Schröder–Bernstein Theorem\nThe final section of HTPI proves the Cantor–Schröder–Bernstein theorem. The theorem says that if we have two sets such that there is a one-to-one function from each set to the other, then the two sets are equinumerous. We will prove it in Lean for types, but of course we can apply it to sets as well by coercing the sets to subtypes.\ntheorem Cantor_Schroeder_Bernstein_theorem\n    {U V : Type} {f : U → V} {g : V → U}\n    (h1 : one_to_one f) (h2 : one_to_one g) : U ∼ V\nTo prove the theorem, we must produce a one-to-one, onto function h from U to V. Imitating the proof in HTPI, we will do this by defining a set X : Set U and then using f to determine the values of h on elements of the domain that belong to X and the inverse of g for those that don’t. That is, the graph of h will be the set csb_func_graph f g X defined as follows:\ndef csb_func_graph {U V : Type}\n  (f : U → V) (g : V → U) (X : Set U) : Set (U × V) :=\n  {(x, y) : U × V | (x ∈ X ∧ f x = y) ∨ (x ∉ X ∧ g y = x)}\nIs csb_func_graph f g X the graph of a function? It is not hard to show that it is, as long as ∀ (x : U), x ∉ X → x ∈ range g. We first state lemmas spelling out the two cases in the definition of csb_func_graph f g X, leaving the proof of the second as an exercise for you.\nlemma csb_func_graph_X {U V : Type} {X : Set U} {x : U}\n    (f : U → V) (g : V → U) (h : x ∈ X) (y : V) :\n    (x, y) ∈ csb_func_graph f g X ↔ f x = y := by\n  apply Iff.intro\n  · -- (→)\n    assume h1 : (x, y) ∈ csb_func_graph f g X\n    define at h1\n    have h2 : ¬(x ∉ X ∧ g y = x) := by\n      demorgan\n      show x ∈ X ∨ g y ≠ x from Or.inl h\n      done\n    disj_syll h1 h2        --h1 : x ∈ X ∧ f x = y\n    show f x = y from h1.right\n    done\n  · -- (←)\n    assume h1 : f x = y\n    define\n    apply Or.inl\n    show x ∈ X ∧ f x = y from And.intro h h1\n    done\n  done\n\nlemma csb_func_graph_not_X {U V : Type} {X : Set U} {x : U}\n    (f : U → V) (g : V → U) (h : x ∉ X) (y : V) :\n    (x, y) ∈ csb_func_graph f g X ↔ g y = x := sorry\n\nlemma csb_func_graph_is_func_graph {U V : Type} {g : V → U} {X : Set U}\n    (f : U → V) (h1 : ∀ (x : U), x ∉ X → x ∈ range g) (h2 : one_to_one g) :\n    is_func_graph (csb_func_graph f g X) := by\n  define\n  fix x : U\n  by_cases h3 : x ∈ X\n  · -- Case 1. h3 : x ∈ X\n    exists_unique\n    · -- Existence\n      apply Exists.intro (f x)\n      rewrite [csb_func_graph_X f g h3]\n      rfl\n      done\n    · -- Uniqueness\n      fix y1 : V; fix y2 : V\n      assume h4 : (x, y1) ∈ csb_func_graph f g X\n      assume h5 : (x, y2) ∈ csb_func_graph f g X\n      rewrite [csb_func_graph_X f g h3] at h4  --h4 : f x = y1\n      rewrite [csb_func_graph_X f g h3] at h5  --h5 : f x = y2\n      rewrite [←h4, ←h5]\n      rfl\n      done\n    done\n  · -- Case 2. h3 : x ∉ X\n    exists_unique\n    · -- Existence\n      obtain (y : V) (h4 : g y = x) from h1 x h3\n      apply Exists.intro y\n      rewrite [csb_func_graph_not_X f g h3]\n      show g y = x from h4\n      done\n    · -- Uniqueness\n      fix y1 : V; fix y2 : V\n      assume h4 : (x, y1) ∈ csb_func_graph f g X\n      assume h5 : (x, y2) ∈ csb_func_graph f g X\n      rewrite [csb_func_graph_not_X f g h3] at h4 --h4 : g y1 = x\n      rewrite [csb_func_graph_not_X f g h3] at h5 --h5 : g y2 = x\n      rewrite [←h5] at h4\n      show y1 = y2 from h2 y1 y2 h4\n      done\n    done\n  done\nOur plan is to define h to be the function whose graph is csb_func_graph f g X. With this definition, the value of h x for any x : U can be determined by a simple rule: if x ∈ X, then h x = f x, and if x ∉ X, then h x has the property that g (h x) = x:\nlemma csb_func_X\n    {U V : Type} {f h : U → V} {g : V → U} {X : Set U} {x : U}\n    (h1 : graph h = csb_func_graph f g X) (h2 : x ∈ X) : h x = f x := by\n  rewrite [←graph_def, h1, csb_func_graph_X f g h2]\n  rfl\n  done\n\nlemma csb_func_not_X\n    {U V : Type} {f h : U → V} {g : V → U} {X : Set U} {x : U}\n    (h1 : graph h = csb_func_graph f g X) (h2 : x ∉ X) : g (h x) = x := by\n  have h3 : (x, h x) ∈ graph h := by rfl\n  rewrite [h1, csb_func_graph_not_X f g h2] at h3\n  show g (h x) = x from h3\n  done\nWe still have to say how X will be defined. Let A0 = {x : U | x ∉ range g}. To make sure that the condition ∀ (x : U), x ∉ X → x ∈ range g is satisfied, we will need to have A0 ⊆ X. As explained in HTPI, we can now get a suitable set X by repeatedly taking the image of A0 under g ∘ f. Fortunately, we defined functions in Section 6.5 that do what we need: rep_image (g ∘ f) n A0 is the result of taking the image of A0 under g ∘ f n times. That is, rep_image (g ∘ f) 0 A0 = A0, rep_image (g ∘ f) 1 A0 = image (g ∘ f) A0, rep_image (g ∘ f) 2 A0 = image (g ∘ f) (image (g ∘ f) A0), and so on. We will define X to be the union of all of the sets rep_image (g ∘ f) n A0, which is given by the function cumul_image (g ∘ f) A0.\nTo prove that h is one-to-one, we will need to know that it cannot happen that h x1 = h x2, x1 ∈ X, and x2 ∉ X. After proving this last lemma, we are ready to prove the Cantor–Schröder–Bernstein theorem.\nlemma csb_X_of_X\n    {U V : Type} {f h : U → V} {g : V → U} {A0 : Set U} {x1 x2 : U}\n    (h1 : graph h = csb_func_graph f g (cumul_image (g ∘ f) A0))\n    (h2 : h x1 = h x2) (h3 : x1 ∈ cumul_image (g ∘ f) A0) :\n    x2 ∈ cumul_image (g ∘ f) A0 := by\n  by_contra h4                      --h4 : x2 ∉ cumul_image (g ∘ f) A0\n  rewrite [csb_func_X h1 h3] at h2  --h2 : f x1 = h x2\n  have h5 : (g ∘ f) x1 = x2 :=\n    calc (g ∘ f) x1\n      _ = g (f x1) := by rfl\n      _ = g (h x2) := by rw [h2]\n      _ = x2 := csb_func_not_X h1 h4\n  obtain (n : Nat) (h6 : x1 ∈ rep_image (g ∘ f) n A0) from h3\n  contradict h4               --Goal : x2 ∈ cumul_image (g ∘ f) A0\n  apply Exists.intro (n + 1)  --Goal : x2 ∈ rep_image (g ∘ f) (n + 1) A0\n  rewrite [rep_image_step]\n  apply Exists.intro x1\n  show x1 ∈ rep_image (g ∘ f) n A0 ∧ (g ∘ f) x1 = x2 from\n    And.intro h6 h5\n  done\n\ntheorem Cantor_Schroeder_Bernstein_theorem\n    {U V : Type} {f : U → V} {g : V → U}\n    (h1 : one_to_one f) (h2 : one_to_one g) : U ∼ V := by\n  set A0 : Set U := {x : U | x ∉ range g}\n  set X : Set U := cumul_image (g ∘ f) A0\n  set H : Set (U × V) := csb_func_graph f g X\n  have h3 : ∀ (x : U), x ∉ X → x ∈ range g := by\n    fix x : U\n    contrapos\n    assume h3 : x ∉ range g\n    define\n    apply Exists.intro 0\n    rewrite [rep_image_base]\n    show x ∈ A0 from h3\n    done\n  have h4 : is_func_graph H := csb_func_graph_is_func_graph f h3 h2\n  rewrite [←func_from_graph] at h4\n  obtain (h : U → V) (h5 : graph h = H) from h4\n  apply Exists.intro h\n  apply And.intro\n  · -- proof that h is one-to-one\n    fix x1 : U; fix x2 : U\n    assume h6 : h x1 = h x2\n    by_cases h7 : x1 ∈ X\n    · -- Case 1. h7 : x1 ∈ X\n      have h8 : x2 ∈ X := csb_X_of_X h5 h6 h7\n      rewrite [csb_func_X h5 h7, csb_func_X h5 h8] at h6 --h6 : f x1 = f x2\n      show x1 = x2 from h1 x1 x2 h6\n      done\n    · -- Case 2. h7 : x1 ∉ X\n      have h8 : x2 ∉ X := by\n        contradict h7 with h8\n        show x1 ∈ X from csb_X_of_X h5 h6.symm h8\n        done\n      show x1 = x2 from\n        calc x1\n          _ = g (h x1) := (csb_func_not_X h5 h7).symm\n          _ = g (h x2) := by rw [h6]\n          _ = x2 := csb_func_not_X h5 h8\n      done\n    done\n  · -- proof that h is onto\n    fix y : V\n    by_cases h6 : g y ∈ X\n    · -- Case 1. h6 : g y ∈ X\n      define at h6\n      obtain (n : Nat) (h7 : g y ∈ rep_image (g ∘ f) n A0) from h6\n      have h8 : n ≠ 0 := by\n        by_contra h8\n        rewrite [h8, rep_image_base] at h7 --h7 : g y ∈ A0\n        define at h7                       --h7 : ¬∃ (x : V), g x = g y\n        contradict h7\n        apply Exists.intro y\n        rfl\n        done\n      obtain (k : Nat) (h9 : n = k + 1) from\n        exists_eq_add_one_of_ne_zero h8\n      rewrite [h9, rep_image_step] at h7\n      obtain (x : U)\n        (h10 : x ∈ rep_image (g ∘ f) k A0 ∧ (g ∘ f) x = g y) from h7\n      have h11 : g (f x) = g y := h10.right\n      have h12 : f x = y := h2 (f x) y h11\n      have h13 : x ∈ X := Exists.intro k h10.left\n      apply Exists.intro x\n      rewrite [csb_func_X h5 h13]\n      show f x = y from h12\n      done\n    · -- Case 2. h6 : g y ∉ X\n      apply Exists.intro (g y)\n      have h7 : g (h (g y)) = g y := csb_func_not_X h5 h6\n      show h (g y) = y from h2 (h (g y)) y h7\n      done\n    done\n  done\n\nExercises\n\nlemma csb_func_graph_not_X {U V : Type} {X : Set U} {x : U}\n    (f : U → V) (g : V → U) (h : x ∉ X) (y : V) :\n    (x, y) ∈ csb_func_graph f g X ↔ g y = x := sorry\n\n\ntheorem intervals_equinum :\n    {x : Real | 0 < x ∧ x < 1} ∼ {x : Real | 0 < x ∧ x ≤ 1} := sorry\n\n3. The following theorem could be thought of as an extensionality principle for relations. You may find it useful in later exercises. Hint for proof: First show that extension R = extension S, and then use the fact that R and S can be determined from extension R and extension S (see Section 4.3).\ntheorem relext {U V : Type} {R S : Rel U V}\n    (h : ∀ (u : U) (v : V), R u v ↔ S u v) : R = S := sorry\n\n\n\nThe next six exercises lead up to a proof that the set of all equivalence relations on the natural numbers is equinumerous with the type Set Nat. These exercises use the following definitions:\ndef EqRel (U : Type) : Set (BinRel U) :=\n  {R : BinRel U | equiv_rel R}\n\ndef Part (U : Type) : Set (Set (Set U)) :=\n  {P : Set (Set U) | partition P}\n\ndef EqRelExt (U : Type) : Set (Set (U × U)) :=\n  {E : Set (U × U) | ∃ (R : BinRel U), equiv_rel R ∧ extension R = E}\n\ndef shift_and_zero (X : Set Nat) : Set Nat :=\n  {x + 2 | x ∈ X} ∪ {0}\n\ndef shift_and_zero_comp (X : Set Nat) : Set Nat :=\n  {n : Nat | n ∉ shift_and_zero X}\n\ndef saz_pair (X : Set Nat) : Set (Set Nat) :=\n  {shift_and_zero X, shift_and_zero_comp X}\n\ntheorem EqRel_equinum_Part (U : Type) : EqRel U ∼ Part U := sorry\n\n\ntheorem EqRel_equinum_EqRelExt (U : Type) :\n    EqRel U ∼ EqRelExt U := sorry\n\n\ntheorem EqRel_Nat_to_Set_Nat :\n    ∃ (f : EqRel Nat → Set Nat), one_to_one f := sorry\n\n\ntheorem saz_pair_part (X : Set Nat) : saz_pair X ∈ Part Nat := sorry\n\n\ntheorem Set_Nat_to_EqRel_Nat :\n    ∃ (f : Set Nat → EqRel Nat), one_to_one f := sorry\n\n\ntheorem EqRel_Nat_equinum_Set_Nat : EqRel Nat ∼ Set Nat := sorry"
  },
  {
    "objectID": "Appendix.html",
    "href": "Appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "$$\n\\newcommand{\\setmin}{\\mathbin{\\backslash}}\n\\newcommand{\\symmdiff}{\\mathbin{∆}}\n$$"
  },
  {
    "objectID": "Appendix.html#tactics-used",
    "href": "Appendix.html#tactics-used",
    "title": "Appendix",
    "section": "Tactics Used",
    "text": "Tactics Used\nTactics marked with an asterisk (*) are defined in the file HTPIDefs.lean in the HTPI Lean Package that accompanies this book. They will not work without that file. The others are standard Lean tactics or are defined in Lean’s mathematics library, mathlib.\n\n\n\n\nTactic\nWhere Introduced\n\n\n\n\napply\nSections 3.1 & 3.2\n\n\napply?\nSection 3.6\n\n\nassume*\nIntroduction to Lean: A First Example\n\n\nbicond_neg*\nIntroduction to Lean: Tactic Mode\n\n\nby_cases\nSection 3.5\n\n\nby_cases on*\nSection 3.5\n\n\nby_contra\nSections 3.1 & 3.2\n\n\nby_induc*\nSection 6.1\n\n\nby_strong_induc*\nSection 6.4\n\n\nconditional*\nIntroduction to Lean: Tactic Mode\n\n\ncontradict*\nSections 3.1 & 3.2\n\n\ncontrapos*\nIntroduction to Lean: A First Example\n\n\ndecide\nSection 6.1\n\n\ndefine*\nIntroduction to Lean: Types\n\n\ndemorgan*\nIntroduction to Lean: Tactic Mode\n\n\ndisj_syll*\nSection 3.5\n\n\ndouble_neg*\nIntroduction to Lean: Tactic Mode\n\n\nexact\nSection 3.6\n\n\nexists_unique*\nSection 3.6\n\n\nfix*\nSection 3.3\n\n\nhave\nIntroduction to Lean: A First Example\n\n\nlinarith\nSection 6.1\n\n\nobtain*\nSection 3.3\n\n\nor_left*\nSection 3.5\n\n\nor_right*\nSection 3.5\n\n\npush_neg\nSection 8.1\n\n\nquant_neg*\nSection 3.3\n\n\nrel\nSection 6.3\n\n\nrewrite\nSection 3.6\n\n\nrfl\nSection 3.7\n\n\nring\nSection 3.7\n\n\nrw\nSection 3.7\n\n\nset\nSection 4.5\n\n\nshow*\nIntroduction to Lean: A First Example\n\n\ntrivial\nSection 7.2"
  },
  {
    "objectID": "Appendix.html#transitioning-to-standard-lean",
    "href": "Appendix.html#transitioning-to-standard-lean",
    "title": "Appendix",
    "section": "Transitioning to Standard Lean",
    "text": "Transitioning to Standard Lean\nIf you want to continue to use Lean to write mathematical proofs, you may want to learn more about Lean. A good place to start is the Lean Community website. The resources there use “standard” Lean, which is somewhat different from the Lean in this book.\nIn a few cases we have used notation in this book that differs from standard Lean notation. For example, if h is a proof of P ↔︎ Q, then we have used h.ltr and h.rtl to denote proofs of the left-to-right and right-to-left directions of the biconditional. The standard Lean notation for these is h.mp and h.mpr, respectively (“mp” and “mpr” stand for “modus ponens” and “modus ponens reverse”). As explained at the end of Section 5.4, the notations Pred U and Rel A B denote the types U → Prop and A → B → Prop, respectively. Although Rel is standard notation (defined in Lean’s math library mathlib), Pred is not; the notation BinRel A is also not standard Lean. In place of Pred U you should use U → Prop, and in place of BinRel A you should use Rel A A.\nHowever, the biggest difference between the Lean in this book and standard Lean is that the tactics marked with an asterisk in the table above are not a part of standard Lean. If you want to learn to write proofs in standard Lean, you’ll need to learn replacements for those tactics. We discuss some such replacements below. Some of these replacements are built into Lean, and some are defined in mathlib.\n\nassume, fix\n\nIf you are proving P → Q and you want to begin by assuming h : P, in standard Lean you would begin your proof by writing intro h. You don’t need to specify that h is an identifier for the assumption P; Lean will figure that out on its own.\nIf you are proving ∀ (x : U), P x and you want to begin by introducing the variable x to stand for an arbitrary object of type U, in standard Lean you would begin your proof by writing intro x. Again, you don’t need to specify the type of x, because Lean will figure it out.\nThus, the tactic intro does the job of both assume and fix. Furthermore, you can introduce multiple assumptions or objects with a single use of the intro tactic: intro a b c is equivalent to intro a; intro b; intro c.\n\nbicond_neg, demorgan, double_neg, quant_neg\n\nWe have mostly used these tactics to reexpress negative statements as more useful positive statements. The tactic push_neg can be used for this purpose.\n\nby_cases on\n\nIf you have h : P ∨ Q, then you can break your proof into cases by using the tactic rcases h with hP | hQ. In case 1, h : P ∨ Q will be replaced by hP : P, and in case 2 it will be replaced by hQ : Q. In both cases, you have to prove the original goal.\n\nby_induc, by_strong_induc\n\nWe saw in Section 7.2 that if you are proving a statement of the form ∀ (l : List U), ..., then you can begin a proof by induction on the length of l by using the tactic apply List.rec. Similarly, if you are proving ∀ (n : Nat), ..., you can begin a proof by induction by using the tactic apply Nat.recAux. For strong induction, you can use apply Nat.strongRec.\nThere are also tactics induction and induction' that you may want to learn about.\n\nconditional\n\nThe commands #check @imp_iff_not_or and #check @not_imp produce the results\n\n@imp_iff_not_or : ∀ {a b : Prop}, a → b ↔ ¬a ∨ b\n@not_imp : ∀ {a b : Prop}, ¬(a → b) ↔ a ∧ ¬b\n\nThus, rewrite [imp_iff_not_or] will convert a statement of the form P → Q into ¬P ∨ Q, and rewrite [←imp_iff_not_or] will go in the other direction. Similarly, rewrite [not_imp] will convert a statement of the form ¬(P → Q) into P ∧ ¬Q, and rewrite [←not_imp] will go in the other direction.\n\ncontradict\n\nSuppose your goal is False (as it would be if you are doing a proof by contradiction), and you have h : ¬P. Recall that Lean treats ¬P as meaning the same thing as P → False, and therefore h _ will prove the goal, if the blank is filled in with a proof of P. It follows that apply h will set P as the goal. In other words, in this situation apply h has the same effect as contradict h.\nYou could also get the same effect with the tactic suffices hP : P from h hP. Think of this as meaning “it would suffice now to prove P, because if hP were a proof of P, then h hP would prove the goal.” Lean therefore sets P to be the goal.\nSimilarly, in a proof by contradiction, if you have h : P, then suffices hnP : ¬P from hnP h will set ¬P as the goal.\nYet another possibility is contrapose! h. (This is a variant on the contrapose! tactic, discussed in the next section.)\n\ncontrapos\n\nIf your goal is a conditional statement, then the tactics contrapose and contrapose! will replace the goal with its contrapositive (contrapose! also uses push_neg to try to simplify the negated statements that arise when forming a contrapositive). You may also find the theorem not_imp_not useful:\n\n@not_imp_not : ∀ {a b : Prop}, ¬a → ¬b ↔ b → a\n\n\ndefine\n\nThe tactic whnf (which stands for “weak head normal form”) is similar to define, although it sometimes produces results that are a little confusing.\nAnother way to write out definitions is to prove a lemma stating the definition and then use that lemma as a rewriting rule in the rewrite tactic. See, for example, the use of the theorem inv_def in Section 4.2.\n\ndisj_syll\n\nThe following theorems can be useful:\n\n@Or.resolve_left : ∀ {a b : Prop}, a ∨ b → ¬a → b\n@Or.resolve_right : ∀ {a b : Prop}, a ∨ b → ¬b → a\n@Or.neg_resolve_left : ∀ {a b : Prop}, ¬a ∨ b → a → b\n@Or.neg_resolve_right : ∀ {a b : Prop}, a ∨ ¬b → b → a\n\nFor example, if you have h1 : P ∨ Q and h2 : ¬P, then Or.resolve_left h1 h2 is a proof of Q.\n\nexists_unique\n\nIf your goal is ∃! (x : U), P x and you think that a is the unique value of x that makes P x true, then you can use the tactic apply ExistsUnique.intro a. This will leave you with two goals to prove, P a and ∀ (y : U), P y → y = a.\n\nobtain\n\nIf you have h : ∃ (x : U), P x, then the tactic obtain ⟨u, h1⟩ := h will introduce both u : U and h1 : P u into the tactic state. Note that u and h1 must be enclosed in angle brackets, ⟨ ⟩. To enter those brackets, type \\< and \\>.\nIf you have h : ∃! (x : U), P x, then obtain ⟨u, h1, h2⟩ := h will also introduce u : U and h1 : P u into the tactic state. In addition, it will introduce h2 as an identifier for a statement that is equivalent to ∀ (y : U), P y → y = u. (Unfortunately, the statement introduced is more complicated.)\nYou may also find the theorems ExistsUnique.exists and ExistsUnique.unique useful:\n\n@ExistsUnique.exists : ∀ {α : Sort u_1} {p : α → Prop},\n              (∃! (x : α), p x) → ∃ (x : α), p x\n@ExistsUnique.unique : ∀ {α : Sort u_1} {p : α → Prop},\n              (∃! (x : α), p x) → ∀ {y₁ y₂ : α}, p y₁ → p y₂ → y₁ = y₂\n\n\nor_left, or_right\n\nIf your goal is P ∨ Q, then the tactics or_left and or_right let you assume that one of P and Q is false and prove the other. Perhaps the easiest way to do that in standard Lean is to use proof by cases. For example, to assume P is false and prove Q you might proceed as follows:\n  -- Goal is P ∨ Q\n  by_cases hP : P\n  · -- Case 1. hP : P\n    exact Or.inl hP\n    done\n  · -- Case 2. hP : ¬P\n    apply Or.inr\n    --We now have hP : ¬P, and goal is Q\n    **done::\n\nshow\n\nThere is a show tactic in standard Lean, but it works a little differently from the show tactic we have used in this book. When our goal was a statement P and we had an expression t that was a proof of P, we usually completed the proof by writing show P from t. In standard Lean you can complete the proof by writing exact t, as explained near the end of Section 3.6."
  },
  {
    "objectID": "Appendix.html#typing-symbols",
    "href": "Appendix.html#typing-symbols",
    "title": "Appendix",
    "section": "Typing Symbols",
    "text": "Typing Symbols\n\n\n\n\nSymbol\nHow To Type It\n\n\n\n\n¬\n\\not or \\n\n\n\n∧\n\\and\n\n\n∨\n\\or or \\v\n\n\n→\n\\to or \\r or \\imp\n\n\n↔︎\n\\iff or \\lr\n\n\n∀\n\\forall or \\all\n\n\n∃\n\\exists or \\ex\n\n\n⦃\n\\{{\n\n\n⦄\n\\}}\n\n\n=\n=\n\n\n≠\n\\ne\n\n\n∈\n\\in\n\n\n∉\n\\notin or \\inn\n\n\n⊆\n\\sub\n\n\n⊈\n\\subn\n\n\n∪\n\\union or \\cup\n\n\n∩\n\\inter or \\cap\n\n\n⋃₀\n\\U0\n\n\n⋂₀\n\\I0\n\n\n\\\n\\\\\n\n\n∆\n\\symmdiff\n\n\n∅\n\\emptyset\n\n\n𝒫\n\\powerset\n\n\n·\n\\.\n\n\n←\n\\leftarrow or \\l\n\n\n↑\n\\uparrow or \\u\n\n\nℕ\n\\N\n\n\nℤ\n\\Z\n\n\nℚ\n\\Q\n\n\nℝ\n\\R\n\n\nℂ\n\\C\n\n\n≤\n\\le\n\n\n≥\n\\ge\n\n\n∣\n\\|\n\n\n×\n\\times or \\x\n\n\n∘\n\\comp or \\circ\n\n\n≡\n\\==\n\n\n∼\n\\sim or \\~\n\n\nₛ\n\\_s\n\n\n⟨\n\\<\n\n\n⟩\n\\>"
  }
]